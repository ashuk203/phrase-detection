dual recurrent attention units for visual question answering
we propose an architecture for vqa which utilizes recurrent layers to generate visual and textual attention. the memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question. our single model outperforms the first place winner on the vqa 1.0 dataset performs within margin to the current state of the art ensemble model. we also experiment with replacing attention mechanisms in other state of the art models with our implementation and show increased accuracy. in both cases our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the vqa dataset.
sequential short text classification with recurrent and convolutional neural networks
recent approaches based on artificial neural networks anns have shown promising results for short text classification. however many short texts occur in sequences e.g. sentences in a document or utterances in a dialog and most existing ann based systems do not leverage the preceding short texts when classifying a subsequent one. in this work we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. our model achieves state of the art results on three different datasets for dialog act prediction.
multiresolution recurrent neural networks an application to dialogue response generation
we introduce the multiresolution recurrent neural network which extends the sequence to sequence framework to model natural language generation as two parallel discrete stochastic processes a sequence of high level coarse tokens and a sequence of natural language tokens. there are many ways to estimate or learn the high level coarse tokens but we argue that a simple extraction procedure is sufficient to capture a wealth of high level discourse semantics. such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log likelihood over both sequences. in contrast to the standard log likelihood objective w.r.t. natural language tokens word perplexity optimizing the joint log likelihood biases the model towards modeling high level abstractions. we apply the proposed model to the task of dialogue response generation in two challenging domains the ubuntu technical support domain and twitter conversations. on ubuntu the model outperforms competing approaches by a substantial margin achieving state of the art results according to both automatic evaluation metrics and a human evaluation study. on twitter the model appears to generate more relevant and on topic responses according to automatic evaluation metrics. finally our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long term structure.
learning what to share between loosely related tasks
multi task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones. similarly deep neural networks can profit from related tasks by sharing parameters with other networks. however humans do not consciously decide to transfer knowledge between tasks. in natural language processing nlp it is hard to predict if sharing will lead to improvements particularly if tasks are only loosely related. to overcome this we introduce sluice networks a general framework for multi task learning where trainable parameters control the amount of sharing. our framework generalizes previous proposals in enabling sharing of all combinations of subspaces layers and skip connections. we perform experiments on three task pairs and across seven different domains using data from ontonotes 5.0 and achieve up to 15 average error reductions over common approaches to multi task learning. we show that a label entropy is predictive of gains in sluice networks confirming findings for hard parameter sharing and b while sluice networks easily fit noise they are robust across domains in practice.
a deep reinforcement learning chatbot
we present milabot a deep reinforcement learning chatbot developed by the montreal institute for learning algorithms mila for the amazon alexa prize competition. milabot is capable of conversing with humans on popular small talk topics through both speech and text. the system consists of an ensemble of natural language generation and retrieval models including template based models bag of words models sequence to sequence neural network and latent variable neural network models. by applying reinforcement learning to crowdsourced data and real world user interactions the system has been trained to select an appropriate response from the models in its ensemble. the system has been evaluated through a b testing with real world users where it performed significantly better than many competing systems. due to its machine learning architecture the system is likely to improve with additional data.
generating sentences by editing prototypes
we propose a new generative model of sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. compared to traditional models that generate from scratch either left to right or by first sampling a latent sentence vector our prototype then edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. furthermore the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence level analogies.
a deep reinforcement learning chatbot short version 
we present milabot a deep reinforcement learning chatbot developed by the montreal institute for learning algorithms mila for the amazon alexa prize competition. milabot is capable of conversing with humans on popular small talk topics through both speech and text. the system consists of an ensemble of natural language generation and retrieval models including neural network and template based models. by applying reinforcement learning to crowdsourced data and real world user interactions the system has been trained to select an appropriate response from the models in its ensemble. the system has been evaluated through a b testing with real world users where it performed significantly better than other systems. the results highlight the potential of coupling ensemble systems with deep reinforcement learning as a fruitful path for developing real world open domain conversational agents.
document image coding and clustering for script discrimination
the paper introduces a new method for discrimination of documents given in different scripts. the document is mapped into a uniformly coded text of numerical values. it is derived from the position of the letters in the text line based on their typographical characteristics. each code is considered as a gray level. accordingly the coded text determines a 1 d image on which texture analysis by run length statistics and local binary pattern is performed. it defines feature vectors representing the script content of the document. a modified clustering approach employed on document feature vector groups documents written in the same script. experimentation performed on two custom oriented databases of historical documents in old cyrillic angular and round glagolitic as well as antiqua and fraktur scripts demonstrates the superiority of the proposed method with respect to well known methods in the state of the art.
tutorial on answering questions about images with deep learning
together with the development of more accurate methods in computer vision and natural language understanding holistic architectures that answer on questions about the content of real world images have emerged. in this tutorial we build a neural based approach to answer questions about images. we base our tutorial on two datasets mostly on daquar and a bit on vqa. with small tweaks the models that we present here can achieve a competitive performance on both datasets in fact they are among the best methods that use a combination of lstm with a global full frame cnn representation of an image. we hope that after reading this tutorial the reader will be able to use deep learning frameworks such as keras and introduced kraino to build various architectures that will lead to a further performance improvement on this challenging task.
pix2code generating code from a graphical user interface screenshot
transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software websites and mobile applications. in this paper we show that deep learning methods can be leveraged to train a model end to end to automatically generate code from a single input image with over 77 of accuracy for three different platforms i.e. ios android and web based technologies .
a unified deep neural network for speaker and language recognition
learned feature representations and sub phoneme posteriors from deep neural networks dnns have been used separately to produce significant performance gains for speaker and language recognition tasks. in this work we show how these gains are possible using a single dnn for both speaker and language recognition. the unified dnn approach is shown to yield substantial performance improvements on the the 2013 domain adaptation challenge speaker recognition task 55 reduction in eer for the out of domain condition and on the nist 2011 language recognition evaluation 48 reduction in eer for the 30s test condition .
efficient neural architecture search via parameter sharing
we propose efficient neural architecture search enas a fast and inexpensive approach for automatic model design. in enas a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. the controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. thanks to parameter sharing between child models enas is fast it delivers strong empirical performances using much fewer gpu hours than all existing automatic model design approaches and notably 1000x less expensive than standard neural architecture search. on the penn treebank dataset enas discovers a novel architecture that achieves a test perplexity of 55.8 establishing a new state of the art among all methods without post training processing. on the cifar 10 dataset enas designs novel architectures that achieve a test error of 2.89 which is on par with nasnet zoph et al. 2018 whose test error is 2.65 .
building machines that learn and think like people
recent progress in artificial intelligence ai has renewed interest in building systems that learn and think like people. many advances have come from using deep neural networks trained end to end in tasks such as object recognition video games and board games achieving performance that equals or even beats humans in some respects. despite their biological inspiration and performance achievements these systems differ from human intelligence in crucial ways. we review progress in cognitive science suggesting that truly human like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. specifically we argue that these machines should a build causal models of the world that support explanation and understanding rather than merely solving pattern recognition problems b ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned and c harness compositionality and learning to learn to rapidly acquire and generalize knowledge to new tasks and situations. we suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.
towards bayesian deep learning a survey
while perception tasks such as visual object recognition and text understanding play an important role in human intelligence the subsequent tasks that involve inference reasoning and planning require an even higher level of intelligence. the past few years have seen major advances in many perception tasks using deep learning models. for higher level inference however probabilistic graphical models with their bayesian nature are still more powerful and flexible. to achieve integrated intelligence that involves both perception and inference it is naturally desirable to tightly integrate deep learning and bayesian models within a principled probabilistic framework which we call bayesian deep learning. in this unified framework the perception of text or images using deep learning can boost the performance of higher level inference and in return the feedback from the inference process is able to enhance the perception of text or images. this survey provides a general introduction to bayesian deep learning and reviews its recent applications on recommender systems topic models and control. in this survey we also discuss the relationship and differences between bayesian deep learning and other related topics like bayesian treatment of neural networks.
hierarchical deep reinforcement learning integrating temporal abstraction and intrinsic motivation
learning goal directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. the primary difficulty arises due to insufficient exploration resulting in an agent being unable to learn robust value functions. intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. we present hierarchical dqn h dqn a framework to integrate hierarchical value functions operating at different temporal scales with intrinsically motivated deep reinforcement learning. a top level value function learns a policy over intrinsic goals and a lower level function learns a policy over atomic actions to satisfy the given goals. h dqn allows for flexible goal specifications such as functions over entities and relations. this provides an efficient space for exploration in complicated environments. we demonstrate the strength of our approach on two problems with very sparse delayed feedback 1 a complex discrete stochastic decision process and 2 the classic atari game montezuma s revenge .
learning features by watching objects move
this paper presents a novel yet intuitive approach to unsupervised feature learning. inspired by the human visual system we explore whether low level motion based grouping cues can be used to learn an effective visual representation. specifically we use unsupervised motion based segmentation on videos to obtain segments which we use as pseudo ground truth to train a convolutional network to segment objects from a single frame. given the extensive evidence that motion plays a key role in the development of the human visual system we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed pretext tasks studied in the literature. indeed our extensive experiments show that this is the case. when used for transfer learning on object detection our representation significantly outperforms previous unsupervised approaches across multiple settings especially when training data for the target task is scarce.
domain adaptive neural networks for object recognition
we propose a simple neural network model to deal with the domain adaptation problem in object recognition. our model incorporates the maximum mean discrepancy mmd measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. from experiments we demonstrate that the mmd regularization is an effective tool to provide good domain adaptation models on both surf features and raw image pixels of a particular image data set. we also show that our proposed model preceded by the denoising auto encoder pretraining achieves better performance than recent benchmark models on the same data sets. this work represents the first study of mmd measure in the context of neural networks.
beyond temporal pooling recurrence and temporal convolutions for gesture recognition in video
recent studies have demonstrated the power of recurrent neural networks for machine translation image captioning and speech recognition. for the task of capturing temporal structure in video however there still remain numerous open research questions. current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. we demonstrate that this method is not sufficient for gesture recognition where temporal information is more discriminative compared to general video classification tasks. we explore deep architectures for gesture recognition in video and propose a new end to end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. our main contributions are twofold first we show that recurrence is crucial for this task second we show that adding temporal convolutions leads to significant improvements. we evaluate the different approaches on the montalbano gesture recognition dataset where we achieve state of the art results.
telugu ocr framework using deep learning
in this paper we address the task of optical character recognition ocr for the telugu script. we present an end to end framework that segments the text image classifies the characters and extracts lines using a language model. the segmentation is based on mathematical morphology. the classification module which is the most challenging task of the three is a deep convolutional neural network. the language is modelled as a third degree markov chain at the glyph level. telugu script is a complex alphasyllabary and the language is agglutinative making the problem hard. in this paper we apply the latest advances in neural networks to achieve state of the art error rates. we also review convolutional neural networks in great detail and expound the statistical justification behind the many tricks needed to make deep learning work.
adversarial feature learning
the ability of the generative adversarial networks gans framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. intuitively models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. however in their existing form gans have no means of learning the inverse mapping projecting data back into the latent space. we propose bidirectional generative adversarial networks bigans as a means of learning this inverse mapping and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks competitive with contemporary approaches to unsupervised and self supervised feature learning.
the mythos of model interpretability
supervised machine learning models boast remarkable predictive capabilities. but can you trust your model will it work in deployment what else can it tell you about the world we want models to be not only good but interpretable. and yet the task of interpretation appears underspecified. papers provide diverse and sometimes non overlapping motivations for interpretability and offer myriad notions of what attributes render models interpretable. despite this ambiguity many papers proclaim interpretability axiomatically absent further explanation. in this paper we seek to refine the discourse on interpretability. first we examine the motivations underlying interest in interpretability finding them to be diverse and occasionally discordant. then we address model properties and techniques thought to confer interpretability identifying transparency to humans and post hoc explanations as competing notions. throughout we discuss the feasibility and desirability of different notions and question the oft made assertions that linear models are interpretable and that deep neural networks are not.
neurogenesis inspired dictionary learning online model adaption in a changing world
in this paper we focus on online representation learning in non stationary environments which may require continuous adaptation of model architecture. we propose a novel online dictionary learning sparse coding framework which incorporates the addition and deletion of hidden units dictionary elements and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus known to be associated with improved cognitive function and adaptation to new environments. in the online learning setting where new input instances arrive sequentially in batches the neuronal birth is implemented by adding new units with random initial weights random dictionary elements the number of new units is determined by the current performance representation error of the dictionary higher error causing an increase in the birth rate. neuronal death is implemented by imposing l1 l2 regularization group sparsity on the dictionary within the block coordinate descent optimization at each iteration of our online alternating minimization scheme which iterates between the code and dictionary updates. finally hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. our empirical evaluation on several real life datasets images and language as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state of art fixed size nonadaptive online sparse coding of mairal et al. 2009 in the presence of nonstationary data. moreover we identify certain properties of the data e.g. sparse inputs with nearly non overlapping supports and of the model e.g. dictionary sparsity associated with such improvements.
borrowing treasures from the wealthy deep transfer learning through selective joint fine tuning
deep neural networks require a large amount of labeled training data during supervised learning. however collecting and labeling so much data might be infeasible in many cases. in this paper we introduce a source target selective joint fine tuning scheme for improving the performance of deep learning tasks with insufficient training data. in this scheme a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. however the source learning task does not use all existing training data. our core idea is to identify and use a subset of training images from the original source learning task whose low level characteristics are similar to those from the target learning task and jointly fine tune shared convolutional layers for both tasks. specifically we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks and use such descriptors to search for a desired subset of training samples for the source learning task. experiments demonstrate that our selective joint fine tuning scheme achieves state of the art performance on multiple visual classification tasks with insufficient training data for deep learning. such tasks include caltech 256 mit indoor 67 oxford flowers 102 and stanford dogs 120. in comparison to fine tuning without a source domain the proposed method can improve the classification accuracy by 2 10 using a single model.
aligned image word representations improve inductive transfer across vision language tasks
an important goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. in this paper we investigate a vision language embedding as a core representation and show that it leads to better cross task transfer than standard multi task learning. in particular the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word region embeddings. we show this leads to greater inductive transfer from recognition to vqa than standard multitask learning. visual recognition also improves especially for categories that have relatively few recognition training labels but appear often in the vqa setting. thus our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable flexible and trainable core representations.
universal adversarial perturbations against semantic image segmentation
while deep learning is remarkably successful on perceptual tasks it was also shown to be vulnerable to adversarial perturbations of the input. these perturbations denote noise added to the input that was generated specifically to fool the system while being quasi imperceptible for humans. more severely there even exist universal perturbations that are input agnostic but fool the network on the majority of inputs. while recent work has focused on image classification this work proposes attacks against semantic image segmentation we present an approach for generating universal adversarial perturbations that make the network yield a desired target segmentation as output. we show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. furthermore we also show the existence of universal noise which removes a target class e.g. all pedestrians from the segmentation while leaving the segmentation mostly unchanged otherwise.
the loss surface of deep and wide neural networks
while the optimization problem behind deep neural networks is highly non convex it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. it has been argued that this is the case as all local minima are close to being globally optimal. we show that this is almost true in fact almost all local minima are globally optimal for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.
semantically decomposing the latent spaces of generative adversarial networks
we propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities e.g. individual humans and observations e.g. specific photographs . by fixing the identity portion of the latent codes we can generate diverse images of the same subject and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. corresponding samples from the real dataset consist of two distinct photographs of the same subject. in order to fool the discriminator the generator must produce pairs that are photorealistic distinct and appear to depict the same individual. we augment both the dcgan and began approaches with siamese discriminators to facilitate pairwise training. experiments with human judges and an off the shelf face verification system demonstrate our algorithm s ability to generate convincing identity matched photographs.
variants of rmsprop and adagrad with logarithmic regret bounds
adaptive gradient methods have become recently very popular in particular as they have been shown to be useful in the training of deep neural networks. in this paper we have analyzed rmsprop originally proposed for the training of deep neural networks in the context of online convex optimization and show sqrt t type regret bounds. moreover we propose two variants sc adagrad and sc rmsprop for which we show logarithmic regret bounds for strongly convex functions. finally we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.
alice towards understanding adversarial learning for joint distribution matching
we investigate the non identifiability issues associated with bidirectional adversarial training for joint distribution matching. within a framework of conditional entropy we propose both adversarial and non adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. we unify a broad family of adversarial models as joint distribution matching problems. our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. further we introduce an extension for semi supervised learning tasks. theoretical results are validated in synthetic data and real world applications.
a systematic study of the class imbalance problem in convolutional neural networks
in this study we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks cnns and compare frequently used methods to address the issue. class imbalance is a common problem that has been comprehensively studied in classical machine learning yet very limited systematic research is available in the context of deep learning. in our study we use three benchmark datasets of increasing complexity mnist cifar 10 and imagenet to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue oversampling undersampling two phase training and thresholding that compensates for prior class probabilities. our main evaluation metric is area under the receiver operating characteristic curve roc auc adjusted to multi class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. based on results from our experiments we conclude that i the effect of class imbalance on classification performance is detrimental ii the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling iii oversampling should be applied to the level that totally eliminates the imbalance whereas undersampling can perform better when the imbalance is only removed to some extent iv as opposed to some classical machine learning models oversampling does not necessarily cause overfitting of cnns v thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.
regularization for deep learning a taxonomy
regularization is one of the crucial ingredients of deep learning yet the term regularization has various definitions and regularization methods are often studied separately from each other. in our work we present a systematic unifying taxonomy to categorize existing methods. we distinguish methods that affect data network architectures error terms regularization terms and optimization procedures. we do not provide all details about the listed methods instead we present an overview of how the methods can be sorted into meaningful categories and sub categories. this helps revealing links and fundamental similarities between them. finally we include practical recommendations both for users and for developers of new regularization methods.
clustering with deep learning taxonomy and new methods
clustering is a fundamental machine learning method. the quality of its results is dependent on the data distribution. for this reason deep neural networks can be used for learning better representations of the data. in this paper we propose a systematic taxonomy for clustering with deep learning in addition to a review of methods from the field. based on our taxonomy creating new methods is more straightforward. we also propose a new approach which is built on the taxonomy and surpasses some of the limitations of some previous work. our experimental evaluation on image datasets shows that the method approaches state of the art clustering quality and performs better in some cases.
coarse to fine non rigid registration a chain of scale specific neural networks for multimodal image alignment with application to remote sensing
we tackle here the problem of multimodal image non rigid registration which is of prime importance in remote sensing and medical imaging. the difficulties encountered by classical registration approaches include feature design and slow optimization by gradient descent. by analyzing these methods we note the significance of the notion of scale. we design easy to train fully convolutional neural networks able to learn scale specific features. once chained appropriately they perform global registration in linear time getting rid of gradient descent schemes by predicting directly the deformation.we show their performance in terms of quality and speed through various tasks of remote sensing multimodal image alignment. in particular we are able to register correctly cadastral maps of buildings as well as road polylines onto rgb images and outperform current keypoint matching methods.
describing videos by exploiting temporal structure
recent progress in using recurrent neural networks rnns for image description has motivated the exploration of their application for video description. however while images are static working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. in this context we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. first our approach incorporates a spatial temporal 3 d convolutional neural network 3 d cnn representation of the short temporal dynamics. the 3 d cnn representation is trained on video action recognition tasks so as to produce a representation that is tuned to human motion and behavior. second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text generating rnn. our approach exceeds the current state of art for both bleu and meteor metrics on the youtube2text dataset. we also present results on a new larger and more challenging dataset of paired video and natural language descriptions.
collaborative recurrent autoencoder recommend while learning to fill in the blanks
hybrid methods that utilize both content and rating information are commonly used in many recommender systems. however most of them use either handcrafted features or the bag of words representation as a surrogate for the content information but they are neither effective nor natural enough. to address this problem we develop a collaborative recurrent autoencoder crae which is a denoising recurrent autoencoder drae that models the generation of content sequences in the collaborative filtering cf setting. the model generalizes recent advances in recurrent deep learning from i.i.d. input to non i.i.d. cf based input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. to do this we first develop a hierarchical bayesian model for the drae and then generalize it to the cf setting. the synergy between denoising and cf enables crae to make accurate recommendations while learning to fill in the blanks in sequences. experiments on real world datasets from different domains citeulike and netflix show that by jointly modeling the order aware generation of sequences for the content information and performing cf for the ratings crae is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.
sentiment classification using images and label embeddings
in this project we analysed how much semantic information images carry and how much value image data can add to sentiment analysis of the text associated with the images. to better understand the contribution from images we compared models which only made use of image data models which only made use of text data and models which combined both data types. we also analysed if this approach could help sentiment classifiers generalize to unknown sentiments.
natural parameter networks a class of probabilistic neural networks
neural networks nn have achieved state of the art performance in various applications. unfortunately in applications where training data is insufficient they are often prone to overfitting. one effective way to alleviate this problem is to exploit the bayesian approach by using bayesian neural networks bnn . another shortcoming of nn is the lack of flexibility to customize different distributions for the weights and neurons according to the data as is often done in probabilistic graphical models. to address these problems we propose a class of probabilistic neural networks dubbed natural parameter networks npn as a novel and lightweight bayesian treatment of nn. npn allows the usage of arbitrary exponential family distributions to model the weights and neurons. different from traditional nn and bnn npn takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. as a bayesian treatment efficient backpropagation bp is performed to learn the natural parameters for the distributions over both the weights and neurons. the output distributions of each layer as byproducts may be used as second order representations for the associated tasks such as link prediction. experiments on real world datasets show that npn can achieve state of the art performance.
learning to perform physics experiments via deep reinforcement learning
when encountering novel objects humans are able to infer a wide range of physical properties such as mass friction and deformability by interacting with them in a goal driven way. this process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in go atari natural language processing and complex control problems however it is not clear that these systems can rival the scientific intuition of even a young child. in this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. we found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. by systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.
a network based end to end trainable task oriented dialogue system
teaching machines to accomplish tasks by conversing naturally with humans is challenging. currently developing task oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting or acquiring costly labelled datasets to solve a statistical learning problem for each component. in this work we introduce a neural network based text in text out end to end trainable goal oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe lined wizard of oz framework. this approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. the results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.
a factorization machine framework for testing bigram embeddings in knowledgebase completion
embedding based knowledge base completion models have so far mostly combined distributed representations of individual entities or relations to compute truth scores of missing links. facts can however also be represented using pairwise embeddings i.e. embeddings for pairs of entities and relations. in this paper we explore such bigram embeddings with a flexible factorization machine model and several ablations from it. we investigate the relevance of various bigram types on the fb15k237 dataset and find relative improvements compared to a compositional model.
neural networks for joint sentence classification in medical paper abstracts
existing models based on artificial neural networks anns for sentence classification often do not incorporate the context in which sentences appear and classify sentences individually. however traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences such as with conditional random fields. in this work we present an ann architecture that combines the effectiveness of typical ann models to classify sentences in isolation with the strength of structured prediction. our model achieves state of the art results on two different datasets for sequential sentence classification in medical abstracts.
de identification of patient notes with recurrent neural networks
objective patient notes in electronic health records ehrs may contain critical information for medical investigations. however the vast majority of medical investigators can only access de identified notes in order to protect the confidentiality of patients. in the united states the health insurance portability and accountability act hipaa defines 18 types of protected health information phi that needs to be removed to de identify patient notes. manual de identification is impractical given the size of ehr databases the limited number of researchers with access to the non de identified notes and the frequent mistakes of human annotators. a reliable automated de identification system would consequently be of high value. materials and methods we introduce the first de identification system based on artificial neural networks anns which requires no handcrafted features or rules unlike existing systems. we compare the performance of the system with state of the art systems on two datasets the i2b2 2014 de identification challenge dataset which is the largest publicly available de identification dataset and the mimic de identification dataset which we assembled and is twice as large as the i2b2 2014 dataset. results our ann model outperforms the state of the art systems. it yields an f1 score of 97.85 on the i2b2 2014 dataset with a recall 97.38 and a precision of 97.32 and an f1 score of 99.23 on the mimic de identification dataset with a recall 99.25 and a precision of 99.06. conclusion our findings support the use of anns for de identification of patient notes as they show better performance than previously published systems while requiring no feature engineering.
reasoning with memory augmented neural networks for language comprehension
hypothesis testing is an important cognitive process that supports human reasoning. in this paper we introduce a computational hypothesis testing approach based on memory augmented neural networks. our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. we apply the proposed approach to language comprehension task by using neural semantic encoders nse . our nse models achieve the state of the art results showing an absolute improvement of 1.2 to 2.6 accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the children s book test cbt and who did what wdw news article datasets.
automatic rule extraction from long short term memory networks
although deep learning models have proven effective at solving problems in natural language processing the mechanism by which they come to their conclusions is often unclear. as a result these models are generally treated as black boxes yielding no insight of the underlying learned patterns. in this paper we consider long short term memory networks lstms and demonstrate a new approach for tracking the importance of a given input to the lstm for a given output. by identifying consistently important patterns of words we are able to distill state of the art lstms on sentiment analysis and question answering into a set of representative phrases. this representation is then quantitatively validated by using the extracted phrases to construct a simple rule based classifier which approximates the output of the lstm.
comparing rule based and deep learning models for patient phenotyping
objective we investigate whether deep learning techniques for natural language processing nlp can be used efficiently for patient phenotyping. patient phenotyping is a classification task for determining whether a patient has a medical condition and is a crucial part of secondary analysis of healthcare data. we assess the performance of deep learning algorithms and compare them with classical nlp approaches. materials and methods we compare convolutional neural networks cnns n gram models and approaches based on ctakes that extract pre defined medical concepts from clinical notes and use them to predict patient phenotypes. the performance is tested on 10 different phenotyping tasks using 1 610 discharge summaries extracted from the mimic iii database. results cnns outperform other phenotyping algorithms in all 10 tasks. the average f1 score of our model is 76 ppv of 83 and sensitivity of 71 with our model having an f1 score up to 37 points higher than alternative approaches. we additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction. conclusion we show that nlp methods based on deep learning improve the performance of patient phenotyping. our cnn based algorithm automatically learns the phrases associated with each patient phenotype. as such it reduces the annotation complexity for clinical domain experts who are normally required to develop task specific annotation rules and identify relevant phrases. our method performs well in terms of both performance and interpretability which indicates that deep learning is an effective approach to patient phenotyping based on clinicians notes.
mit at semeval 2017 task 10 relation extraction with convolutional neural networks
over 50 million scholarly articles have been published they constitute a unique repository of knowledge. in particular one may infer from them relations between scientific concepts such as synonyms and hyponyms. artificial neural networks have been recently explored for relation extraction. in this work we continue this line of work and present a system based on a convolutional neural network to extract relations. our model ranked first in the semeval 2017 task 10 scienceie for relation extraction in scientific articles subtask c .
transfer learning for named entity recognition with neural networks
recent approaches based on artificial neural networks anns have shown promising results for named entity recognition ner . in order to achieve high performances anns need to be trained on a large labeled dataset. however labels might be difficult to obtain for the dataset on which the user wants to perform ner label scarcity is particularly pronounced for patient note de identification which is an instance of ner. in this work we analyze to what extent transfer learning may address this issue. in particular we demonstrate that transferring an ann model trained on a large labeled dataset to another dataset with a limited number of labels improves upon the state of the art results on two different datasets for patient note de identification.
adversarial generation of natural language
generative adversarial networks gans have gathered a lot of attention from the computer vision community yielding impressive results for image generation. advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images and still lag far behind likelihood based methods. in this paper we take a step towards generating natural language with a gan objective alone. we introduce a simple baseline that addresses the discrete output space problem without relying on gradient estimators and show that it is able to achieve state of the art results on a chinese poem generation dataset. we present quantitative results on generating sentences from context free and probabilistic context free grammars and qualitative language modeling results. a conditional version is also described that can generate sequences conditioned on sentence characteristics.
explaining recurrent neural network predictions in sentiment analysis
recently a technique called layer wise relevance propagation lrp was shown to deliver insightful explanations in the form of input space relevances for understanding feed forward neural network classification decisions. in the present work we extend the usage of lrp to recurrent neural networks. we propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as lstms and grus. we apply our technique to a word based bi directional lstm model on a five class sentiment prediction task and evaluate the resulting lrp relevances both qualitatively and quantitatively obtaining better results than a gradient based related method which was used in previous work.
text compression for sentiment analysis via evolutionary algorithms
can textual data be compressed intelligently without losing accuracy in evaluating sentiment in this study we propose a novel evolutionary compression algorithm parsec parts of speech for sentiment compression which makes use of parts of speech tags to compress text in a way that sacrifices minimal classification accuracy when used in conjunction with sentiment analysis algorithms. an analysis of parsec with eight commercial and non commercial sentiment analysis algorithms on twelve english sentiment data sets reveals that accurate compression is possible with 0 1.3 3.3 loss in sentiment classification accuracy for 20 50 75 data compression with parsec using lingpipe the most accurate of the sentiment algorithms. other sentiment analysis algorithms are more severely affected by compression. we conclude that significant compression of text data is possible for sentiment analysis depending on the accuracy demands of the specific application and the specific sentiment analysis algorithm used.
building competitive direct acoustics to word models for english conversational speech recognition
direct acoustics to word a2w models in the end to end paradigm have received increasing attention compared to conventional sub word based automatic speech recognition models using phones characters or context dependent hidden markov model states. this is because a2w models recognize words from speech without any decoder pronunciation lexicon or externally trained language model making training and decoding with such models simple. prior work has shown that a2w models require orders of magnitude more training data in order to perform comparably to conventional models. our work also showed this accuracy gap when using the english switchboard fisher data set. this paper describes a recipe to train an a2w model that closes this gap and is at par with state of the art sub word based models. we achieve a word error rate of 8.8 13.9 on the hub5 2000 switchboard callhome test sets without any decoder or language model. we find that model initialization training data order and regularization have the most impact on the a2w model performance. next we present a joint word character a2w model that learns to first spell the word and then recognize it. this model provides a rich output to the user instead of simple word hypotheses making it especially useful in the case of words unseen or rarely seen during training.
ask attend and answer exploring question guided spatial attention for visual question answering
we address the problem of visual question answering vqa which requires joint image and language understanding to answer a question about a given photograph. recent approaches have applied deep image captioning methods based on convolutional recurrent networks to this problem but have failed to model spatial inference. to remedy this we propose a model we call the spatial memory network and apply it to the vqa task. memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. our spatial memory network stores neuron activations from different spatial regions of the image in its memory and uses the question to choose relevant regions for computing the answer a process of which constitutes a single hop in the network. we propose a novel spatial attention architecture that aligns words with image patches in the first hop and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. to better understand the inference process learned by the network we design synthetic questions that specifically require spatial inference and visualize the attention weights. we evaluate our model on two published visual question answering datasets daquar 1 and vqa 2 and obtain improved results compared to a strong deep baseline model ibowimg which concatenates image and question features to predict the answer 3 .
task driven visual saliency and attention based visual question answering
visual question answering vqa has witnessed great progress since may 2015 as a classic problem unifying visual and textual data into a system. many enlightening vqa works explore deep into the image and question encodings and fusing methods of which attention is the most effective and infusive mechanism. current attention based methods focus on adequate fusion of visual and textual features but lack the attention to where people focus to ask questions about the image. traditional attention based methods attach a single value to the feature at each spatial location which losses many useful information. to remedy these problems we propose a general method to perform saliency like pre selection on overlapped region features by the interrelation of bidirectional lstm bilstm and use a novel element wise multiplication based attention method to capture more competent correlation information between visual and textual features. we conduct experiments on the large scale coco vqa dataset and analyze the effectiveness of our model demonstrated by strong empirical results.
optimising the input window alignment in cd dnn based phoneme recognition for low latency processing
we present a systematic analysis on the performance of a phonetic recogniser when the window of input features is not symmetric with respect to the current frame. the recogniser is based on context dependent deep neural networks cd dnns and hidden markov models hmms . the objective is to reduce the latency of the system by reducing the number of future feature frames required to estimate the current output. our tests performed on the timit database show that the performance does not degrade when the input window is shifted up to 5 frames in the past compared to common practice no future frame . this corresponds to improving the latency by 50 ms in our settings. our tests also show that the best results are not obtained with the symmetric window commonly employed but with an asymmetric window with eight past and two future context frames although this observation should be confirmed on other data sets. the reduction in latency suggested by our results is critical for specific applications such as real time lip synchronisation for tele presence but may also be beneficial in general applications to improve the lag in human machine spoken interaction.
bridging lstm architecture and the neural dynamics during reading
recently the long short term memory neural network lstm has attracted wide interest due to its success in many tasks. lstm architecture consists of a memory cell and three gates which looks similar to the neuronal networks in the brain. however there still lacks the evidence of the cognitive plausibility of lstm architecture as well as its working mechanism. in this paper we study the cognitive plausibility of lstm by aligning its internal architecture with the brain activity observed via fmri when the subjects read a story. experiment results show that the artificial memory vector in lstm can accurately predict the observed sequential brain activities indicating the correlation between lstm architecture and the cognitive process of story reading.
feature weight tuning for recursive neural networks
this paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence in other words to perform weight tuning for higher level representation acquisition. we propose two models weighted neural network wnn and binary expectation neural network benn which automatically control how much one specific unit contributes to the higher level representation. the proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. experimental results demonstrate the significant improvement over standard neural models.
a new data representation based on training data characteristics to extract drug named entity in medical text
one essential task in information extraction from the medical corpus is drug name recognition. compared with text sources come from other domains the medical text is special and has unique characteristics. in addition the medical text mining poses more challenges e.g. more unstructured text the fast growing of new terms addition a wide range of name variation for the same drug. the mining is even more challenging due to the lack of labeled dataset sources and external knowledge as well as multiple token representations for a single drug name that is more common in the real application setting. although many approaches have been proposed to overwhelm the task some problems remained with poor f score performance less than 0.75 . this paper presents a new treatment in data representation techniques to overcome some of those challenges. we propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. the first technique is evaluated with the standard nn model i.e. mlp multi layer perceptrons . the second technique involves two deep network classifiers i.e. dbn deep belief networks and sae stacked denoising encoders . the third technique represents the sentence as a sequence that is evaluated with a recurrent nn model i.e. lstm long short term memory . in extracting the drug name entities the third technique gives the best f score performance compared to the state of the art with its average f score being 0.8645.
dopelearning a computational approach to rap lyrics generation
writing rap lyrics requires both creativity to construct a meaningful interesting story and lyrical skills to produce complex rhyme patterns which form the cornerstone of good flow. we present a rap lyrics generation method that captures both of these aspects. first we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. this model is based on two machine learning techniques the ranksvm algorithm and a deep neural network model with a novel structure. results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17 i.e. over 50 times more likely than by random. second we employ the prediction model to combine lines from existing songs producing lyrics with rhyme and a meaning. an evaluation of the produced lyrics shows that in terms of quantitative rhyme density the method outperforms the best human rappers by 21 . the rap lyrics generator has been deployed as an online tool called deepbeat and the performance of the tool has been assessed by analyzing its usage logs. this analysis shows that machine learned rankings correlate with user preferences.
match srnn modeling the recursive matching structure with spatial rnn
semantic matching which aims to determine the matching degree between two texts is a fundamental problem for many nlp applications. recently deep learning approach has been applied to this problem and significant improvements have been achieved. in this paper we propose to view the generation of the global interaction between two texts as a recursive process i.e. the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position. based on this idea we propose a novel deep architecture namely match srnn to model the recursive matching structure. firstly a tensor is constructed to capture the word level interactions. then a spatial rnn is applied to integrate the local interactions recursively with importance determined by four types of gates. finally the matching score is calculated based on the global interaction. we show that after degenerated to the exact matching scenario match srnn can approximate the dynamic programming process of longest common subsequence. thus there exists a clear interpretation for match srnn. our experiments on two semantic matching tasks showed the effectiveness of match srnn and its ability of visualizing the learned matching structure.
piecewise latent variables for neural variational text processing
advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables such as variational autoencoders. the hope is that such models will learn to represent rich multi modal latent factors in real world data such as natural language text. however current models often assume simplistic priors on the latent variables such as the uni modal gaussian distribution which are incapable of representing complex latent factors efficiently. to overcome this restriction we propose the simple but highly flexible piecewise constant distribution. this distribution has the capacity to represent an exponential number of modes of a latent target distribution while remaining mathematically tractable. our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.
recurrent neural networks with external memory for language understanding
recurrent neural networks rnns have become increasingly popular for the task of language understanding. in this task a semantic tagger is deployed to associate a semantic label to each word in an input sequence. the success of rnn may be attributed to its ability to memorize long term dependence that relates the current time semantic label prediction to the observations many time instances away. however the memory capacity of simple rnns is limited because of the gradient vanishing and exploding problem. we propose to use an external memory to improve memorization capability of rnns. we conducted experiments on the atis dataset and observed that the proposed model was able to achieve the state of the art results. we compare our proposed model with alternative models and report analysis results that may provide insights for future research.
a neural network approach to context sensitive generation of conversational responses
we present a novel response generation system that can be trained end to end on large quantities of unstructured twitter conversations. a neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models allowing the system to take into account previous dialog utterances. our dynamic context generative models show consistent gains over both context sensitive and non context sensitive machine translation and information retrieval baselines.
the ubuntu dialogue corpus a large dataset for research in unstructured multi turn dialogue systems
this paper introduces the ubuntu dialogue corpus a dataset containing almost 1 million multi turn dialogues with a total of over 7 million utterances and 100 million words. this provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. the dataset has both the multi turn property of conversations in the dialog state tracking challenge datasets and the unstructured nature of interactions from microblog services such as twitter. we also describe two neural learning architectures suitable for analyzing this dataset and provide benchmark performance on the task of selecting the best next response.
building end to end dialogue systems using generative hierarchical neural network models
we investigate the task of building open domain conversational dialogue systems based on large dialogue corpora using generative models. generative models produce system responses that are autonomously generated word by word opening up the possibility for realistic flexible interactions. in support of this goal we extend the recently proposed hierarchical recurrent encoder decoder neural network to the dialogue domain and demonstrate that this model is competitive with state of the art neural language models and back off n gram models. we investigate the limitations of this and similar approaches and show how its performance can be improved by bootstrapping the learning from a larger question answer pair corpus and from pretrained word embeddings.
end to end attention based large vocabulary speech recognition
many of the current state of the art large vocabulary continuous speech recognition systems lvcsr are hybrids of neural networks and hidden markov models hmms . most of these systems contain separate components that deal with the acoustic modelling language modelling and sequence decoding. we investigate a more direct approach in which the hmm is replaced with a recurrent neural network rnn that performs sequence prediction directly at the character level. alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the rnn. for each predicted character the attention mechanism scans the input sequence and chooses relevant frames. we propose two methods to speed up this operation limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames thereby reducing source sequence length. integrating an n gram language model into the decoding process yields recognition accuracies similar to other hmm free rnn based approaches.
towards neural network based reasoning
we propose neural reasoner a framework for neural network based reasoning over natural language sentences. given a question neural reasoner can infer over multiple supporting facts and find an answer to the question in specific forms. neural reasoner has 1 a specific interaction pooling mechanism allowing it to examine multiple facts and 2 a deep architecture allowing it to model the complicated logical relations in reasoning tasks. assuming no particular structure exists in the question and facts neural reasoner is able to accommodate different types of reasoning and different forms of language expressions. despite the model complexity neural reasoner can still be trained effectively in an end to end manner. our empirical studies show that neural reasoner can outperform existing neural reasoning systems with remarkable margins on two difficult artificial tasks positional reasoning and path finding proposed in 8 . for example it improves the accuracy on path finding 10k from 33.4 6 to over 98 .
what to talk about and how selective generation using lstms with coarse to fine alignment
we propose an end to end domain independent neural encoder aligner decoder model for selective generation i.e. the joint task of content selection and surface realization. our model first encodes a full set of over determined database event records via an lstm based recurrent neural network then utilizes a novel coarse to fine aligner to identify the small subset of salient records to talk about and finally employs a decoder to generate free form descriptions of the aligned selected records. our model achieves the best selection and generation results reported to date with 59 relative improvement in generation on the benchmark weathergov dataset despite using no specialized features or linguistic resources. using an improved k nearest neighbor beam filter helps further. we also perform a series of ablations and visualizations to elucidate the contributions of our key model components. lastly we evaluate the generalizability of our model on the robocup dataset and get results that are competitive with or better than the state of the art despite being severely data starved.
reasoning about entailment with neural attention
while most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines in practice their performance has been only slightly better than bag of word pair classifiers using only lexical similarity. the only attempt so far to build an end to end differentiable neural network for entailment failed to outperform such a simple similarity classifier. in this paper we propose a neural model that reads two sentences to determine entailment using long short term memory units. we extend this model with a word by word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. furthermore we present a qualitative analysis of attention weights produced by this model demonstrating such reasoning capabilities. on a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. it is the first generic end to end differentiable system that achieves state of the art accuracy on a textual entailment dataset.
highway long short term memory rnns for distant speech recognition
in this paper we extend the deep long short term memory dlstm recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. these direct links called highway connections enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper lstms. we further introduce the latency controlled bidirectional lstms blstms which can exploit the whole history while keeping the latency under control. efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. experiments on the ami distant speech recognition dsr task indicate that we can train deeper lstms and achieve better improvement from sequence training with highway lstms hlstms . our novel model obtains 43.9 47.7 wer on ami sdm dev and eval sets outperforming all previous works. it beats the strong dnn and dlstm baselines with 15.7 and 5.3 relative improvement respectively.
neural enquirer learning to query tables with natural language
we proposed neural enquirer as a neural network architecture to execute a natural language nl query on a knowledge base kb for answers. basically neural enquirer finds the distributed representation of a query and then executes it on knowledge base tables to obtain the answer as one of the values in the tables. unlike similar efforts in end to end training of semantic parsers neural enquirer is fully neuralized it not only gives distributional representation of the query and the knowledge base but also realizes the execution of compositional queries as a series of differentiable operations with intermediate results consisting of annotations of the tables at different levels saved on multiple layers of memory. neural enquirer can be trained with gradient descent with which not only the parameters of the controlling components and semantic parsing component but also the embeddings of the tables and query words can be learned from scratch. the training can be done in an end to end fashion but it can take stronger guidance e.g. the step by step supervision for complicated queries and benefit from it. neural enquirer is one step towards building neural network systems which seek to understand language by executing it on real world. our experiments show that neural enquirer can learn to execute fairly complicated nl queries on tables with rich structures.
sentence pair scoring towards unified framework for text comprehension
we review the task of sentence pair scoring popular in the literature in various forms viewed as answer sentence selection semantic text scoring next utterance ranking recognizing textual entailment paraphrasing or e.g. a component of memory networks. we argue that all such tasks are similar from the model perspective and propose new baselines by comparing the performance of common ir metrics and popular convolutional recurrent and attention based neural models across many sentence pair scoring tasks and datasets. we discuss the problem of evaluating randomized models propose a statistically grounded methodology and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored benchmarks. we introduce a unified open source software framework with easily pluggable models and tasks which enables us to experiment with multi task reusability of trained sentence model. we set a new state of art in performance on the ubuntu dialogue dataset.
incorporating copying mechanism in sequence to sequence learning
we address an important problem in sequence to sequence seq2seq learning referred to as copying in which certain segments in the input sequence are selectively replicated in the output sequence. a similar phenomenon is observable in human language communication. for example humans tend to repeat entity names or even long phrases in conversation. the challenge with regard to copying in seq2seq is that new machinery is needed to decide when to perform the operation. in this paper we incorporate copying into neural network based seq2seq learning and propose a new model called copynet with encoder decoder structure. copynet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub sequences in the input sequence and put them at proper places in the output sequence. our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of copynet. for example copynet can outperform regular rnn based model with remarkable margins on text summarization tasks.
generating factoid questions with recurrent neural networks the 30m factoid question answer corpus
over the past decade large scale supervised learning corpora have enabled machine learning researchers to make substantial advances. however to this date there are no large scale question answer corpora available. in this paper we present the 30m factoid question answer corpus an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base freebase to transduce facts into natural language questions. the produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics including well established machine translation and sentence similarity metrics. across all evaluation criteria the question generation model outperforms the competing template based baseline. furthermore when presented to human evaluators the generated questions appear comparable in quality to real human generated questions.
how not to evaluate your dialogue system an empirical study of unsupervised evaluation metrics for dialogue response generation
we investigate evaluation metrics for dialogue response generation systems where supervised labels such as task completion are not available. recent works in response generation have adopted metrics from machine translation to compare a model s generated response to a single target response. we show that these metrics correlate very weakly with human judgements in the non technical twitter domain and not at all in the technical ubuntu domain. we provide quantitative and qualitative results highlighting specific weaknesses in existing metrics and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.
a hierarchical latent variable encoder decoder model for generating dialogues
sequential data often possesses a hierarchical structure with complex dependencies between subsequences such as found between the utterances in a dialogue. in an effort to model this kind of generative process we propose a neural network based generative architecture with latent stochastic variables that span a variable number of time steps. we apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. we evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. the experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.
neural associative memory for dual sequence modeling
many important nlp problems can be posed as dual sequence or sequence to sequence modeling tasks. recent advances in building end to end neural architectures have been highly successful in solving such tasks. in this work we propose a new architecture for dual sequence modeling that is based on associative memory. we derive am rnns a recurrent associative memory am which augments generic recurrent neural networks rnn . this architecture is extended to the dual am rnn which operates on two ams at once. our models achieve very competitive results on textual entailment. a qualitative analysis demonstrates that long range dependencies between source and target sequence can be bridged effectively using dual am rnns. however an initial experiment on auto encoding reveals that these benefits are not exploited by the system when learning to solve sequence to sequence tasks which indicates that additional supervision or regularization is needed.
log linear rnns towards recurrent neural networks with flexible prior knowledge
we introduce ll rnns log linear rnns an extension of recurrent neural networks that replaces the softmax output layer by a log linear output layer of which the softmax is a special case. this conceptually simple move has two main advantages. first it allows the learner to combat training data sparsity by allowing it to model words or more generally output symbols as complex combinations of attributes without requiring that each combination is directly observed in the training data as the softmax does . second it permits the inclusion of flexible prior knowledge in the form of a priori specified modular features where the neural network component learns to dynamically control the weights of a log linear distribution exploiting these features. we conduct experiments in the domain of language modelling of french that exploit morphological prior knowledge and show an important decrease in perplexity relative to a baseline rnn. we provide other motivating iillustrations and finally argue that the log linear and the neural network components contribute complementary strengths to the ll rnn the ll aspect allows the model to incorporate rich prior knowledge while the nn aspect according to the representation learning paradigm allows the model to discover novel combination of characteristics.
embracing data abundance booktest dataset for reading comprehension
there is a practically unlimited amount of natural language data available. still recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. this article is making a case for the community to move to larger data and as a step in that direction it is proposing the booktest a new dataset similar to the popular children s book test cbt however more than 60 times larger. we show that training on the new data improves the accuracy of our attention sum reader model on the original cbt test data by a much larger margin than many recent attempts to improve the model architecture. on one version of the dataset our ensemble even exceeds the human baseline provided by facebook. we then show in our own human study that there is still space for further improvement.
quasi recurrent neural networks
recurrent neural networks are a powerful tool for modeling sequential data but the dependence of each timestep s computation on the previous timestep s output limits parallelism and makes rnns unwieldy for very long sequences. we introduce quasi recurrent neural networks qrnns an approach to neural sequence modeling that alternates convolutional layers which apply in parallel across timesteps and a minimalist recurrent pooling function that applies in parallel across channels. despite lacking trainable recurrent layers stacked qrnns have better predictive accuracy than stacked lstms of the same hidden size. due to their increased parallelism they are up to 16 times faster at train and test time. experiments on language modeling sentiment classification and character level neural machine translation demonstrate these advantages and underline the viability of qrnns as a basic building block for a variety of sequence tasks.
input switched affine networks an rnn architecture designed for interpretability
there exist many problem domains where the interpretability of neural network models is essential for deployment. here we introduce a recurrent architecture composed of input switched affine transformations in other words an rnn without any explicit nonlinearities but with input dependent recurrent weights. this simple form allows the rnn to be analyzed via straightforward linear methods we can exactly characterize the linear contribution of each input to the model predictions we can use a change of basis to disentangle input output and computational hidden unit subspaces we can fully reverse engineer the architecture s solution to a simple task. despite this ease of interpretation the input switched affine network achieves reasonable performance on a text modeling tasks and allows greater computational efficiency than networks with standard nonlinearities.
frustratingly short attention spans in neural language modeling
neural language models predict the next token using a latent representation of the immediate token history. recently various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. for predicting the next token these models query information from a memory of the recent history which can facilitate learning mid and long range dependencies. however conventional attention mechanisms used in memory augmented neural language models produce a single output vector per time step. this vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. in this paper we propose a neural language model with a key value attention mechanism that outputs separate representations for the key and value of a differentiable memory as well as for encoding the next word distribution. this model outperforms existing memory augmented neural language models on two corpora. yet we found that our method mainly utilizes a memory of the five most recent output representations. this led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory augmented neural language models.
a structured self attentive sentence embedding
this paper proposes a new model for extracting an interpretable sentence embedding by introducing self attention. instead of using a vector we use a 2 d matrix to represent the embedding with each row of the matrix attending on a different part of the sentence. we also propose a self attention mechanism and a special regularization term for the model. as a side effect the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. we evaluate our model on 3 different tasks author profiling sentiment classification and textual entailment. results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.
a recurrent neural model with attention for the recognition of chinese implicit discourse relations
we introduce an attention based bi lstm for chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order agnostic approaches. our model benefits from a partial sampling scheme and is conceptually simple yet achieves state of the art performance on the chinese discourse treebank. we also visualize its attention activity to illustrate the model s ability to selectively focus on the relevant parts of an input sequence.
event representations for automated story generation with deep neural nets
automated story generation is the problem of automatically selecting a sequence of events actions or words that can be told as a story. we seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. to date recurrent neural networks that learn language models at character word or sentence levels have had little success generating coherent stories. we explore the question of event representations that provide a mid level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. we present a technique for preprocessing textual story data into event sequences. we then present a technique for automated story generation whereby we decompose the problem into the generation of successive events event2event and the generation of natural language sentences from events event2sentence . we give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.
a joint model for question answering and question generation
we propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. the proposed model uses a sequence to sequence framework that encodes the document and generates a question answer given an answer question . significant improvement in model performance is observed empirically on the squad corpus confirming our hypothesis that the model benefits from jointly learning to perform both tasks. we believe the joint model s novelty offers a new perspective on machine comprehension beyond architectural engineering and serves as a first step towards autonomous information seeking.
learning intrinsic sparse structures within long short term memory
model compression is significant for the wide adoption of recurrent neural networks rnns in both user devices possessing limited resources and business clusters requiring quick responses to large scale service requests. this work aims to learn structurally sparse long short term memory lstm by reducing the sizes of basic structures within lstm units including input updates gates hidden states cell states and outputs. independently reducing the sizes of basic structures can result in inconsistent dimensions among them and consequently end up with invalid lstm units. to overcome the problem we propose intrinsic sparse structures iss in lstms. removing a component of iss will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. by learning iss within lstm units the obtained lstms remain regular while having much smaller basic structures. based on group lasso regularization our method achieves 10.59x speedup without losing any perplexity of a language modeling of penn treebank dataset. it is also successfully evaluated through a compact model with only 2.69m weights for machine question answering of squad dataset. our approach is successfully extended to non lstm rnns like recurrent highway networks rhns . our source code is publicly available at https github.com wenwei202 iss rnns
why pairdiff works a mathematical analysis of bilinear relational compositional operators for analogy detection
representing the semantic relations that exist between two given words or entities is an important first step in a wide range of nlp applications such as analogical reasoning knowledge base completion and relational information retrieval. a simple yet surprisingly accurate method for representing a relation between two words is to compute the vector offset pairdiff between their corresponding word embeddings. despite the empirical success it remains unclear as to whether pairdiff is the best operator for obtaining a relational representation from word embeddings. we conduct a theoretical analysis of generalised bilinear operators that can be used to measure the ell 2 relational distance between two word pairs. we show that if the word embeddings are standardised and uncorrelated such an operator will be independent of bilinear terms and can be simplified to a linear form where pairdiff is a special case. for numerous word embedding types we empirically verify the uncorrelation assumption demonstrating the general applicability of our theoretical result. moreover we experimentally discover pairdiff from the bilinear relation composition operator on several benchmark analogy datasets.
object oriented neural programming oonp for document understanding
we propose object oriented neural programming oonp a framework for semantically parsing documents in specific domains. basically oonp reads a document and parses it into a predesigned object oriented data structure referred to as ontology in this paper that reflects the domain specific semantics of the document. an oonp parser models semantic parsing as a decision process a neural net based reader sequentially goes through the document and during the process it builds and updates an intermediate ontology to summarize its partial understanding of the text it covers. oonp supports a rich family of operations both symbolic and differentiable for composing the ontology and a big variety of forms both symbolic and differentiable for representing the state and the document. an oonp parser can be trained with supervision of different forms and strength including supervised learning sl reinforcement learning rl and hybrid of the two. our experiments on both synthetic and real world document parsing tasks have shown that oonp can learn to handle fairly complicated ontology with training data of modest sizes.
a neural comprehensive ranker ncr for open domain question answering
this paper proposes a novel neural machine reading model for open domain question answering at scale. existing machine comprehension models typically assume that a short piece of relevant text containing answers is already identified and given to the models from which the models are designed to extract answers. this assumption however is not realistic for building a large scale open domain question answering system which requires both deep text understanding and identifying relevant text from corpus simultaneously. in this paper we introduce neural comprehensive ranker ncr that integrates both passage ranking and answer extraction in one single framework. a q a system based on this framework allows users to issue an open domain question without needing to provide a piece of text that must contain the answer. experiments show that the unified ncr model is able to outperform the states of the art in both retrieval of relevant text and answer extraction.
improving speech recognition by revising gated recurrent units
speech recognition is largely taking advantage of deep learning showing that substantial benefits can be obtained by modern recurrent neural networks rnns . the most popular rnns are long short term memory lstms which typically reach state of the art performance in many tasks thanks to their ability to learn long term dependencies and robustness to vanishing gradients. nevertheless lstms have a rather complex design with three multiplicative gates that might impair their efficient implementation. an attempt to simplify lstms has recently led to gated recurrent units grus which are based on just two multiplicative gates. this paper builds on these efforts by further revising grus and proposing a simplified architecture potentially more suitable for speech recognition. the contribution of this work is two fold. first we suggest to remove the reset gate in the gru design resulting in a more efficient single gate architecture. second we propose to replace tanh with relu activations in the state update equations. results show that in our implementation the revised architecture reduces the per epoch training time with more than 30 and consistently improves recognition performance across different tasks input features and noisy conditions when compared to a standard gru.
integrating planning for task completion dialogue policy learning
training a task completion dialogue agent with real users via reinforcement learning rl could be prohibitively expensive because it requires many interactions with users. one alternative is to resort to a user simulator while the discrepancy of between simulated and real users makes the learned policy unreliable in practice. this paper addresses these challenges by integrating planning into the dialogue policy learning based on dyna q framework and provides a more sample efficient approach to learn the dialogue polices. the proposed agent consists of a planner trained on line with limited real user experience that can generate large amounts of simulated experience to supplement with limited real user experience and a policy model trained on these hybrid experiences. the effectiveness of our approach is validated on a movie booking task in both a simulation setting and a human in the loop setting.
building dnn acoustic models for large vocabulary speech recognition
deep neural networks dnns are now a central component of nearly all state of the art speech recognition systems. building neural network acoustic models requires several design decisions including network architecture size and training loss function. this paper offers an empirical investigation on which aspects of dnn acoustic model design are most important for speech recognition system performance. we report dnn classifier performance and final speech recognizer word error rates and compare dnns using several metrics to quantify factors influencing differences in task performance. our first set of experiments use the standard switchboard benchmark corpus which contains approximately 300 hours of conversational telephone speech. we compare standard dnns to convolutional networks and present the first experiments using locally connected untied neural networks for acoustic modeling. we additionally build systems on a corpus of 2 100 hours of training data by combining the switchboard and fisher corpora. this larger corpus allows us to more thoroughly examine performance of large dnn models with up to ten times more parameters than those typically used in speech recognition systems. our results suggest that a relatively simple dnn architecture and optimization technique produces strong results. these findings along with previous work help establish a set of best practices for building dnn hybrid speech recognition systems with maximum likelihood training. our experiments in dnn optimization additionally serve as a case study for training dnns with discriminative loss functions for speech tasks as well as dnn classifiers more generally.
deep recurrent neural networks for acoustic modelling
we present a novel deep recurrent neural network rnn model for acoustic modelling in automatic speech recognition asr . we term our contribution as a tc dnn blstm dnn model the model combines a deep neural network dnn with time convolution tc followed by a bidirectional long short term memory blstm and a final dnn. the first dnn acts as a feature processor to our model the blstm then generates a context from the sequence acoustic signal and the final dnn takes the context and models the posterior probabilities of the acoustic states. we achieve a 3.47 wer on the wall street journal wsj eval92 task or more than 8 relative improvement over the baseline dnn models.
regularizing rnns by stabilizing activations
we stabilize the activations of recurrent neural networks rnns by penalizing the squared distance between successive hidden states norms. this penalty term is an effective regularizer for rnns including lstms and irnns improving performance on character level language modeling and phoneme recognition and outperforming weight noise and dropout. we achieve competitive performance 18.6 per on the timit phoneme recognition task for rnns evaluated without beam search or an rnn transducer. with this penalty term irnn can achieve similar performance to lstm on language modeling although adding the penalty term to the lstm results in superior performance. our penalty term also prevents the exponential growth of irnn s activations outside of their training horizon allowing them to generalize to much longer sequences.
outrageously large neural networks the sparsely gated mixture of experts layer
the capacity of a neural network to absorb information is limited by its number of parameters. conditional computation where parts of the network are active on a per example basis has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. in practice however there are significant algorithmic and performance challenges. in this work we address these challenges and finally realize the promise of conditional computation achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern gpu clusters. we introduce a sparsely gated mixture of experts layer moe consisting of up to thousands of feed forward sub networks. a trainable gating network determines a sparse combination of these experts to use for each example. we apply the moe to the tasks of language modeling and machine translation where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. we present model architectures in which a moe with up to 137 billion parameters is applied convolutionally between stacked lstm layers. on large language modeling and machine translation benchmarks these models achieve significantly better results than state of the art at lower computational cost.
discourse based objectives for fast unsupervised sentence representation learning
this work presents a novel objective function for the unsupervised training of neural network sentence encoders. it exploits signals from paragraph level discourse coherence to train these models to understand text. our objective is purely discriminative allowing us to train models many times faster than was possible under prior methods and it yields models which perform well in extrinsic evaluations.
learning convolutional text representations for visual question answering
visual question answering is a recently proposed artificial intelligence task that requires a deep understanding of both images and texts. in deep learning images are typically modeled through convolutional neural networks and texts are typically modeled through recurrent neural networks. while the requirement for modeling images is similar to traditional computer vision tasks such as object recognition and image classification visual question answering raises a different need for textual representation as compared to other natural language processing tasks. in this work we perform a detailed analysis on natural language questions in visual question answering. based on the analysis we propose to rely on convolutional neural networks for learning textual representations. by exploring the various properties of convolutional neural networks specialized for text data such as width and depth we present our cnn inception gate model. we show that our model improves question representations and thus the overall accuracy of visual question answering models. we also show that the text representation requirement in visual question answering is more complicated and comprehensive than that in conventional natural language processing tasks making it a better task to evaluate textual representation methods. shallow models like fasttext which can obtain comparable results with deep learning models in tasks like text classification are not suitable in visual question answering.
learning phrase representations using rnn encoder decoder for statistical machine translation
in this paper we propose a novel neural network model called rnn encoder decoder that consists of two recurrent neural networks rnn . one rnn encodes a sequence of symbols into a fixed length vector representation and the other decodes the representation into another sequence of symbols. the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder decoder as an additional feature in the existing log linear model. qualitatively we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.
recurrent neural network training with dark knowledge transfer
recurrent neural networks rnns particularly long short term memory lstm have gained much attention in automatic speech recognition asr . although some successful stories have been reported training rnns remains highly challenging especially with limited training data. recent research found that a well trained model can be used as a teacher to train other child models by using the predictions generated by the teacher model as supervision. this knowledge transfer learning has been employed to train simple neural nets with a complex one so that the final performance can reach a level that is infeasible to obtain by regular training. in this paper we employ the knowledge transfer learning approach to train rnns precisely lstm using a deep neural network dnn model as the teacher. this is different from most of the existing research on knowledge transfer learning since the teacher dnn is assumed to be weaker than the child rnn however our experiments on an asr task showed that it works fairly well without applying any tricks on the learning scheme this approach can train rnns successfully even with limited training data.
long short term memory based recurrent neural network architectures for large vocabulary speech recognition
long short term memory lstm is a recurrent neural network rnn architecture that has been designed to address the vanishing and exploding gradient problems of conventional rnns. unlike feedforward neural networks rnns have cyclic connections making them powerful for modeling sequences. they have been successfully used for sequence labeling and sequence prediction tasks such as handwriting recognition language modeling phonetic labeling of acoustic frames. however in contrast to the deep neural networks the use of rnns in speech recognition has been limited to phone recognition in small scale tasks. in this paper we present novel lstm based rnn architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. we train and compare lstm rnn and dnn models at various numbers of parameters and configurations. we show that lstm models converge quickly and give state of the art speech recognition performance for relatively small sized models.
monitoring term drift based on semantic consistency in an evolving vector field
based on the aristotelian concept of potentiality vs. actuality allowing for the study of energy and dynamics in language we propose a field approach to lexical analysis. falling back on the distributional hypothesis to statistically model word meaning we used evolving fields as a metaphor to express time dependent changes in a vector space model by a combination of random indexing and evolving self organizing maps esom . to monitor semantic drifts within the observation period an experiment was carried out on the term space of a collection of 12.8 million amazon book reviews. for evaluation the semantic consistency of esom term clusters was compared with their respective neighbourhoods in wordnet and contrasted with distances among term vectors by random indexing. we found that at 0.05 level of significance the terms in the clusters showed a high level of semantic consistency. tracking the drift of distributional patterns in the term space across time periods we found that consistency decreased but not at a statistically significant level. our method is highly scalable with interpretations in philosophy.
towards better decoding and language model integration in sequence to sequence models
the recently proposed sequence to sequence seq2seq framework advocates replacing complex data processing pipelines such as an entire automatic speech recognition system with a single neural network trained in an end to end fashion. in this contribution we analyse an attention based seq2seq speech recognition system that directly transcribes recordings into characters. we observe two shortcomings overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used. we propose practical solutions to both problems achieving competitive speaker independent word error rates on the wall street journal dataset without separate language models we reach 10.6 wer while together with a trigram language model we reach 6.7 wer.
neural machine translation by jointly learning to align and translate
neural machine translation is a recently proposed approach to machine translation. unlike the traditional statistical machine translation the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. the models proposed recently for neural machine translation often belong to a family of encoder decoders and consists of an encoder that encodes a source sentence into a fixed length vector from which a decoder generates a translation. in this paper we conjecture that the use of a fixed length vector is a bottleneck in improving the performance of this basic encoder decoder architecture and propose to extend this by allowing a model to automatically soft search for parts of a source sentence that are relevant to predicting a target word without having to form these parts as a hard segment explicitly. with this new approach we achieve a translation performance comparable to the existing state of the art phrase based system on the task of english to french translation. furthermore qualitative analysis reveals that the soft alignments found by the model agree well with our intuition.
overcoming the curse of sentence length for neural machine translation using automatic segmentation
the authors of cho et al. 2014a have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences unlike existing phrase based translation systems. in this paper we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. once each segment has been independently translated by the neural machine translation model the translated clauses are concatenated to form a final translation. empirical results show a significant improvement in translation quality for long sentences.
transferring knowledge from a rnn to a dnn
deep neural network dnn acoustic models have yielded many state of the art results in automatic speech recognition asr tasks. more recently recurrent neural network rnn models have been shown to outperform dnns counterparts. however state of the art dnn and rnn models tend to be impractical to deploy on embedded systems with limited computational capacity. traditionally the approach for embedded platforms is to either train a small dnn directly or to train a small dnn that learns the output distribution of a large dnn. in this paper we utilize a state of the art rnn to transfer knowledge to small dnn. we use the rnn model to generate soft alignments and minimize the kullback leibler divergence against the small dnn. the small dnn trained on the soft rnn alignments achieved a 3.93 wer on the wall street journal wsj eval92 task compared to a baseline 4.54 wer or more than 13 relative improvement.
correlational neural networks
common representation learning crl wherein different descriptions or views of the data are embedded in a common subspace is receiving a lot of attention recently. two popular paradigms here are canonical correlation analysis cca based approaches and autoencoder ae based approaches. cca based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace. ae based methods learn a common representation by minimizing the error of reconstructing the two views. each of these approaches has its own advantages and disadvantages. for example while cca based approaches outperform ae based approaches for the task of transfer learning they are not as scalable as the latter. in this work we propose an ae based approach called correlational neural network corrnet that explicitly maximizes correlation among the views when projected to the common subspace. through a series of experiments we demonstrate that the proposed corrnet is better than the above mentioned approaches with respect to its ability to learn correlated common representations. further we employ corrnet for several cross language tasks and show that the representations learned using corrnet perform better than the ones learned using other state of the art approaches.
attention based models for speech recognition
recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in cluding machine translation handwriting synthesis and image caption gen eration. we extend the attention mechanism with features needed for speech recognition. we show that while an adaptation of the model used for machine translation in reaches a competitive 18.7 phoneme error rate per on the timit phoneme recognition task it can only be applied to utterances which are roughly as long as the ones it was trained on. we offer a qualitative explanation of this failure and propose a novel and generic method of adding location awareness to the attention mechanism to alleviate this issue. the new method yields a model that is robust to long inputs and achieves 18 per in single utterances and 20 in 10 times longer repeated utterances. finally we propose a change to the at tention mechanism that prevents it from concentrating too much on single frames which further reduces per to 17.6 level.
fast and accurate recurrent neural network acoustic models for speech recognition
we have recently shown that deep long short term memory lstm recurrent neural networks rnns outperform feed forward deep neural networks dnns as acoustic models for speech recognition. more recently we have shown that the performance of sequence trained context dependent cd hidden markov model hmm acoustic models using such lstm rnns can be equaled by sequence trained phone models initialized with connectionist temporal classification ctc . in this paper we present techniques that further improve performance of lstm rnn acoustic models for large vocabulary speech recognition. we show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. cd phone modeling leads to further improvements. we also present initial results for lstm rnn models outputting words directly.
listen attend and spell
we present listen attend and spell las a neural network that learns to transcribe speech utterances to characters. unlike traditional dnn hmm models this model learns all the components of a speech recognizer jointly. our system has two components a listener and a speller. the listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. the speller is an attention based recurrent network decoder that emits characters as outputs. the network produces character sequences without making any independence assumptions between the characters. this is the key improvement of las over previous end to end ctc models. on a subset of the google voice search task las achieves a word error rate wer of 14.1 without a dictionary or a language model and 10.3 with language model rescoring over the top 32 beams. by comparison the state of the art cldnn hmm model achieves a wer of 8.0 .
blackout speeding up recurrent neural network language models with very large vocabularies
we propose blackout an approximation algorithm to efficiently train massive recurrent neural network language models rnnlms with million word vocabularies. blackout is motivated by using a discriminative loss and we describe a new sampling strategy which significantly reduces computation while improving stability sample efficiency and rate of convergence. one way to understand blackout is to view it as an extension of the dropout strategy to the output layer wherein we use a discriminative training loss and a weighted sampling scheme. we also establish close connections between blackout importance sampling and noise contrastive estimation nce . our experiments on the recently released one billion word language modeling benchmark demonstrate scalability and accuracy of blackout we outperform the state of the art and achieve the lowest perplexity scores on this dataset. moreover unlike other established methods which typically require gpus or cpu clusters we show that a carefully implemented version of blackout requires only 1 10 days on a single machine to train a rnnlm with a million word vocabulary and billions of parameters on one billion words. although we describe blackout in the context of rnnlm training it can be used to any networks with large softmax output layers.
character based neural machine translation
neural machine translation mt has reached state of the art results. however one of the main challenges that neural mt still faces is dealing with very large vocabularies and morphologically rich languages. in this paper we propose a neural mt system using character based embeddings in combination with convolutional and highway layers to replace the standard lookup based word representations. the resulting unlimited vocabulary and affix aware source word embeddings are tested in a state of the art neural mt based on an attention based bidirectional recurrent neural network. the proposed mt scheme provides improved results even when the source language is not morphologically rich. improvements up to 3 bleu points are obtained in the german english wmt task.
a latent variable recurrent neural network for discourse relation language models
this paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and possibly latent discourse relations between adjacent sentences. a recurrent neural network generates individual words thus reaping the benefits of discriminatively trained vector representations. the discourse relations are represented with a latent variable which can be predicted or marginalized depending on the task. the resulting model can therefore employ a training objective that includes not only discourse relation classification but also word prediction. as a result it outperforms state of the art alternatives for two tasks implicit discourse relation classification in the penn discourse treebank and dialog act classification in the switchboard corpus. furthermore by marginalizing over latent discourse relations at test time we obtain a discourse informed language model which improves over a strong lstm baseline.
multi task recurrent model for speech and speaker recognition
although highly correlated speech and speaker recognition have been regarded as two independent tasks and studied by two communities. this is certainly not the way that people behave we decipher both speech content and speaker traits at the same time. this paper presents a unified model to perform speech and speaker recognition simultaneously and altogether. the model is based on a unified neural network where the output of one task is fed to the input of the other leading to a multi task recurrent network. experiments show that the joint model outperforms the task specific models on both the two tasks.
hierarchical memory networks
memory networks are neural networks with an explicit memory component that can be both read and written to by the network. the memory is often addressed in a soft way using a softmax function making end to end training with backpropagation possible. however this is not computationally scalable for applications which require the network to read from extremely large memories. on the other hand it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. in this paper we explore a form of hierarchical memory network which can be considered as a hybrid between hard and soft attention memory networks. the memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory while also being easier to train than hard attention over a flat memory. specifically we propose to incorporate maximum inner product search mips in the training and inference procedures for our hierarchical memory network. we explore the use of various state of the art approximate mips techniques and report results on simplequestions a challenging large scale factoid question answering task.
sequence to sequence learning as beam search optimization
sequence to sequence seq2seq modeling has rapidly become an important general purpose nlp tool that has proven effective for many text generation and sequence labeling tasks. seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local next word distributions. in this work we introduce a model and beam search training scheme based on the work of daume iii and marcu 2005 that extends seq2seq to learn global sequence scores. this structured approach avoids classical biases associated with local training and unifies the training loss with the test time usage while preserving the proven model architecture of seq2seq and its efficient training approach. we show that our system outperforms a highly optimized attention based seq2seq system and other baselines on three different sequence to sequence tasks word ordering parsing and machine translation.
grounded recurrent neural networks
in this work we present the grounded recurrent neural network grnn a recurrent neural network architecture for multi label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state we call this process grounding . the approach is particularly well suited for extracting large numbers of concepts from text. we apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text. using a publicly available dataset derived from intensive care units we learn to label a patient s diagnoses and procedures from their discharge summary. our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines.
latent intention dialogue models
developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long term goals of machine learning research. traditional approaches either rely on hand crafting a small state action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. in this paper we propose a latent intention dialogue model lidm that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. in a goal oriented dialogue scenario these latent intentions can be interpreted as actions guiding the generation of machine responses which can be further refined autonomously by reinforcement learning. the experimental evaluation of lidm shows that the model out performs published benchmarks for both corpus based and human evaluation demonstrating the effectiveness of discrete latent variable models for learning goal oriented dialogues.
transfer learning for speech recognition on a budget
end to end training of automated speech recognition asr systems requires massive data and compute resources. we explore transfer learning based on model adaptation as an approach for training asr models under constrained gpu memory throughput and training data. we conduct several systematic experiments adapting a wav2letter convolutional neural network originally trained for english asr to the german language. we show that this technique allows faster training on consumer grade resources while requiring less training data in order to achieve the same accuracy thereby lowering the cost of training asr models in other languages. model introspection revealed that small adaptations to the network s weights were sufficient for good performance especially for inner layers.
optimizing expected word error rate via sampling for speech recognition
state level minimum bayes risk smbr training has become the de facto standard for sequence level training of speech recognition acoustic models. it has an elegant formulation using the expectation semiring and gives large improvements in word error rate wer over models trained solely using cross entropy ce or connectionist temporal classification ctc . smbr training optimizes the expected number of frames at which the reference and hypothesized acoustic states differ. it may be preferable to optimize the expected wer but wer does not interact well with the expectation semiring and previous approaches based on computing expected wer exactly involve expanding the lattices used during training. in this paper we show how to perform optimization of the expected wer by sampling paths from the lattices used during conventional smbr training. the gradient of the expected wer is itself an expectation and so may be approximated using monte carlo sampling. we show experimentally that optimizing wer during acoustic model training gives 5 relative improvement in wer over a well tuned smbr baseline on a 2 channel query recognition task google home .
neural networks compression for language modeling
in this paper we consider several compression techniques for the language modeling problem based on recurrent neural networks rnns . it is known that conventional rnns e.g lstm based networks in language modeling are characterized with either high space complexity or substantial inference time. this problem is especially crucial for mobile applications in which the constant interaction with the remote server is inappropriate. by using the penn treebank ptb dataset we compare pruning quantization low rank factorization tensor train decomposition for lstm networks in terms of model size and suitability for fast inference.
avoiding your teacher s mistakes training neural networks with controlled weak supervision
training deep neural networks requires massive amounts of training data but for many tasks only limited labeled data is available. this makes weak supervision attractive using weak or noisy signals like the output of heuristic methods or user click through data for training. in a semi supervised setting we can use a large set of data with weak labels to pretrain a neural network and then fine tune the parameters with a small amount of data with true labels. this feels intuitively sub optimal as these two independent stages leave the model unaware about the varying label quality. what if we could somehow inform the model about the label quality in this paper we propose a semi supervised learning method where we train two neural networks in a multi task fashion a target network and a confidence network . the target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. we propose to weight the gradient updates to the target network using the scores provided by the second confidence network which is trained on a small amount of supervised data. thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model. we evaluate our learning strategy on two different tasks document ranking and sentiment classification. the results demonstrate that our approach not only enhances the performance compared to the baselines but also speeds up the learning process from weak labels.
uncertainty estimates for efficient neural network based dialogue policy optimisation
in statistical dialogue management the dialogue manager learns a policy that maps a belief state to an action for the system to perform. efficient exploration is key to successful policy optimisation. current deep reinforcement learning methods are very promising but rely on epsilon greedy exploration thus subjecting the user to a random choice of action during learning. alternative approaches such as gaussian process sarsa gpsarsa estimate uncertainties and are sample efficient leading to better user experience but on the expense of a greater computational complexity. this paper examines approaches to extract uncertainty estimates from deep q networks dqn in the context of dialogue management. we perform an extensive benchmark of deep bayesian methods to extract uncertainty estimates namely bayes by backprop dropout its concrete variation bootstrapped ensemble and alpha divergences combining it with dqn algorithm.
on extended long short term memory and dependent bidirectional recurrent neural network
in this work we investigate the memory capability of recurrent neural networks rnns where this capability is defined as a function that maps an element in a sequence to the current output. we first analyze the system function of a recurrent neural network rnn cell and provide analytical results for three rnns. they are the simple recurrent neural network srn the long short term memory lstm and the gated recurrent unit gru . based on the analysis we propose a new design to extend the memory length of a cell and call it the extended long short term memory elstm . next we present a dependent bidirectional recurrent neural network dbrnn for the sequence in sequence out siso problem which is more robust to previous erroneous predictions. extensive experiments are carried out on different language tasks to demonstrate the superiority of our proposed elstm and dbrnn solutions.
learning to answer questions from image using convolutional neural network
in this paper we propose to employ the convolutional neural network cnn for the image question answering qa . our proposed cnn provides an end to end framework with convolutional architectures for learning not only the image and question representations but also their inter modal interactions to produce the answer. more specifically our model consists of three cnns one image cnn to encode the image content one sentence cnn to compose the words of the question and one multimodal convolution layer to learn their joint representation for the classification in the space of candidate answer words. we demonstrate the efficacy of our proposed model on the daquar and coco qa datasets which are two benchmark datasets for the image qa with the performances significantly outperforming the state of the art.
stacked attention networks for image question answering
this paper presents stacked attention networks sans that learn to answer natural language questions from images. sans use semantic representation of a question as query to search for the regions in an image that are related to the answer. we argue that image question answering qa often requires multiple steps of reasoning. thus we develop a multiple layer san in which we query an image multiple times to infer the answer progressively. experiments conducted on four image qa data sets demonstrate that the proposed sans significantly outperform previous state of the art approaches. the visualization of the attention layers illustrates the progress that the san locates the relevant visual clues that lead to the answer of the question layer by layer.
neural module networks
visual question answering is fundamentally compositional in nature a question like where is the dog shares substructure with questions like what color is the dog and where is the cat this paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. we describe a procedure for constructing and learning neural module networks which compose collections of jointly trained neural modules into deep networks for question answering. our approach decomposes questions into their linguistic substructures and uses these structures to dynamically instantiate modular networks with reusable components for recognizing dogs classifying colors etc. . the resulting compound networks are jointly trained. we evaluate our approach on two challenging datasets for visual question answering achieving state of the art results on both the vqa natural image dataset and a new dataset of complex questions about abstract shapes.
symbol grounding association in multimodal sequences with missing elements
in this paper we extend a symbolic association framework for being able to handle missing elements in multimodal sequences. the general scope of the work is the symbolic associations of object word mappings as it happens in language development in infants. in other words two different representations of the same abstract concepts can associate in both directions. this scenario has been long interested in artificial intelligence psychology and neuroscience. in this work we extend a recent approach for multimodal sequences visual and audio to also cope with missing elements in one or both modalities. our method uses two parallel long short term memories lstms with a learning rule based on em algorithm. it aligns both lstm outputs via dynamic time warping dtw . we propose to include an extra step for the combination with the max operation for exploiting the common elements between both sequences. the motivation behind is that the combination acts as a condition selector for choosing the best representation from both lstms. we evaluated the proposed extension in the following scenarios missing elements in one modality visual or audio and missing elements in both modalities visual and sound . the performance of our extension reaches better results than the original model and similar results to individual lstm trained in each modality.
using trusted data to train deep networks on labels corrupted by severe noise
the growing importance of massive datasets with the advent of deep learning makes robustness to label noise a critical property for classifiers to have. sources of label noise include automatic labeling for large datasets non expert labeling and label corruption by data poisoning adversaries. in the latter case corruptions may be arbitrarily bad even so bad that a classifier predicts the wrong labels with high confidence. to protect against such sources of noise we leverage the fact that a small set of clean labels is often easy to procure. we demonstrate that robustness to label noise up to severe strengths can be achieved by using a set of trusted data with clean labels and propose a loss correction that utilizes trusted examples in a data efficient manner to mitigate the effects of label noise on deep neural network classifiers. across vision and natural language processing tasks we experiment with various label noises at several strengths and show that our method significantly outperforms existing methods.
describing multimedia content using attention based encoder decoder networks
whereas deep neural networks were first mostly used for classification tasks they are rapidly expanding in the realm of structured output problems where the observed target is composed of multiple random variables that have a rich joint distribution given the input. we focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. we describe systems that learn to attend to different places in the input for each element of the output for a variety of tasks machine translation image caption generation video clip description and speech recognition. all these systems are based on a shared set of building blocks gated recurrent neural networks and convolutional neural networks along with trained attention mechanisms. we report on experimental results with these systems showing impressively good performance and the advantage of the attention mechanism.
multilingual image description with neural sequence models
in this paper we present an approach to multi language image description bringing together insights from neural machine translation and neural image description. to create a description of an image for a given target language our sequence generation models condition on feature vectors from the image the description from the source language and or a multimodal vector computed over the image and a description in the source language. in image description experiments on the iapr tc12 dataset of images aligned with english and german sentences we find significant and substantial improvements in bleu4 and meteor scores for models trained over multiple languages compared to a monolingual baseline.
deep embedding for spatial role labeling
this paper introduces the visually informed embedding of word view a continuous vector representation for a word extracted from a deep neural model trained using the microsoft coco data set to forecast the spatial arrangements between visual objects given a textual description. the model is composed of a deep multilayer perceptron mlp stacked on the top of a long short term memory lstm network the latter being preceded by an embedding layer. the view is applied to transferring multimodal background knowledge to spatial role labeling sprl algorithms which recognize spatial relations between objects mentioned in the text. this work also contributes with a new method to select complementary features and a fine tuning method for mlp that improves the f1 measure in classifying the words into spatial roles. the view is evaluated with the task 3 of semeval 2013 benchmark data set spaceeval.
image to markup generation with coarse to fine attention
we present a neural encoder decoder model to convert images into presentational markup based on a scalable coarse to fine attention mechanism. our method is evaluated in the context of image to latex generation and we introduce a new dataset of real world rendered mathematical expressions paired with latex markup. we show that unlike neural ocr techniques using ctc based models attention based approaches can tackle this non standard ocr task. our approach outperforms classical mathematical ocr systems by a large margin on in domain rendered data and with pretraining also performs well on out of domain handwritten data. to reduce the inference complexity associated with the attention based approaches we introduce a new coarse to fine attention layer that selects a support region before applying attention.
teaching machines to code neural markup generation with visual attention
we present a deep recurrent neural network model with soft visual attention that learns to generate latex markup of real world math formulas given their images. applying neural sequence generation techniques that have been very successful in the fields of machine translation and image handwriting speech captioning recognition transcription and synthesis we construct an image to markup model that learns to produce syntactically and semantically correct latex markup code of over 150 words long and achieves a bleu score of 89 the best reported so far for the im2latex problem. we also visually demonstrate that the model learns to scan the image left right up down much as a human would read it.
evolution in groups a deeper look at synaptic cluster driven evolution of deep neural networks
a promising paradigm for achieving highly efficient deep neural networks is the idea of evolutionary deep intelligence which mimics biological evolution processes to progressively synthesize more efficient networks. a crucial design factor in evolutionary deep intelligence is the genetic encoding scheme used to simulate heredity and determine the architectures of offspring networks. in this study we take a deeper look at the notion of synaptic cluster driven evolution of deep neural networks which guides the evolution process towards the formation of a highly sparse set of synaptic clusters in offspring networks. utilizing a synaptic cluster driven genetic encoding the probabilistic encoding of synaptic traits considers not only individual synaptic properties but also inter synaptic relationships within a deep neural network. this process results in highly sparse offspring networks which are particularly tailored for parallel computational devices such as gpus and deep neural network accelerator chips. comprehensive experimental results using four well known deep neural network architectures lenet 5 alexnet resnet 56 and detectnet on two different tasks object categorization and object detection demonstrate the efficiency of the proposed method. cluster driven genetic encoding scheme synthesizes networks that can achieve state of the art performance with significantly smaller number of synapses than that of the original ancestor network. sim 125 fold decrease in synapses for mnist . furthermore the improved cluster efficiency in the generated offspring networks sim 9.71 fold decrease in clusters for mnist and a sim 8.16 fold decrease in clusters for kitti is particularly useful for accelerated performance on parallel computing hardware architectures such as those in gpus and deep neural network accelerator chips.
mesh learning for classifying cognitive processes
a relatively recent advance in cognitive neuroscience has been multi voxel pattern analysis mvpa which enables researchers to decode brain states and or the type of information represented in the brain during a cognitive operation. mvpa methods utilize machine learning algorithms to distinguish among types of information or cognitive states represented in the brain based on distributed patterns of neural activity. in the current investigation we propose a new approach for representation of neural data for pattern analysis namely a mesh learning model. in this approach at each time instant a star mesh is formed around each voxel such that the voxel corresponding to the center node is surrounded by its p nearest neighbors. the arc weights of each mesh are estimated from the voxel intensity values by least squares method. the estimated arc weights of all the meshes called mesh arc descriptors mads are then used to train a classifier such as neural networks k nearest neighbor na ive bayes and support vector machines. the proposed mesh model was tested on neuroimaging data acquired via functional magnetic resonance imaging fmri during a recognition memory experiment using categorized word lists employing a previously established experimental paradigm oztekin badre 2011 . results suggest that the proposed mesh learning approach can provide an effective algorithm for pattern analysis of brain activity during cognitive processing.
synthesizing deep neural network architectures using biological synaptic strength distributions
in this work we perform an exploratory study on synthesizing deep neural networks using biological synaptic strength distributions and the potential influence of different distributions on modelling performance particularly for the scenario associated with small data sets. surprisingly a cnn with convolutional layer synaptic strengths drawn from biologically inspired distributions such as log normal or correlated center surround distributions performed relatively well suggesting a possibility for designing deep neural network architectures that do not require many data samples to learn and can sidestep current training procedures while maintaining or boosting modelling performance.
a pso and pattern search based memetic algorithm for svms parameters optimization
addressing the issue of svms parameters optimization this study proposes an efficient memetic algorithm based on particle swarm optimization algorithm pso and pattern search ps . in the proposed memetic algorithm pso is responsible for exploration of the search space and the detection of the potential regions with optimum solutions while pattern search ps is used to produce an effective exploitation on the potential regions obtained by pso. moreover a novel probabilistic selection strategy is proposed to select the appropriate individuals among the current population to undergo local refinement keeping a well balance between exploration and exploitation. experimental results confirm that the local refinement with ps and our proposed selection strategy are effective and finally demonstrate effectiveness and robustness of the proposed pso ps based ma for svms parameters optimization.
density estimation using real nvp
unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. specifically designing models with tractable learning sampling inference and evaluation is crucial in solving this task. we extend the space of such models using real valued non volume preserving real nvp transformations a set of powerful invertible and learnable transformations resulting in an unsupervised learning algorithm with exact log likelihood computation exact sampling exact inference of latent variables and an interpretable latent space. we demonstrate its ability to model natural images on four datasets through sampling log likelihood evaluation and latent variable manipulations.
evolution strategies as a scalable alternative to reinforcement learning
we explore the use of evolution strategies es a class of black box optimization algorithms as an alternative to popular mdp based rl techniques such as q learning and policy gradients. experiments on mujoco and atari show that es is a viable solution strategy that scales extremely well with the number of cpus available by using a novel communication strategy based on common random numbers our es implementation only needs to communicate scalars making it possible to scale to over a thousand parallel workers. this allows us to solve 3d humanoid walking in 10 minutes and obtain competitive results on most atari games after one hour of training. in addition we highlight several advantages of es as a black box optimization technique it is invariant to action frequency and delayed rewards tolerant of extremely long horizons and does not need temporal discounting or value function approximation.
qmdp net deep learning for planning under partial observability
this paper introduces the qmdp net a neural network architecture for planning under partial observability. the qmdp net combines the strengths of model free learning and model based planning. it is a recurrent policy network but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model thus embedding the solution structure of planning in a network learning architecture. the qmdp net is fully differentiable and allows for end to end training. we train a qmdp net on different tasks so that it can generalize to new ones in the parameterized task set and transfer to other similar tasks beyond the set. in preliminary experiments qmdp net showed strong performance on several robotic tasks in simulation. interestingly while qmdp net encodes the qmdp algorithm it sometimes outperforms the qmdp algorithm in the experiments as a result of end to end learning.
treeqn and atreec differentiable tree structured models for deep reinforcement learning
combining deep model free reinforcement learning with on line planning is a promising approach to building on the successes of deep rl. on line planning with look ahead trees has proven successful in environments where transition models are known a priori. however in complex environments where transition models need to be learned from data the deficiencies of learned models have limited their utility for planning. to address these challenges we propose treeqn a differentiable recursive tree structured model that serves as a drop in replacement for any value function network in deep rl with discrete actions. treeqn dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state values using a tree backup to estimate q values. we also propose atreec an actor critic variant that augments treeqn with a softmax layer to form a stochastic policy network. both approaches are trained end to end such that the learned model is optimised for its actual use in the tree. we show that treeqn and atreec outperform n step dqn and a2c on a box pushing task as well as n step dqn and value prediction networks oh et al. 2017 on multiple atari games. furthermore we present ablation studies that demonstrate the effect of different auxiliary losses on learning transition models.
sparse attentive backtracking long range credit assignment in recurrent networks
a major drawback of backpropagation through time bptt is the difficulty of learning long term dependencies coming from having to propagate credit information backwards through every single step of the forward computation. this makes bptt both computationally impractical and biologically implausible. for this reason full backpropagation through time is rarely used on long sequences and truncated backpropagation through time is used as a heuristic. however this usually leads to biased estimates of the gradient in which longer term dependencies are ignored. addressing this issue we propose an alternative algorithm sparse attentive backtracking which might also be related to principles used by brains to learn long term dependencies. sparse attentive backtracking learns an attention mechanism over the hidden states of the past and selectively backpropagates through paths with high attention weights. this allows the model to learn long term dependencies while only backtracking for a small number of time steps not just from the recent past but also from attended relevant past states.
stochastic deep learning in memristive networks
we study the performance of stochastically trained deep neural networks dnns whose synaptic weights are implemented using emerging memristive devices that exhibit limited dynamic range resolution and variability in their programming characteristics. we show that a key device parameter to optimize the learning efficiency of dnns is the variability in its programming characteristics. dnns with such memristive synapses even with dynamic range as low as 15 and only 32 discrete levels when trained based on stochastic updates suffer less than 3 loss in accuracy compared to floating point software baseline. we also study the performance of stochastic memristive dnns when used as inference engines with noise corrupted data and find that if the device variability can be minimized the relative degradation in performance for the stochastic dnn is better than that of the software baseline. hence our study presents a new optimization corner for memristive devices for building large noise immune deep learning systems.
pso mismo modeling strategy for multi step ahead time series prediction
multi step ahead time series prediction is one of the most challenging research topics in the field of time series modeling and prediction and is continually under research. recently the multiple input several multiple outputs mismo modeling strategy has been proposed as a promising alternative for multi step ahead time series prediction exhibiting advantages compared with the two currently dominating strategies the iterated and the direct strategies. built on the established mismo strategy this study proposes a particle swarm optimization pso based mismo modeling strategy which is capable of determining the number of sub models in a self adaptive mode with varying prediction horizons. rather than deriving crisp divides with equal size s prediction horizons from the established mismo the proposed pso mismo strategy implemented with neural networks employs a heuristic to create flexible divides with varying sizes of prediction horizons and to generate corresponding sub models providing considerable flexibility in model construction which has been validated with simulated and real datasets.
norm based capacity control in neural networks
we investigate the capacity convexity and characterization of a general family of norm constrained feed forward networks.
improving the performance of neural networks in regression tasks using drawering
the method presented extends a given regression neural network to make its performance improve. the modification affects the learning procedure only hence the extension may be easily omitted during evaluation without any change in prediction. it means that the modified model may be evaluated as quickly as the original one but tends to perform better. this improvement is possible because the modification gives better expressive power provides better behaved gradients and works as a regularization. the knowledge gained by the temporarily extended neural network is contained in the parameters shared with the original neural network. the only cost is an increase in learning time.
learning unbiased features
a key element in transfer learning is representation learning if representations can be developed that expose the relevant factors underlying the data then new tasks and domains can be learned readily based on mappings of these salient factors. we propose that an important aim for these representations are to be unbiased. different forms of representation learning can be derived from alternative definitions of unwanted bias e.g. bias to particular tasks domains or irrelevant underlying data dimensions. one very useful approach to estimating the amount of bias in a representation comes from maximum mean discrepancy mmd 5 a measure of distance between probability distributions. we are not the first to suggest that mmd can be a useful criterion in developing representations that apply across multiple domains or tasks 1 . however in this paper we describe a number of novel applications of this criterion that we have devised all based on the idea of developing unbiased representations. these formulations include a standard domain adaptation framework a method of learning invariant representations an approach based on noise insensitive autoencoders and a novel form of generative model.
compatible value gradients for reinforcement learning of continuous deep policies
this paper proposes gprop a deep reinforcement learning algorithm for continuous policies with compatible function approximation. the algorithm is based on two innovations. firstly we present a temporal difference based method for learning the gradient of the value function. secondly we present the deviator actor critic dac model which comprises three neural networks that estimate the value function its gradient and determine the actor s policy respectively. we evaluate gprop on two challenging tasks a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients and the octopus arm a challenging reinforcement learning benchmark. gprop is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.
learning dynamic boltzmann machines with spike timing dependent plasticity
we propose a particularly structured boltzmann machine which we refer to as a dynamic boltzmann machine dybm as a stochastic model of a multi dimensional time series. the dybm can have infinitely many layers of units but allows exact and efficient inference and learning when its parameters have a proposed structure. this proposed structure is motivated by postulates and observations from biological neural networks that the synaptic weight is strengthened or weakened depending on the timing of spikes i.e. spike timing dependent plasticity or stdp . we show that the learning rule of updating the parameters of the dybm in the direction of maximizing the likelihood of given time series can be interpreted as stdp with long term potentiation and long term depression. the learning rule has a guarantee of convergence and can be performed in a distributed matter i.e. local in space with limited memory i.e. local in time .
gated graph sequence neural networks
graph structured data appears frequently in domains including chemistry natural language semantics social networks and knowledge bases. in this work we study feature learning techniques for graph structured inputs. our starting point is previous work on graph neural networks scarselli et al. 2009 which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. the result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence based models e.g. lstms when the problem is graph structured. we demonstrate the capabilities on some simple ai babi and graph algorithm learning tasks. we then show it achieves state of the art performance on a problem from program verification in which subgraphs need to be matched to abstract data structures.
deep reinforcement learning in large discrete action spaces
being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. recommender systems industrial plants and language models are only some of the many real world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. an ability to generalize over the set of actions as well as sub linear complexity relative to the size of the set are both necessary to handle such tasks. current approaches are not able to provide both of these which motivates the work in this paper. our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. additionally approximate nearest neighbor methods allow for logarithmic time lookup complexity relative to the number of actions which is necessary for time wise tractable training. this combined approach allows reinforcement learning methods to be applied to large scale learning problems previously intractable with current methods. we demonstrate our algorithm s abilities on a series of tasks having up to one million actions.
value iteration networks
we introduce the value iteration network vin a fully differentiable neural network with a planning module embedded within. vins can learn to plan and are suitable for predicting outcomes that involve planning based reasoning such as policies for reinforcement learning. key to our approach is a novel differentiable approximation of the value iteration algorithm which can be represented as a convolutional neural network and trained end to end using standard backpropagation. we evaluate vin based policies on discrete and continuous path planning domains and on a natural language based search task. we show that by learning an explicit planning computation vin policies generalize better to new unseen domains.
recurrent orthogonal networks and long memory tasks
although rnns have been shown to be powerful tools for processing sequential data finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. in this work we carefully analyze two synthetic datasets originally outlined in hochreiter and schmidhuber 1997 which are used to evaluate the ability of rnns to store information over many time steps. we explicitly construct rnn solutions to these problems and using these constructions illuminate both the problems themselves and the way in which rnns store different types of information in their hidden states. these constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.
learning values across many orders of magnitude
most learning algorithms are not invariant to the scale of the function that is being approximated. we propose to adaptively normalize the targets used in learning. this is useful in value based reinforcement learning where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. our main motivation is prior work on learning to play atari games where the rewards were all clipped to a predetermined range. this clipping facilitates learning across many different games with a single learning algorithm but a clipped reward function can result in qualitatively different behavior. using the adaptive normalization we can remove this domain specific heuristic without diminishing overall performance.
genetic architect discovering genomic structure with learned neural architectures
each human genome is a 3 billion base pair set of encoding instructions. decoding the genome using deep learning fundamentally differs from most tasks as we do not know the full structure of the data and therefore cannot design architectures to suit it. as such architectures that fit the structure of genomics should be learned not prescribed. here we develop a novel search algorithm applicable across domains that discovers an optimal architecture which simultaneously learns general genomic patterns and identifies the most important sequence motifs in predicting functional genomic outcomes. the architectures we find using this algorithm succeed at using only rna expression data to predict gene regulatory structure learn human interpretable visualizations of key sequence motifs and surpass state of the art results on benchmark genomics challenges.
deep successor reinforcement learning
learning robust value functions given raw observations and rewards is now possible with model free and model based deep reinforcement learning algorithms. there is a third alternative called successor representations sr which decomposes the value function into two components a reward predictor and a successor map. the successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. the value function of a state can be computed as the inner product between the successor map and the reward weights. in this paper we present dsr which generalizes sr within an end to end deep reinforcement learning framework. dsr has several appealing properties including increased sensitivity to distal reward changes due to factorization of reward and world dynamics and the ability to extract bottleneck states subgoals given successor maps trained under a random policy. we show the efficacy of our approach on two diverse environments given raw pixel observations simple grid world domains mazebase and the doom game engine.
rl 2 fast reinforcement learning via slow reinforcement learning
deep reinforcement learning deep rl has been successful in learning sophisticated behaviors automatically however the learning process requires a huge number of trials. in contrast animals can learn new tasks in just a few trials benefiting from their prior knowledge about the world. this paper seeks to bridge this gap. rather than designing a fast reinforcement learning algorithm we propose to represent it as a recurrent neural network rnn and learn it from data. in our proposed method rl 2 the algorithm is encoded in the weights of the rnn which are learned slowly through a general purpose slow rl algorithm. the rnn receives all information a typical rl algorithm would receive including observations actions rewards and termination flags and it retains its state across episodes in a given markov decision process mdp . the activations of the rnn store the state of the fast rl algorithm on the current previously unseen mdp. we evaluate rl 2 experimentally on both small scale and large scale problems. on the small scale side we train it to solve randomly generated multi arm bandit problems and finite mdps. after rl 2 is trained its performance on new mdps is close to human designed algorithms with optimality guarantees. on the large scale side we test rl 2 on a vision based navigation task and show that it scales up to high dimensional problems.
capacity and trainability in recurrent neural networks
two potential bottlenecks on the expressiveness of recurrent neural networks rnns are their ability to store information about the task in their parameters and to store information about the input history in their units. we show experimentally that all common rnn architectures achieve nearly the same per task and per unit capacity bounds with careful training for a variety of tasks and stacking depths. they can store an amount of task information which is linear in the number of parameters and is approximately 5 bits per parameter. they can additionally store approximately one real number from their input history per hidden unit. we further find that for several tasks it is the per task parameter capacity bound that determines performance. these results suggest that many previous results comparing rnn architectures are driven primarily by differences in training effectiveness rather than differences in capacity. supporting this observation we compare training difficulty for several architectures and show that vanilla rnns are far more difficult to train yet have slightly higher capacity. finally we propose two novel rnn architectures one of which is easier to train than the lstm or gru for deeply stacked architectures.
causal regularization
in application domains such as healthcare we want accurate predictive models that are also causally interpretable. in pursuit of such models we propose a causal regularizer to steer predictive models towards causally interpretable solutions and theoretically study its properties. in a large scale analysis of electronic health records ehr our causally regularized model outperforms its l1 regularized counterpart in causal accuracy and is competitive in predictive performance. we perform non linear causality analysis by causally regularizing a special neural network architecture. we also show that the proposed causal regularizer can be used together with neural representation learning algorithms to yield up to 20 improvement over multilayer perceptron in detecting multivariate causation a situation common in healthcare where many causal factors should occur simultaneously to have an effect on the target variable.
on the behavior of convolutional nets for feature extraction
deep neural networks are representation learning techniques. during training a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning. extracting the descriptive language coded within a trained cnn model in the case of image data and reusing it for other purposes is a field of interest as it provides access to the visual descriptors previously learnt by the cnn after processing millions of images without requiring an expensive training phase. contributions to this field commonly known as feature representation transfer or transfer learning have been purely empirical so far extracting all cnn features from a single layer close to the output and testing their performance by feeding them to a classifier. this approach has provided consistent results although its relevance is limited to classification tasks. in a completely different approach in this paper we statistically measure the discriminative power of every single feature found within a deep cnn when used for characterizing every class of 11 datasets. we seek to provide new insights into the behavior of cnn features particularly the ones from convolutional layers as this can be relevant for their application to knowledge representation and reasoning. our results confirm that low and middle level features may behave differently to high level features but only under certain conditions. we find that all cnn features can be used for knowledge representation purposes both by their presence or by their absence doubling the information a single cnn feature may provide. we also study how much noise these features may include and propose a thresholding approach to discard most of it. all these insights have a direct application to the generation of cnn embedding spaces.
flow gan combining maximum likelihood and adversarial learning in generative models
adversarial learning of probabilistic models has recently emerged as a promising alternative to maximum likelihood. implicit models such as generative adversarial networks gan often generate better samples compared to explicit models trained by maximum likelihood. yet gans sidestep the characterization of an explicit density which makes quantitative evaluations challenging. to bridge this gap we propose flow gans a generative adversarial network for which we can perform exact likelihood evaluation thus supporting both adversarial and maximum likelihood training. when trained adversarially flow gans generate high quality samples but attain extremely poor log likelihood scores inferior even to a mixture model memorizing the training data the opposite is true when trained by maximum likelihood. results on mnist and cifar 10 demonstrate that hybrid training can attain high held out likelihoods while retaining visual fidelity in the generated samples.
filtering variational objectives
when used as a surrogate objective for maximum likelihood estimation in latent variable models the evidence lower bound elbo produces state of the art results. inspired by this we consider the extension of the elbo to a family of lower bounds defined by a particle filter s estimator of the marginal likelihood the filtering variational objectives fivos . fivos take the same arguments as the elbo but can exploit a model s sequential structure to form tighter bounds. we present results that relate the tightness of fivo s bound to the variance of the particle filter s estimator by considering the generic case of bounds defined as log transformed likelihood estimators. experimentally we show that training with fivo results in substantial improvements over training the same model architecture with the elbo on sequential data.
kernel implicit variational inference
recent progress in variational inference has paid much attention to the flexibility of variational posteriors. one promising direction is to use implicit distributions i.e. distributions without tractable densities as the variational posterior. however existing methods on implicit posteriors still face challenges of noisy estimation and computational infeasibility when applied to models with high dimensional latent variables. in this paper we present a new approach named kernel implicit variational inference that addresses these challenges. as far as we know for the first time implicit variational inference is successfully applied to bayesian neural networks which shows promising results on both regression and classification tasks.
non markovian control with gated end to end memory policy networks
partially observable environments present an important open challenge in the domain of sequential control learning with delayed rewards. despite numerous attempts during the two last decades the majority of reinforcement learning algorithms and associated approximate models applied to this context still assume markovian state transitions. in this paper we explore the use of a recently proposed attention based model the gated end to end memory network for sequential control. we call the resulting model the gated end to end memory policy network. more precisely we use a model free value based algorithm to learn policies for partially observed domains using this memory enhanced neural network. this model is end to end learnable and it features unbounded memory. indeed because of its attention mechanism and associated non parametric memory the proposed model allows us to define an attention mechanism over the observation stream unlike recurrent models. we show encouraging results that illustrate the capability of our attention based model in the context of the continuous state non stationary control problem of stock trading. we also present an openai gym environment for simulated stock exchange and explain its relevance as a benchmark for the field of non markovian decision process learning.
automated problem identification regression vs classification via evolutionary deep networks
regression or classification this is perhaps the most basic question faced when tackling a new supervised learning problem. we present an evolutionary deep learning edl algorithm that automatically solves this by identifying the question type with high accuracy along with a proposed deep architecture. typically a significant amount of human insight and preparation is required prior to executing machine learning algorithms. for example when creating deep neural networks the number of parameters must be selected in advance and furthermore a lot of these choices are made based upon pre existing knowledge of the data such as the use of a categorical cross entropy loss function. humans are able to study a dataset and decide whether it represents a classification or a regression problem and consequently make decisions which will be applied to the execution of the neural network. we propose the automated problem identification api algorithm which uses an evolutionary algorithm interface to tensorflow to manipulate a deep neural network to decide if a dataset represents a classification or a regression problem. we test api on 16 different classification regression and sentiment analysis datasets with up to 10 000 features and up to 17 000 unique target values. api achieves an average accuracy of 96.3 in identifying the problem type without hardcoding any insights about the general characteristics of regression or classification problems. for example api successfully identifies classification problems even with 1000 target values. furthermore the algorithm recommends which loss function to use and also recommends a neural network architecture. our work is therefore a step towards fully automated machine learning.
a simple neural attentive meta learner
deep neural networks excel in regimes with large amounts of data but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. in response recent work in meta learning proposes training a meta learner on a distribution of similar tasks in the hopes of generalization to novel but related tasks by learning a high level strategy that captures the essence of the problem it is asked to solve. however many recent meta learning approaches are extensively hand designed either using architectures specialized to a particular application or hard coding algorithmic components that constrain how the meta learner solves the task. we propose a class of simple and generic meta learner architectures that use a novel combination of temporal convolutions and soft attention the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. in the most extensive set of meta learning experiments to date we evaluate the resulting simple neural attentive learner or snail on several heavily benchmarked tasks. on all tasks in both supervised and reinforcement learning snail attains state of the art performance by significant margins.
kafnets kernel based non parametric activation functions for neural networks
neural networks are generally built by interleaving adaptable linear layers with fixed nonlinear activation functions. to increase their flexibility several authors have proposed methods for adapting the activation functions themselves endowing them with varying degrees of flexibility. none of these approaches however have gained wide acceptance in practice and research in this topic remains open. in this paper we introduce a novel family of flexible activation functions that are based on an inexpensive kernel expansion at every neuron. leveraging over several properties of kernel based models we propose multiple variations for designing and initializing these kernel activation functions kafs including a multidimensional scheme allowing to nonlinearly combine information from different paths in the network. the resulting kafs can approximate any mapping defined over a subset of the real line either convex or nonconvex. furthermore they are smooth over their entire domain linear in their parameters and they can be regularized using any known scheme including the use of ell 1 penalties to enforce sparseness. to the best of our knowledge no other known model satisfies all these properties simultaneously. in addition we provide a relatively complete overview on alternative techniques for adapting the activation functions which is currently lacking in the literature. a large set of experiments validates our proposal.
learning model based planning from scratch
conventional wisdom holds that model based planning is a powerful approach to sequential decision making. it is often very challenging in practice however because while a model can be used to evaluate a plan it does not prescribe how to construct a plan. here we introduce the imagination based planner the first model based sequential decision making agent that can learn to construct evaluate and execute plans. before any action it can perform a variable number of imagination steps which involve proposing an imagined action and evaluating it with its model based imagination. all imagined actions and outcomes are aggregated iteratively into a plan context which conditions future real and imagined actions. the agent can even decide how to imagine testing out alternative imagined actions chaining sequences of actions together or building a more complex imagination tree by navigating flexibly among the previously imagined states using a learned policy. and our agent can learn to plan economically jointly optimizing for external rewards and computational costs associated with using its imagination. we show that our architecture can learn to solve a challenging continuous control problem and also learn elaborate planning strategies in a discrete maze solving task. our work opens a new direction toward learning the components of a model based planning system and how to use them.
recurrent ladder networks
we propose a recurrent extension of the ladder networks whose structure is motivated by the inference required in hierarchical latent variable models. we demonstrate that the recurrent ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. the architecture shows close to optimal results on temporal modeling of video data competitive results on music modeling and improved perceptual grouping based on higher order abstractions such as stochastic textures and motion cues. we present results for fully supervised semi supervised and unsupervised tasks. the results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions learning iterative inference and handling temporal information.
generalization in deep learning
with a direct analysis of neural networks this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. unlike previous bound based theory our main theory is quantitatively as tight as possible for every dataset individually while producing qualitative insights competitively. our results give insight into why and how deep learning can generalize well despite its large capacity complexity possible algorithmic instability nonrobustness and sharp minima answering to an open question in the literature. we also discuss limitations of our results and propose additional open problems.
parametrizing filters of a cnn with a gan
it is commonly agreed that the use of relevant invariances as a good statistical bias is important in machine learning. however most approaches that explicitly incorporate invariances into a model architecture only make use of very simple transformations such as translations and rotations. hence there is a need for methods to model and extract richer transformations that capture much higher level invariances. to that end we introduce a tool allowing to parametrize the set of filters of a trained convolutional neural network with the latent space of a generative adversarial network. we then show that the method can capture highly non linear invariances of the data by visualizing their effect in the data space.
wider and deeper cheaper and faster tensorized lstms for sequence learning
long short term memory lstm is a popular approach to boosting the ability of recurrent neural networks to store longer term temporal information. the capacity of an lstm network can be increased by widening and adding layers. however usually the former introduces additional parameters while the latter increases the runtime. as an alternative we propose the tensorized lstm in which the hidden states are represented by tensors and updated via a cross layer convolution. by increasing the tensor size the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor by delaying the output the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.
learning and real time classification of hand written digits with spiking neural networks
we describe a novel spiking neural network snn for automated real time handwritten digit classification and its implementation on a gp gpu platform. information processing within the network from feature extraction to classification is implemented by mimicking the basic aspects of neuronal spike initiation and propagation in the brain. the feature extraction layer of the snn uses fixed synaptic weight maps to extract the key features of the image and the classifier layer uses the recently developed normad approximate gradient descent based supervised learning algorithm for spiking neural networks to adjust the synaptic weights. on the standard mnist database images of handwritten digits our network achieves an accuracy of 99.80 on the training set and 98.06 on the test set with nearly 7x fewer parameters compared to the state of the art spiking networks. we further use this network in a gpu based user interface system demonstrating real time snn simulation to infer digits written by different users. on a test set of 500 such images this real time platform achieves an accuracy exceeding 97 while making a prediction within an snn emulation time of less than 100ms.
overcoming catastrophic forgetting with hard attention to the task
catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. this problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. in this paper we propose a task based hard attention mechanism that preserves previous tasks information without affecting the current task s learning. a hard attention mask is learned concurrently to every task through stochastic gradient descent and previous masks are exploited to condition such learning. we show that the proposed mechanism is effective for reducing catastrophic forgetting cutting current rates by 45 to 80 . we also show that it is robust to different hyperparameter choices and that it offers a number of monitoring capabilities. the approach features the possibility to control both the stability and compactness of the learned knowledge which we believe makes it also attractive for online learning or network compression applications.
detecting and correcting for label shift with black box predictors
faced with distribution shift between training and test set we wish to detect and quantify the shift and to correct our classifiers without test set labels. motivated by medical diagnosis where diseases targets cause symptoms observations we focus on label shift where the label marginal p y changes but the conditional p x y does not. we propose black box shift estimation bbse to estimate the test distribution p y . bbse exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. while better predictors give tighter estimates bbse works even when predictors are biased inaccurate or uncalibrated so long as their confusion matrices are invertible. we prove bbse s consistency bound its error and introduce a statistical test that uses bbse to detect shift. we also leverage bbse to correct classifiers. experiments demonstrate accurate estimates and improved prediction even on high dimensional datasets of natural images.
generalization in machine learning via analytical learning theory
this paper introduces a novel measure theoretic learning theory to analyze generalization behaviors of practical interest. the proposed learning theory has the following abilities 1 to utilize the qualities of each learned representation on the path from raw inputs to outputs in representation learning 2 to guarantee good generalization errors possibly with arbitrarily rich hypothesis spaces e.g. arbitrarily large capacity and rademacher complexity and non stable non robust learning algorithms and 3 to clearly distinguish each individual problem instance from each other. our generalization bounds are relative to a representation of the data and hold true even if the representation is learned. we discuss several consequences of our results on deep learning one shot learning and curriculum learning. unlike statistical learning theory the proposed learning theory analyzes each problem instance individually via measure theory rather than a set of problem instances via statistics. because of the differences in the assumptions and the objectives the proposed learning theory is meant to be complementary to previous learning theory and is not designed to compete with it.
sensitivity and generalization in neural networks an empirical study
in practice it is often found that large over parameterized neural networks generalize better than their smaller counterparts an observation that appears to conflict with classical notions of function complexity which typically favor smaller models. in this work we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. our experiments survey thousands of models with various fully connected architectures optimizers and other hyper parameters as well as four different image classification datasets. we find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold as measured by the norm of the input output jacobian of the network and that it correlates well with generalization. we further establish that factors associated with poor generalization such as full batch training or using random labels correspond to lower robustness while factors associated with good generalization such as data augmentation and relu non linearities give rise to more robust functions. finally we demonstrate how the input output jacobian norm can be predictive of generalization at the level of individual test points.
on the importance of single directions for generalization
despite their ability to memorize large datasets deep neural networks often achieve good generalization performance. however the differences between the learned solutions of networks which generalize and those which do not remain unclear. additionally the tuning properties of single directions defined as the activation of a single unit or some linear combination of units in response to some input have been highlighted but their importance has not been evaluated. here we connect these lines of inquiry to demonstrate that a network s reliance on single directions is a good predictor of its generalization performance across networks trained on datasets with different fractions of corrupted labels across ensembles of networks trained on datasets with unmodified labels across different hyperparameters and over the course of training. while dropout only regularizes this quantity up to a point batch normalization implicitly discourages single direction reliance in part by decreasing the class selectivity of individual units. finally we find that class selectivity is a poor predictor of task importance suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity but also that individually selective units may not be necessary for strong network performance.
maximin affinity learning of image segmentation
images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. however this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. we present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the rand index a well known segmentation performance measure. the rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. by using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph we are able to train an affinity classifier to directly minimize the rand index of segmentations resulting from the graph partitioning. our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs which are predictive of the pixel pair connectivity.
a general framework for development of the cortex like visual object recognition system waves of spikes predictive coding and universal dictionary of features
this study is focused on the development of the cortex like visual object recognition system. we propose a general framework which consists of three hierarchical levels modules . these modules functionally correspond to the v1 v4 and it areas. both bottom up and top down connections between the hierarchical levels v4 and it are employed. the higher the degree of matching between the input and the preferred stimulus the shorter the response time of the neuron. therefore information about a single stimulus is distributed in time and is transmitted by the waves of spikes. the reciprocal connections and waves of spikes implement predictive coding an initial hypothesis is generated on the basis of information delivered by the first wave of spikes and is tested with the information carried by the consecutive waves. the development is considered as extraction and accumulation of features in v4 and objects in it. once stored a feature can be disposed if rarely activated. this cause update of feature repository. consequently objects in it are also updated. this illustrates the growing process and dynamical change of topological structures of v4 it and connections between these areas.
handwritten digit recognition with a committee of deep neural nets on gpus
the competitive mnist handwritten digit recognition benchmark has a long history of broken records since 1998. the most recent substantial improvement by others dates back 7 years error rate 0.4 . recently we were able to significantly improve this result using graphics cards to greatly speed up training of simple but deep mlps which achieved 0.35 outperforming all the previous more complex methods. here we report another substantial improvement 0.31 obtained using a committee of mlps.
eclectic extraction of propositional rules from neural networks
artificial neural network is among the most popular algorithm for supervised learning. however neural networks have a well known drawback of being a black box learner that is not comprehensible to the users. this lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision. rule extraction methods attempt to curb this limitation by extracting comprehensible rules from a trained network. many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. they have been broadly categorized into three types based on their approach to use internal model of the network. eclectic methods are hybrid algorithms that combine the other approaches to attain more performance. in this paper we present an eclectic method called heretic. our algorithm uses inductive decision tree learning combined with information of the neural network structure for extracting logical rules. experiments and theoretical analysis show heretic to be better in terms of speed and performance.
message passing multi agent gans
communicating and sharing intelligence among agents is an important facet of achieving artificial general intelligence. as a first step towards this challenge we introduce a novel framework for image generation message passing multi agent generative adversarial networks mpm gans . while gans have recently been shown to be very effective for image generation and other tasks these networks have been limited to mostly single generator discriminator networks. we show that we can obtain multi agent gans that communicate through message passing to achieve better image generation. the objectives of the individual agents in this framework are two fold a co operation objective and a competing objective. the co operation objective ensures that the message sharing mechanism guides the other generator to generate better than itself while the competing objective encourages each generator to generate better than its counterpart. we analyze and visualize the messages that these gans share among themselves in various scenarios. we quantitatively show that the message sharing formulation serves as a regularizer for the adversarial training. qualitatively we show that the different generators capture different traits of the underlying data distribution.
mode regularized generative adversarial networks
although generative adversarial networks achieve state of the art results on a variety of generative tasks they are regarded as highly unstable and prone to miss modes. we argue that these bad behaviors of gans are due to the very particular functional shape of the trained discriminators in high dimensional spaces which can easily make training stuck or push probability mass in the wrong direction towards that of higher concentration than that of the data generating distribution. we introduce several ways of regularizing the objective which can dramatically stabilize the training of gan models. we also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution during the early phases of training and thus providing a unified solution to the missing modes problem.
layer specific adaptive learning rates for deep networks
the increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. this slow training is due in part to vanishing gradients in which the gradients used by back propagation are extremely large for weights connecting deep layers layers near the output layer and extremely small for shallow layers near the input layer this results in slow learning in the shallow layers. additionally it has also been shown that in highly non convex problems such as deep neural networks there is a proliferation of high error low curvature saddle points which slows down learning dramatically. in this paper we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function increasing the learning rate at low curvature points. this enables us to speed up learning in the shallow layers of the network and quickly escape high error low curvature saddle points. we test our method on standard image classification datasets such as mnist cifar10 and imagenet and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.
return of frustratingly easy domain adaptation
unlike human learning machine learning often fails to handle changes between training source and test target input distributions. such domain shifts common in practical scenarios severely damage the performance of conventional machine learning methods. supervised domain adaptation methods have been proposed for the case when the target data have labels including some that perform very well despite being frustratingly easy to implement. however in practice the target domain is often unlabeled requiring unsupervised adaptation. we propose a simple effective and efficient method for unsupervised domain adaptation called correlation alignment coral . coral minimizes domain shift by aligning the second order statistics of source and target distributions without requiring any target labels. even though it is extraordinarily simple it can be implemented in four lines of matlab code coral performs remarkably well in extensive evaluations on standard benchmark datasets.
origami a 803 gop s w convolutional network accelerator
an ever increasing number of computer vision and image video processing challenges are being approached using deep convolutional neural networks obtaining state of the art results in object recognition and detection semantic segmentation action recognition optical flow and superresolution. hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. we present a new architecture design and implementation as well as the first reported silicon measurements of such an accelerator outperforming previous work in terms of power area and i o efficiency. the manufactured device provides up to 196 gop s on 3.09 mm 2 of silicon in umc 65nm technology and can achieve a power efficiency of 803 gop s w. the massively reduced bandwidth requirements make it the first architecture scalable to top s performance.
option discovery in hierarchical reinforcement learning using spatio temporal clustering
this paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms. these structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. we use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. the spectral clustering algorithm pcca is used to identify suitable abstractions aligned to the underlying structure. skills are defined in terms of the sequence of actions that lead to transitions between such abstract states. the connectivity information from pcca is used to generate these skills or options. these skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model. this approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate. we also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.
residual networks behave like ensembles of relatively shallow networks
in this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. moreover residual networks seem to enable very deep networks by leveraging only the short paths during training. to support this observation we rewrite residual networks as an explicit collection of paths. unlike traditional models paths through residual networks vary in length. further a lesion study reveals that these paths show ensemble like behavior in the sense that they do not strongly depend on each other. finally and most surprising most paths are shorter than one might expect and only the short paths are needed during training as longer paths do not contribute any gradient. for example most of the gradient in a residual network with 110 layers comes from paths that are only 10 34 layers deep. our results reveal one of the key characteristics that seem to enable the training of very deep networks residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.
synthesizing the preferred inputs for neurons in neural networks via deep generator networks
deep neural networks dnns have demonstrated state of the art results on many pattern recognition tasks especially vision classification problems. understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right similar to why we study the human brain and will enable researchers to further improve dnns. one path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. one such method is called activation maximization am which synthesizes an input e.g. an image that highly activates a neuron. here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful learned prior a deep generator network dgn . the algorithm 1 generates qualitatively state of the art synthetic images that look almost real 2 reveals the features learned by each neuron in an interpretable way 3 generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned and 4 can be considered as a high quality generative method in this case by generating novel creative interesting recognizable images .
structured convolution matrices for energy efficient deep learning
we derive a relationship between network representation in energy efficient neuromorphic architectures and block toplitz convolutional matrices. inspired by this connection we develop deep convolutional networks using a family of structured convolutional matrices and achieve state of the art trade off between energy efficiency and classification accuracy for well known image recognition tasks. we also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy rectified linear units and binary activations.
deep coral correlation alignment for deep domain adaptation
deep neural networks are able to learn powerful representations from large quantities of labeled input data however they cannot always generalize well across changes in input distributions. domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. in this paper we address the case when the target domain is unlabeled requiring unsupervised adaptation. coral is a frustratingly easy unsupervised domain adaptation method that aligns the second order statistics of the source and target distributions with a linear transformation. here we extend coral to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks deep coral . experiments on standard benchmark datasets show state of the art performance.
spatio temporal lstm with trust gates for 3d human action recognition
3d action recognition analysis of human actions based on 3d skeleton data becomes popular recently due to its succinctness robustness and view invariant representation. recent attempts on this problem suggested to develop rnn based learning methods to model the contextual dependency in the temporal domain. in this paper we extend this idea to spatio temporal domains to analyze the hidden sources of action related information within the input data over both domains concurrently. inspired by the graphical structure of the human skeleton we further propose a more powerful tree structure based traversal method. to handle the noise and occlusion in 3d skeleton data we introduce new gating mechanism within lstm to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long term context information stored in the memory cell. our method achieves state of the art performance on 4 challenging benchmark datasets for 3d human action analysis.
generalized dropout
deep neural networks often require good regularizers to generalize well. dropout is one such regularizer that is widely used among deep learning practitioners. recent work has shown that dropout can also be viewed as performing approximate bayesian inference over the network parameters. in this work we generalize this notion and introduce a rich family of regularizers which we call generalized dropout. one set of methods in this family called dropout is a version of dropout with trainable parameters. classical dropout emerges as a special case of this method. another member of this family selects the width of neural network layers. experiments show that these methods help in improving generalization performance over dropout.
parsimonious inference on convolutional neural networks learning and applying on line kernel activation rules
a new radical cnn design approach is presented in this paper considering the reduction of the total computational load during inference. this is achieved by a new holistic intervention on both the cnn architecture and the training procedure which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a cnn architecture. this is accomplished by the introduction of a new structural element that can be inserted as an add on to any contemporary cnn architecture whilst preserving or even improving its recognition accuracy. our approach formulates a systematic and data driven method for developing cnns that are trained to eventually change size and form in real time during inference targeting to the smaller possible computational footprint. results are provided for the optimal implementation on a few modern high end mobile computing platforms indicating a significant speed up of up to x3 times.
model agnostic meta learning for fast adaptation of deep networks
we propose an algorithm for meta learning that is model agnostic in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems including classification regression and reinforcement learning. the goal of meta learning is to train a model on a variety of learning tasks such that it can solve new learning tasks using only a small number of training samples. in our approach the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. in effect our method trains the model to be easy to fine tune. we demonstrate that this approach leads to state of the art performance on two few shot image classification benchmarks produces good results on few shot regression and accelerates fine tuning for policy gradient reinforcement learning with neural network policies.
wrpn training and inference using wide reduced precision networks
for computer vision applications prior works have shown the efficacy of reducing the numeric precision of model parameters network weights in deep neural networks but also that reducing the precision of activations hurts model accuracy much more than reducing the precision of model parameters. we study schemes to train networks from scratch using reduced precision activations without hurting the model accuracy. we reduce the precision of activation maps along with model parameters using a novel quantization scheme and increase the number of filter maps in a layer and find that this scheme compensates or surpasses the accuracy of the baseline full precision network. as a result one can significantly reduce the dynamic memory footprint memory bandwidth computational energy and speed up the training and inference process with appropriate hardware support. we call our scheme wrpn wide reduced precision networks. we report results using our proposed schemes and show that our results are better than previously reported accuracies on ilsvrc 12 dataset while being computationally less expensive compared to previously reported reduced precision networks.
deep learning is robust to massive label noise
deep neural networks trained on large supervised datasets have led to impressive results in image classification and other tasks. however well annotated datasets can be time consuming and expensive to collect lending increased interest to larger but noisy datasets that are more easily obtained. in this paper we show that deep neural networks are capable of generalizing from training data for which true labels are massively outnumbered by incorrect labels. we demonstrate remarkably high test performance after training on corrupted data from mnist cifar and imagenet. for example on mnist we obtain test accuracy above 90 percent even after each clean training example has been diluted with 100 randomly labeled examples. such behavior holds across multiple patterns of label noise even when erroneous labels are biased towards confusing classes. we show that training in this regime requires a significant but manageable increase in dataset size that is related to the factor by which correct labels have been diluted. finally we provide an analysis of our results that shows how increasing noise decreases the effective batch size.
improving content invariance in gated autoencoders for 2d and 3d object rotation
content invariance in mapping codes learned by gaes is a useful feature for various relation learning tasks. in this paper we show that the content invariance of mapping codes for images of 2d and 3d rotated objects can be substantially improved by extending the standard gae loss symmetric reconstruction error with a regularization term that penalizes the symmetric cross reconstruction error. this error term involves reconstruction of pairs with mapping codes obtained from other pairs exhibiting similar transformations. although this would principally require knowledge of the transformations exhibited by training pairs our experiments show that a bootstrapping approach can sidestep this issue and that the regularization term can effectively be used in an unsupervised setting.
deep learning for sensor based activity recognition a survey
sensor based activity recognition seeks the profound high level knowledge about human activities from multitudes of low level sensor readings. conventional pattern recognition approaches have made tremendous progress in the past years. however those methods often heavily rely on heuristic hand crafted feature extraction which could hinder their generalization performance. additionally existing methods are undermined for unsupervised and incremental learning tasks. recently the recent advancement of deep learning makes it possible to perform automatic high level feature extraction thus achieves promising performance in many areas. since then deep learning based methods have been widely adopted for the sensor based activity recognition tasks. this paper surveys the recent advance of deep learning based sensor based activity recognition. we summarize existing literature from three aspects sensor modality deep model and application. we also present detailed insights on existing work and propose grand challenges for future research.
on the importance of consistency in training deep neural networks
we explain that the difficulties of training deep neural networks come from a syndrome of three consistency issues. this paper describes our efforts in their analysis and treatment. the first issue is the training speed inconsistency in different layers. we propose to address it with an intuitive simple to implement low footprint second order method. the second issue is the scale inconsistency between the layer inputs and the layer residuals. we explain how second order information provides favorable convenience in removing this roadblock. the third and most challenging issue is the inconsistency in residual propagation. based on the fundamental theorem of linear algebra we provide a mathematical characterization of the famous vanishing gradient problem. thus an important design principle for future optimization and neural network design is derived. we conclude this paper with the construction of a novel contractive neural network.
ui net interactive artificial neural networks for iterative image segmentation based on a user model
for complex segmentation tasks fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects. especially in cases where only few data sets need to be processed for a highly accurate result semi automatic segmentation techniques exhibit a clear benefit for the user. one area of application is medical image processing during an intervention for a single patient. we propose a learning based cooperative segmentation approach which includes the computing entity as well as the user into the task. our system builds upon a state of the art fully convolutional artificial neural network fcn as well as an active user model for training. during the segmentation process a user of the trained system can iteratively add additional hints in form of pictorial scribbles as seed points into the fcn system to achieve an interactive and precise segmentation result. the segmentation quality of interactive fcns is evaluated. iterative fcn approaches can yield superior results compared to networks without the user input channel component due to a consistent improvement in segmentation quality after each interaction.
lightweight neural networks
most of the weights in a lightweight neural network have a value of zero while the remaining ones are either 1 or 1. these universal approximators require approximately 1.1 bits weight of storage posses a quick forward pass and achieve classification accuracies similar to conventional continuous weight networks. their training regimen focuses on error reduction initially but later emphasizes discretization of weights. they ignore insignificant inputs remove unnecessary weights and drop unneeded hidden neurons. we have successfully tested them on the mnist credit card fraud and credit card defaults data sets using networks having 2 to 16 hidden layers and up to 4.4 million weights.
tensor field networks rotation and translation equivariant neural networks for 3d point clouds
we introduce tensor field networks which are locally equivariant to 3d rotations translations and permutations of points at every layer. 3d rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. our network uses filters built from spherical harmonics due to the mathematical consequences of this filter choice each layer accepts as input and guarantees as output scalars vectors and higher order tensors in the geometric sense of these terms. we demonstrate how tensor field networks learn to model simple physics newtonian gravitation and moment of inertia classify simple 3d shapes trained on one orientation and tested on shapes in arbitrary orientations and given a small organic molecule with an atom removed replace the correct element at the correct location in space.
knowledge matters importance of prior information for optimization
we explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state of the art machine learning algorithms tested failed to learn. we motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. the experiments we have conducted provide positive evidence in favor of this hypothesis. in our experiments a two tiered mlp architecture is trained on a dataset with 64x64 binary inputs images each image with three sprites. the final task is to decide whether all the sprites are the same or one of them is different. sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. the first part of the two tiered mlp is pre trained with intermediate level targets being the presence of sprites at each location while the second part takes the output of the first part as input and predicts the final task s target binary event. the two tiered mlp architecture with a few tens of thousand examples was able to learn the task perfectly whereas all other algorithms include unsupervised pre training but also traditional algorithms like svms decision trees and boosting all perform no better than chance. we hypothesize that the optimization difficulty involved when the intermediate pre training is not performed is due to the em composition of two highly non linear tasks. our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning presumably because of effective local minima.
zero bias autoencoders and the benefits of co adapting features
regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. we show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. we then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. we also propose a new activation function that decouples the two roles of the hidden layer and that allows us to learn representations on data with very high intrinsic dimensionality where standard autoencoders typically fail. since the decoupled activation function acts like an implicit regularizer the model can be trained by minimizing the reconstruction error of training data without requiring any additional regularization.
theory and tools for the conversion of analog to spiking convolutional neural networks
deep convolutional neural networks cnns have shown great potential for numerous real world machine learning applications but performing inference in large cnns in real time remains a challenge. we have previously demonstrated that traditional cnns can be converted into deep spiking neural networks snns which exhibit similar accuracy while reducing both latency and computational load as a consequence of their data driven event based style of computing. here we provide a novel theory that explains why this conversion is successful and derive from it several new tools to convert a larger and more powerful class of deep networks into snns. we identify the main sources of approximation errors in previous conversion methods and propose simple mechanisms to fix these issues. furthermore we develop spiking implementations of common cnn operations such as max pooling softmax and batch normalization which allow almost loss less conversion of arbitrary cnn architectures into the spiking domain. empirical evaluation of different network architectures on the mnist and cifar10 benchmarks leads to the best snn results reported to date.
stacked generative adversarial networks
in this paper we propose a novel generative model named stacked generative adversarial networks sgan which is trained to invert the hierarchical representations of a bottom up discriminative network. our model consists of a top down stack of gans each learned to generate lower level representations conditioned on higher level representations. a representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom up discriminative network leveraging the powerful discriminative representations to guide the generative model. in addition we introduce a conditional loss that encourages the use of conditional information from the layer above and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. we first train each stack independently and then train the whole model end to end. unlike the original gan that uses a single noise vector to represent all the variations our sgan decomposes variations into multiple levels and gradually resolves uncertainties in the top down generative process. based on visual inspection inception scores and visual turing test we demonstrate that sgan is able to generate images of much higher quality than gans without stacking.
self informed neural network structure learning
we study the problem of large scale multi label visual recognition with a large number of possible classes. we propose a method for augmenting a trained neural network classifier with auxiliary capacity in a manner designed to significantly improve upon an already well performing model while minimally impacting its computational footprint. using the predictions of the network itself as a descriptor for assessing visual similarity we define a partitioning of the label space into groups of visually similar entities. we then augment the network with auxilliary hidden layer pathways with connectivity only to these groups of label units. we report a significant improvement in mean average precision on a large scale object recognition task with the augmented model while increasing the number of multiply adds by less than 3 .
learning activation functions to improve deep neural networks
artificial neural networks typically have a fixed non linear activation function at each neuron. we have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. with this adaptive activation function we are able to improve upon deep neural network architectures composed of static rectified linear units achieving state of the art performance on cifar 10 7.51 cifar 100 30.83 and a benchmark from high energy physics involving higgs boson decay modes.
denoising autoencoder with modulated lateral connections learns invariant representations of natural images
suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder dae to focus on invariant representations. in regular autoencoders detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. it is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. three dae structures with modulated and additive lateral connections and without lateral connections were compared in experiments using real world images. the experiments verify that adding modulated lateral connections to the model 1 improves the accuracy of the probability model for inputs as measured by denoising performance 2 results in representations whose degree of invariance grows faster towards the higher layers and 3 supports the formation of diverse invariant poolings.
a probabilistic theory of deep learning
a grand challenge in machine learning is the development of computational algorithms that match or outperform humans in perceptual inference tasks that are complicated by nuisance variation. for instance visual object recognition involves the unknown object position orientation and scale in object recognition while speech recognition involves the unknown voice pronunciation pitch and speed. recently a new breed of deep learning algorithms have emerged for high nuisance inference tasks that routinely yield pattern recognition systems with near or super human capabilities. but a fundamental question remains why do they work intuitions abound but a coherent framework for understanding analyzing and synthesizing deep learning architectures has remained elusive. we answer this question by developing a new probabilistic framework for deep learning based on the deep rendering model a generative probabilistic model that explicitly captures latent nuisance variation. by relaxing the generative model to a discriminative one we can recover two of the current leading deep learning systems deep convolutional neural networks and random decision forests providing insights into their successes and shortcomings as well as a principled route to their improvement.
integrated inference and learning of neural factors in structural support vector machines
tackling pattern recognition problems in areas such as computer vision bioinformatics speech or text recognition is often done best by taking into account task specific statistical relations between output variables. in structured prediction this internal structure is used to predict multiple outputs simultaneously leading to more accurate and coherent predictions. structural support vector machines ssvms are nonprobabilistic models that optimize a joint input output function through margin based learning. because ssvms generally disregard the interplay between unary and interaction factors during the training phase final parameters are suboptimal. moreover its factors are often restricted to linear combinations of input features limiting its generalization power. to improve prediction accuracy this paper proposes i joint inference and learning by integration of back propagation and loss augmented inference in ssvm subgradient descent ii extending ssvm factors to neural networks that form highly nonlinear functions of input features. image segmentation benchmark results demonstrate improvements over conventional ssvm training methods in terms of accuracy highlighting the feasibility of end to end ssvm training with neural factors.
what happened to my dog in that network unraveling top down generators in convolutional neural networks
top down information plays a central role in human perception but plays relatively little role in many current state of the art deep networks such as convolutional neural networks cnns . this work seeks to explore a path by which top down information can have a direct impact within current deep networks. we explore this path by learning and using generators corresponding to the network internal effects of three types of transformation each a restriction of a general affine transformation rotation scaling and translation. we demonstrate how these learned generators can be used to transfer top down information to novel settings as mediated by the feature flows that the transformations and the associated generators correspond to inside the network. specifically we explore three aspects 1 using generators as part of a method for synthesizing transformed images given a previously unseen image produce versions of that image corresponding to one or more specified transformations 2 zero shot learning when provided with a feature flow corresponding to the effect of a transformation of unknown amount leverage learned generators as part of a method by which to perform an accurate categorization of the amount of transformation even for amounts never observed during training and 3 inside cnn data augmentation improve the classification performance of an existing network by using the learned generators to directly provide additional training inside the cnn .
virtual worlds as proxy for multi object tracking analysis
modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. in this work we instead leverage the recent progress in computer graphics to generate fully labeled dynamic and photo realistic proxy virtual worlds. we propose an efficient real to virtual world cloning method and validate our approach by building and publicly releasing a new video dataset called virtual kitti see http www.xrce.xerox.com research development computer vision proxy virtual worlds automatically labeled with accurate ground truth for object detection tracking scene and instance segmentation depth and optical flow. we provide quantitative experimental evidence suggesting that i modern deep learning algorithms pre trained on real data behave similarly in real and virtual worlds and ii pre training on virtual data improves performance. as the gap between real and virtual worlds is small virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance all other things being equal. we show these factors may affect drastically otherwise high performing deep models for tracking.
synthesizing dynamic patterns by spatial temporal generative convnet
video sequences contain rich dynamic patterns such as dynamic texture patterns that exhibit stationarity in the temporal domain and action patterns that are non stationary in either spatial or temporal domain. we show that a spatial temporal generative convnet can be used to model and synthesize dynamic patterns. the model defines a probability distribution on the video sequence and the log probability is defined by a spatial temporal convnet that consists of multiple layers of spatial temporal filters to capture spatial temporal patterns of different scales. the model can be learned from the training video sequences by an analysis by synthesis learning algorithm that iterates the following two steps. step 1 synthesizes video sequences from the currently learned model. step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences. we show that the learning algorithm can synthesize realistic dynamic patterns.
deep learning with darwin evolutionary synthesis of deep neural networks
taking inspiration from biological evolution we explore the idea of can deep neural networks evolve naturally over successive generations into highly efficient deep neural networks by introducing the notion of synthesizing new highly efficient yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks. the architectural traits of ancestor deep neural networks are encoded using synaptic probability models which can be viewed as the dna of these networks. new descendant networks with differing network architectures are synthesized based on these synaptic probability models from the ancestor networks and computational environmental factor models in a random manner to mimic heredity natural selection and random mutation. these offspring networks are then trained into fully functional networks like one would train a newborn and have more efficient more diverse network architectures than their ancestor networks while achieving powerful modeling capabilities. experimental results for the task of visual saliency demonstrated that the synthesized evolved offspring networks can achieve state of the art performance while having network architectures that are significantly more efficient with a staggering sim 48 fold decrease in synapses by the fourth generation compared to the original ancestor network.
alternating back propagation for generator network
this paper proposes an alternating back propagation algorithm for learning the generator network model. the model is a non linear generalization of factor analysis. in this model the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. the alternating back propagation algorithm iterates the following two steps 1 inferential back propagation which infers the latent factors by langevin dynamics or gradient descent. 2 learning back propagation which updates the parameters given the inferred latent factors by gradient descent. the gradient computations in both steps are powered by back propagation and they share most of their code in common. we show that the alternating back propagation algorithm can learn realistic generator models of natural images video sequences and sounds. moreover it can also be used to learn from incomplete or indirect training data.
hyperparameter transfer learning through surrogate alignment for efficient deep neural network training
recently several optimization methods have been successfully applied to the hyperparameter optimization of deep neural networks dnns . the methods work by modeling the joint distribution of hyperparameter values and corresponding error. those methods become less practical when applied to modern dnns whose training may take a few days and thus one cannot collect sufficient observations to accurately model the distribution. to address this challenging issue we propose a method that learns to transfer optimal hyperparameter values for a small source dataset to hyperparameter values with comparable performance on a dataset of interest. as opposed to existing transfer learning methods our proposed method does not use hand designed features. instead it uses surrogates to model the hyperparameter error distributions of the two datasets and trains a neural network to learn the transfer function. extensive experiments on three cv benchmark datasets clearly demonstrate the efficiency of our method.
towards bayesian deep learning a framework and some existing methods
while perception tasks such as visual object recognition and text understanding play an important role in human intelligence the subsequent tasks that involve inference reasoning and planning require an even higher level of intelligence. the past few years have seen major advances in many perception tasks using deep learning models. for higher level inference however probabilistic graphical models with their bayesian nature are still more powerful and flexible. to achieve integrated intelligence that involves both perception and inference it is naturally desirable to tightly integrate deep learning and bayesian models within a principled probabilistic framework which we call bayesian deep learning. in this unified framework the perception of text or images using deep learning can boost the performance of higher level inference and in return the feedback from the inference process is able to enhance the perception of text or images. this paper proposes a general framework for bayesian deep learning and reviews its recent applications on recommender systems topic models and control. in this paper we also discuss the relationship and differences between bayesian deep learning and other related topics like bayesian treatment of neural networks.
deciding how to decide dynamic routing in artificial neural networks
we propose and systematically evaluate three strategies for training dynamically routed artificial neural networks graphs of learned transformations through which different input signals may take different paths. though some approaches have advantages over others the resulting networks are often qualitatively similar. we find that in dynamically routed networks trained to classify images layers and branches become specialized to process distinct categories of images. additionally given a fixed computational budget dynamically routed networks tend to perform better than comparable statically routed networks.
pixel deconvolutional networks
deconvolutional layers have been widely used in a variety of deep models for up sampling including encoder decoder networks for semantic segmentation and deep generative models for unsupervised learning. one of the key limitations of deconvolutional operations is that they result in the so called checkerboard problem. this is caused by the fact that no direct relationship exists among adjacent pixels on the output feature map. to address this problem we propose the pixel deconvolutional layer pixeldcl to establish direct relationships among adjacent pixels on the up sampled feature map. our method is based on a fresh interpretation of the regular deconvolution operation. the resulting pixeldcl can be used to replace any deconvolutional layer in a plug and play manner without compromising the fully trainable capabilities of original models. the proposed pixeldcl may result in slight decrease in efficiency but this can be overcome by an implementation trick. experimental results on semantic segmentation demonstrate that pixeldcl can consider spatial features such as edges and shapes and yields more accurate segmentation outputs than deconvolutional layers. when used in image generation tasks our pixeldcl can largely overcome the checkerboard problem suffered by regular deconvolution operations.
gaussian prototypical networks for few shot learning on omniglot
we propose a novel architecture for k shot classification on the omniglot dataset. building on prototypical networks we extend their architecture to what we call gaussian prototypical networks. prototypical networks learn a map between images and embedding vectors and use their clustering for classification. in our model a part of the encoder output is interpreted as a confidence region estimate about the embedding point and expressed as a gaussian covariance matrix. our network then constructs a direction and class dependent distance metric on the embedding space using uncertainties of individual data points as weights. we show that gaussian prototypical networks are a preferred architecture over vanilla prototypical networks with an equivalent number of parameters. we report state of the art performance in 1 shot and 5 shot classification both in 5 way and 20 way regime for 5 shot 5 way we are comparable to previous state of the art on the omniglot dataset. we explore artificially down sampling a fraction of images in the training set which improves our performance even further. we therefore hypothesize that gaussian prototypical networks might perform better in less homogeneous noisier datasets which are commonplace in real world applications.
super convergence very fast training of residual networks using large learning rates
in this paper we show a phenomenon which we named super convergence where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods. the existence of super convergence is relevant to understanding why deep networks generalize well. one of the key elements of super convergence is training with cyclical learning rates and a large maximum learning rate. furthermore we present evidence that training with large learning rates improves performance by regularizing the network. in addition we show that super convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. we also derive a simplification of the hessian free optimization method to compute an estimate of the optimal learning rate. the architectures and code to replicate the figures in this paper are available at github.com lnsmith54 super convergence.
generative learning for deep networks
learning taking into account full distribution of the data referred to as generative is not feasible with deep neural networks dnns because they model only the conditional distribution of the outputs given the inputs. current solutions are either based on joint probability models facing difficult estimation problems or learn two separate networks mapping inputs to outputs recognition and vice versa generation . we propose an intermediate approach. first we show that forward computation in dnns with logistic sigmoid activations corresponds to a simplified approximate bayesian inference in a directed probabilistic multi layer model. this connection allows to interpret dnn as a probabilistic model of the output and all hidden units given the input. second we propose that in order for the recognition and generation networks to be more consistent with the joint model of the data weights of the recognition and generator network should be related by transposition. we demonstrate in a tentative experiment that such a coupled pair can be learned generatively modelling the full distribution of the data and has enough capacity to perform well in both recognition and generation.
hierarchical representations for efficient architecture search
we explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts and an expressive search space that supports complex topologies. our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification obtaining top 1 error of 3.6 on cifar 10 and 20.3 when transferred to imagenet which is competitive with the best existing neural architecture search approaches. we also present results using random search achieving 0.3 less top 1 accuracy on cifar 10 and 0.1 less on imagenet whilst reducing the search time from 36 hours down to 1 hour.
data augmentation generative adversarial networks
effective training of neural networks requires much data. in the low data regime parameters are underdetermined and learnt networks generalise poorly. data augmentation alleviates this by using existing data more effectively. however standard data augmentation produces only limited plausible alternative data. given there is potential to generate a much broader set of augmentations we design and train a generative model to do data augmentation. the model based on image conditional generative adversarial networks takes data from a source domain and learns to take any data item and generalise it to generate other within class data items. as this generative process does not depend on the classes themselves it can be applied to novel unseen classes of data. we show that a data augmentation generative adversarial network dagan augments standard vanilla classifiers well. we also show a dagan can enhance few shot learning systems such as matching networks. we demonstrate these approaches on omniglot on emnist having learnt the dagan on omniglot and vgg face data. in our experiments we can see over 13 increase in accuracy in the low data regime experiments in omniglot from 69 to 82 emnist 73.9 to 76 and vgg face 4.5 to 12 in matching networks for omniglot we observe an increase of 0.5 from 96.9 to 97.4 and an increase of 1.8 in emnist from 59.5 to 61.3 .
dnn buddies a deep neural network based estimation metric for the jigsaw puzzle problem
this paper introduces the first deep neural network based estimation metric for the jigsaw puzzle problem. given two puzzle piece edges the neural network predicts whether or not they should be adjacent in the correct assembly of the puzzle using nothing but the pixels of each piece. the proposed metric exhibits an extremely high precision even though no manual feature extraction is performed. when incorporated into an existing puzzle solver the solution s accuracy increases significantly achieving thereby a new state of the art standard.
deeppainter painter classification using deep convolutional autoencoders
in this paper we describe the problem of painter classification and propose a novel approach based on deep convolutional autoencoder neural networks. while previous approaches relied on image processing and manual feature extraction from paintings our approach operates on the raw pixel level without any preprocessing or manual feature extraction. we first train a deep convolutional autoencoder on a dataset of paintings and subsequently use it to initialize a supervised convolutional neural network for the classification phase. the proposed approach substantially outperforms previous methods improving the previous state of the art for the 3 painter classification problem from 90.44 accuracy previous state of the art to 96.52 accuracy i.e. a 63 reduction in error rate.
deepbrain functional representation of neural in situ hybridization images for gene ontology classification using deep convolutional autoencoders
this paper presents a novel deep learning based method for learning a functional representation of mammalian neural images. the method uses a deep convolutional denoising autoencoder cdae for generating an invariant compact representation of in situ hybridization ish images. while most existing methods for bio imaging analysis were not developed to handle images with highly complex anatomical structures the results presented in this paper show that functional representation extracted by cdae can help learn features of functional gene ontology categories for their classification in a highly accurate manner. using this cdae representation our method outperforms the previous state of the art classification rate by improving the average auc from 0.92 to 0.98 i.e. achieving 75 reduction in error. the method operates on input images that were downsampled significantly with respect to the original ones to make it computationally feasible.
generative adversarial perturbations
in this paper we propose novel generative models for creating adversarial examples slightly perturbed images resembling natural images but maliciously crafted to fool pre trained models. we present trainable deep neural networks for transforming images to adversarial perturbations. our proposed models can produce image agnostic and image dependent perturbations for both targeted and non targeted attacks. we also demonstrate that similar architectures can achieve impressive results in fooling classification and semantic segmentation models obviating the need for hand crafting attack methods for each task. using extensive experiments on challenging high resolution datasets such as imagenet and cityscapes we show that our perturbations achieve high fooling rates with small perturbation norms. moreover our attacks are considerably faster than current iterative methods at inference time.
a rotation and a translation suffice fooling cnns with simple transformations
we show that simple transformations namely translations and rotations alone are sufficient to fool neural network based vision models on a significant fraction of inputs. this is in sharp contrast to previous work that relied on more complicated optimization approaches that are unlikely to appear outside of a truly adversarial setting. moreover fooling rotations and translations are easy to find and require only a few black box queries to the target model. overall our findings emphasize the need for designing robust classifiers even in natural benign contexts.
peephole predicting network performance before training
the quest for performant networks has been a significant force that drives the advancements of deep learning in recent years. while rewarding improving network design has never been an easy journey. the large design space combined with the tremendous cost required for network training poses a major obstacle to this endeavor. in this work we propose a new approach to this problem namely predicting the performance of a network before training based on its architecture. specifically we develop a unified way to encode individual layers into vectors and bring them together to form an integrated description via lstm. taking advantage of the recurrent network s strong expressive power this method can reliably predict the performances of various network architectures. our empirical studies showed that it not only achieved accurate predictions but also produced consistent rankings across datasets a key desideratum in performance prediction.
an architecture combining convolutional neural network cnn and support vector machine svm for image classification
convolutional neural networks cnns are similar to ordinary neural networks in the sense that they are made up of hidden layers consisting of neurons with learnable parameters. these neurons receive inputs performs a dot product and then follows it with a non linearity. the whole network expresses the mapping between raw image pixels and their class scores. conventionally the softmax function is the classifier used at the last layer of this network. however there have been studies alalshekmubarak and smith 2013 agarap 2017 tang 2013 conducted to challenge this norm. the cited studies introduce the usage of linear support vector machine svm in an artificial neural network architecture. this project is yet another take on the subject and is inspired by tang 2013 . empirical data has shown that the cnn svm model was able to achieve a test accuracy of 99.04 using the mnist dataset lecun cortes and burges 2010 . on the other hand the cnn softmax was able to achieve a test accuracy of 99.23 using the same dataset. both models were also tested on the recently published fashion mnist dataset xiao rasul and vollgraf 2017 which is suppose to be a more difficult image classification dataset than mnist zalandoresearch 2017 . this proved to be the case as cnn svm reached a test accuracy of 90.72 while the cnn softmax reached a test accuracy of 91.86 . the said results may be improved if data preprocessing techniques were employed on the datasets and if the base cnn model was a relatively more sophisticated than the one used in this study.
benchmarking decoupled neural interfaces with synthetic gradients
artifical neural networks are a particular class of learning systems modeled after biological neural functions with an interesting penchant for hebbian learning that is neurons that wire together fire together . however unlike their natural counterparts artificial neural networks have a close and stringent coupling between the modules of neurons in the network. this coupling or locking imposes upon the network a strict and inflexible structure that prevent layers in the network from updating their weights until a full feed forward and backward pass has occurred. such a constraint though may have sufficed for a while is now no longer feasible in the era of very large scale machine learning coupled with the increased desire for parallelization of the learning process across multiple computing infrastructures. to solve this problem synthetic gradients sg with decoupled neural interfaces dni are introduced as a viable alternative to the backpropagation algorithm. this paper performs a speed benchmark to compare the speed and accuracy capabilities of sg dni as opposed to a standard neural interface using multilayer perceptron mlp. sg dni shows good promise in that it not only captures the learning problem it is also over 3 fold faster due to it asynchronous learning capabilities.
segmentation hi rarchique faiblement supervis e
image segmentation is the process of partitioning an image into a set of meaningful regions according to some criteria. hierarchical segmentation has emerged as a major trend in this regard as it favors the emergence of important regions at different scales. on the other hand many methods allow us to have prior information on the position of structures of interest in the images. in this paper we present a versatile hierarchical segmentation method that takes into account any prior spatial information and outputs a hierarchical segmentation that emphasizes the contours or regions of interest while preserving the important structures in the image. an application of this method to the weakly supervised segmentation problem is presented.
training wide residual networks for deployment using a single bit for each weight
for fast and energy efficient deployment of trained deep neural networks on resource constrained embedded hardware each learned weight parameter should ideally be represented and stored using a single bit. error rates usually increase when this requirement is imposed. here we report large improvements in error rates on multiple datasets for deep convolutional neural networks deployed with 1 bit per weight. using wide residual networks as our main baseline our approach simplifies existing methods that binarize weights by applying the sign function in training we apply scaling factors for each layer with constant unlearned values equal to the layer specific standard deviations used for initialization. for cifar 10 cifar 100 and imagenet and models with 1 bit per weight requiring less than 10 mb of parameter memory we achieve error rates of 3.9 18.5 and 26.0 8.5 top 1 top 5 respectively. we also considered mnist svhn and imagenet32 achieving 1 bit per weight test results of 0.27 1.9 and 41.3 19.1 respectively. for cifar our error rates halve previously reported values and are within about 1 of our error rates for the same network with full precision weights. for networks that overfit we also show significant improvements in error rate by not learning batch normalization scale and offset parameters. this applies to both full precision and 1 bit per weight networks. using a warm restart learning rate schedule we found that training for 1 bit per weight is just as fast as full precision networks with better accuracy than standard schedules and achieved about 98 99 of peak performance in just 62 training epochs for cifar 10 100. for full training code and trained models in matlab keras and pytorch see https github.com mcdonnell lab 1 bit per weight .
deep learning using rectified linear units relu 
we introduce the use of rectified linear units relu as the classification function in a deep neural network dnn . conventionally relu is used as an activation function in dnns with softmax function as their classification function. however there have been several studies on using a classification function other than softmax and this study is an addition to those. we accomplish this by taking the activation of the penultimate layer h n 1 in a neural network then multiply it by weight parameters theta to get the raw scores o i . afterwards we threshold the raw scores o i by 0 i.e. f o max 0 o i where f o is the relu function. we provide class predictions hat y through argmax function i.e. argmax f x .
rectified factor networks
we propose rectified factor networks rfns to efficiently construct very sparse non linear high dimensional representations of the input. rfn models identify rare and small events in the input have a low interference between code units have a small reconstruction error and explain the data covariance structure. rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non negative and normalized posterior means. we proof convergence and correctness of the rfn learning algorithm. on benchmarks rfns are compared to other unsupervised methods like autoencoders rbms factor analysis ica and pca. in contrast to previous sparse coding methods rfns yield sparser codes capture the data s covariance structure more precisely and have a significantly smaller reconstruction error. we test rfns as pretraining technique for deep networks on different vision datasets where rfns were superior to rbms and autoencoders. on gene expression data from two pharmaceutical drug discovery studies rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.
from maxout to channel out encoding information on sparse pathways
motivated by an important insight from neural science we propose a new framework for understanding the success of the recently proposed maxout networks. the framework is based on encoding information on sparse pathways and recognizing the correct pathway at inference time. elaborating further on this insight we propose a novel deep network architecture called channel out network which takes a much better advantage of sparse pathway encoding. in channel out networks pathways are not only formed a posteriori but they are also actively selected according to the inference outputs from the lower layers. from a mathematical perspective channel out networks can represent a wider class of piece wise continuous functions thereby endowing the network with more expressive power than that of maxout networks. we test our channel out networks on several well known image classification benchmarks setting new state of the art performance on cifar 100 and stl 10 which represent some of the harder image classification benchmarks.
competitive learning with feedforward supervisory signal for pre trained multilayered networks
we propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre trained input. the proposed method effectively uses rich input information in the earlier layer for robust leaning and revising internal representation in a multilayer neural network.
deeply supervised nets
our proposed deeply supervised nets dsn method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. we make an attempt to boost the classification performance by studying a new formulation in deep networks. three aspects in convolutional neural networks cnn style architectures are being looked at 1 transparency of the intermediate layers to the overall classification 2 discriminativeness and robustness of learned features especially in the early layers 3 effectiveness in training due to the presence of the exploding and vanishing gradients. we introduce companion objective to the individual hidden layers in addition to the overall objective at the output layer a different strategy to layer wise pre training . we extend techniques from stochastic gradient methods to analyze our algorithm. the advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods e.g. all state of the art results on mnist cifar 10 cifar 100 and svhn .
path sgd path normalized optimization in deep neural networks
we revisit the choice of sgd for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. we argue for a geometry invariant to rescaling of weights that does not affect the output of the network and suggest path sgd which is an approximate steepest descent method with respect to a path wise regularizer related to max norm regularization. path sgd is easy and efficient to implement and leads to empirical gains over sgd and adagrad.
adapting resilient propagation for deep learning
the resilient propagation rprop algorithm has been very popular for backpropagation training of multilayer feed forward neural networks in various applications. the standard rprop however encounters difficulties in the context of deep neural networks as typically happens with gradient based learning algorithms. in this paper we propose a modification of the rprop that combines standard rprop steps with a special drop out technique. we apply the method for training deep neural networks as standalone components and in ensemble formulations. results on the mnist dataset show that the proposed modification alleviates standard rprop s problems demonstrating improved learning speed and accuracy.
convolutional neural network for stereotypical motor movement detection in autism
autism spectrum disorders asds are often associated with specific atypical postural or motor behaviors of which stereotypical motor movements smms have a specific visibility. while the identification and the quantification of smm patterns remain complex its automation would provide support to accurate tuning of the intervention in the therapy of autism. therefore it is essential to develop automatic smm detection systems in a real world setting taking care of strong inter subject and intra subject variability. wireless accelerometer sensing technology can provide a valid infrastructure for real time smm detection however such variability remains a problem also for machine learning methods in particular whenever handcrafted features extracted from accelerometer signal are considered. here we propose to employ the deep learning paradigm in order to learn discriminating features from multi sensor accelerometer signals. our results provide preliminary evidence that feature learning and transfer learning embedded in the deep architecture achieve higher accurate smm detectors in longitudinal scenarios.
resnet in resnet generalizing residual architectures
residual networks resnets have recently achieved state of the art on challenging computer vision tasks. we introduce resnet in resnet rir a deep dual stream architecture that generalizes resnets and standard cnns and is easily implemented with no computational overhead. rir consistently improves performance over resnets outperforms architectures with similar amounts of augmentation on cifar 10 and establishes a new state of the art on cifar 100.
evolutionary synthesis of deep neural networks via synaptic cluster driven genetic encoding
there has been significant recent interest towards achieving highly efficient deep neural network architectures. a promising paradigm for achieving this is the concept of evolutionary deep intelligence which attempts to mimic biological evolution processes to synthesize highly efficient deep neural networks over successive generations. an important aspect of evolutionary deep intelligence is the genetic encoding scheme used to mimic heredity which can have a significant impact on the quality of offspring deep neural networks. motivated by the neurobiological phenomenon of synaptic clustering we introduce a new genetic encoding scheme where synaptic probability is driven towards the formation of a highly sparse set of synaptic clusters. experimental results for the task of image classification demonstrated that the synthesized offspring networks using this synaptic cluster driven genetic encoding scheme can achieve state of the art performance while having network architectures that are not only significantly more efficient with a 125 fold decrease in synapses for mnist compared to the original ancestor network but also tailored for gpu accelerated machine learning applications.
neural photo editing with introspective adversarial networks
the increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. we present the neural photo editor an interface that leverages the power of generative neural networks to make large semantically coherent changes to existing images. to tackle the challenge of achieving accurate reconstructions without loss of feature quality we introduce the introspective adversarial network a novel hybridization of the vae and gan. our model efficiently captures long range dependencies through use of a computational block based on weight shared dilated convolutions and improves generalization performance with orthogonal regularization a novel weight regularization method. we validate our contributions on celeba svhn and cifar 100 and produce samples and reconstructions with high visual fidelity.
adaptive neural networks for efficient inference
we present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. rather than attempting to redesign or approximate existing networks we propose two schemes that adaptively utilize networks. we first pose an adaptive network evaluation scheme where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. by allowing examples correctly classified using early layers of the system to exit we avoid the computational time associated with full evaluation of the network. we extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. we show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex computationally costly networks are only necessary for a small fraction of examples. we pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer by layer weighted binary classification problem. empirically these approaches yield dramatic reductions in computational cost with up to a 2.8x speedup on state of the art networks from the imagenet image recognition challenge with minimal 1 loss of top5 accuracy.
spatial variational auto encoding via matrix variate normal distributions
the key idea of variational auto encoders vaes resembles that of traditional auto encoder models in which spatial information is supposed to be explicitly encoded in the latent space. however the latent variables in vaes are vectors which are commonly interpreted as multiple feature maps of size 1x1. such representations can only convey spatial information implicitly when coupled with powerful decoders. in this work we propose spatial vaes that use latent variables as feature maps of larger size to explicitly capture spatial information. this is achieved by allowing the latent variables to be sampled from matrix variate normal mvn distributions whose parameters are computed from the encoder network. to increase dependencies among locations on latent feature maps and reduce the number of parameters we further propose spatial vaes via low rank mvn distributions. experimental results show that the proposed spatial vaes outperform original vaes in capturing rich structural and spatial information.
dense transformer networks
the key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel wise predictions. these methods are limited in the sense that the patches are determined by network architecture instead of learned from data. in this work we propose the dense transformer networks which can learn the shapes and sizes of patches from data. the dense transformer networks employ an encoder decoder architecture and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. the novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. the proposed dense transformer modules are differentiable thus the entire network can be trained. we apply the proposed networks on natural and biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods.
progressive learning for systematic design of large neural networks
we develop an algorithm for systematic design of a large artificial neural network using a progression property. we find that some non linear functions such as the rectifier linear unit and its derivatives hold the property. the systematic design addresses the choice of network size and regularization of parameters. the number of nodes and layers in network increases in progression with the objective of consistently reducing an appropriate cost. each layer is optimized at a time where appropriate parameters are learned using convex optimization. regularization parameters for convex optimization do not need a significant manual effort for tuning. we also use random instances for some weight matrices and that helps to reduce the number of parameters we learn. the developed network is expected to show good generalization power due to appropriate regularization and use of random weights in the layers. this expectation is verified by extensive experiments for classification and regression problems using standard databases.
a classification based perspective on gan distributions
a fundamental and still largely unanswered question in the context of generative adversarial networks gans is whether gans are actually able to capture the key characteristics of the datasets they are trained on. the current approaches to examining this issue require significant human supervision such as visual inspection of sampled images and often offer only fairly limited scalability. in this paper we propose new techniques that employ a classification based perspective to evaluate synthetic gan distributions and their capability to accurately reflect the essential properties of the training data. these techniques require only minimal human supervision and can easily be scaled and adapted to evaluate a variety of state of the art gans on large popular datasets. our analysis indicates that gans have significant problems in reproducing the more distributional properties of the training dataset. in particular when seen through the lens of classification the diversity of gan data is orders of magnitude less than that of the original data.
learning visual reasoning without strong priors
achieving artificial visual reasoning the ability to answer image related questions which require a multi step high level process is an important step towards artificial general intelligence. this multi modal task requires learning a question dependent structured reasoning process over images from language. standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure while leading methods learn to visually reason successfully but are hand crafted for reasoning. we show that a general purpose conditional batch normalization approach achieves state of the art results on the clevr visual reasoning benchmark with a 2.4 error rate. we outperform the next best end to end method 4.5 and even methods that use extra supervision 3.1 . we probe our model to shed light on how it reasons showing it has learned a question dependent multi step process. previous work has operated under the assumption that visual reasoning calls for a specialized architecture but we show that a general architecture with proper conditioning can learn to visually reason effectively.
men also like shopping reducing gender bias amplification using corpus level constraints
language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. structured prediction models are used in these tasks to take advantage of correlations between co occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. in this work we study data and models associated with multilabel object classification and visual semantic role labeling. we find that a datasets for these tasks contain significant gender bias and b models trained on these datasets further amplify existing bias. for example the activity cooking is over 33 more likely to involve females than males in a training set and a trained model further amplifies the disparity to 68 at test time. we propose to inject corpus level constraints for calibrating existing structured prediction models and design an algorithm based on lagrangian relaxation for collective inference. our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5 and 40.5 for multilabel classification and visual semantic role labeling respectively.
acquiring common sense spatial knowledge through implicit spatial templates
spatial understanding is a fundamental problem with wide reaching real world applications. the representation of spatial knowledge is often modeled with spatial templates i.e. regions of acceptability of two objects under an explicit spatial relationship e.g. on below etc. . in contrast with prior work that restricts spatial templates to explicit spatial prepositions e.g. glass on table here we extend this concept to implicit spatial language i.e. those relationships generally actions for which the spatial arrangement of the objects is only implicitly implied e.g. man riding horse . in contrast with explicit relationships predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. here we introduce the task of predicting spatial templates for two objects under a relationship which can be seen as a spatial question answering task with a 2d continuous output where is the man w.r.t. a horse when the man is walking the horse . we present two simple neural based models that leverage annotated images and structured text to learn this task. the good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. crucially the models attain similar performance in a challenging generalized setting where the object relation object combinations e.g. man walking dog have never been seen before. next we go one step further by presenting the models with unseen objects e.g. dog . in this scenario we show that leveraging word embeddings enables the models to output accurate spatial predictions proving that the models acquire solid common sense spatial knowledge allowing for such generalization.
film visual reasoning with a general conditioning layer
we introduce a general purpose conditioning method for neural networks called film feature wise linear modulation. film layers influence neural network computation via a simple feature wise affine transformation based on conditioning information. we show that film layers are highly effective for visual reasoning answering image related questions which require a multi step high level process a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. specifically we show on visual reasoning tasks that film layers 1 halve state of the art error for the clevr benchmark 2 modulate features in a coherent manner 3 are robust to ablations and architectural modifications and 4 generalize well to challenging new data from few examples or even zero shot.
unsupervised induction of semantic roles within a reconstruction error minimization framework
we introduce a new approach to unsupervised estimation of feature rich semantic role labeling models. our model consists of two components 1 an encoding component a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features 2 a reconstruction component a tensor factorization model which relies on roles to predict argument fillers. when the components are estimated jointly to minimize errors in argument reconstruction the induced roles largely correspond to roles defined in annotated resources. our method performs on par with most accurate role induction methods on english and german even though unlike these previous approaches we do not incorporate any prior linguistic knowledge about the languages.
man is to computer programmer as woman is to homemaker debiasing word embeddings
the blind application of machine learning runs the risk of amplifying biases present in data. such a danger is facing us with word embedding a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. we show that even word embeddings trained on google news articles exhibit female male gender stereotypes to a disturbing extent. this raises concerns because their widespread use as we describe often tends to amplify these biases. geometrically gender bias is first shown to be captured by a direction in the word embedding. second gender neutral words are shown to be linearly separable from gender definition words in the word embedding. using these properties we provide a methodology for modifying an embedding to remove gender stereotypes such as the association between between the words receptionist and female while maintaining desired associations such as between the words queen and female. we define metrics to quantify both direct and indirect gender biases in embeddings and develop algorithms to debias the embedding. using crowd worker evaluation as well as standard benchmarks we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. the resulting embeddings can be used in applications without amplifying gender bias.
topicrnn a recurrent neural network with long range semantic dependency
in this paper we propose topicrnn a recurrent neural network rnn based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. because of their sequential nature rnns are good at capturing the local structure of a word sequence both semantic and syntactic but might face difficulty remembering long range dependencies. intuitively these long range dependencies are of semantic nature. in contrast latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. the proposed topicrnn model integrates the merits of rnns and latent topic models it captures local syntactic dependencies using an rnn and global semantic dependencies using latent topics. unlike previous work on contextual rnn language modeling our model is learned end to end. empirical results on word prediction show that topicrnn outperforms existing contextual rnn baselines. in addition topicrnn can be used as an unsupervised feature extractor for documents. we do this for sentiment analysis on the imdb movie review dataset and report an error rate of 6.28 . this is comparable to the state of the art 5.91 resulting from a semi supervised approach. finally topicrnn also yields sensible topics making it a useful alternative to document models such as latent dirichlet allocation.
gaussian attention model and its application to knowledge base embedding and question answering
we propose the gaussian attention model for content based neural memory access. with the proposed attention model a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. it is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. we use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. the proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. on a dataset of soccer players who participated in the fifa world cup 2014 we demonstrate that our model can handle both path queries and conjunctive queries well.
variable computation in recurrent neural networks
recurrent neural networks rnns have been used extensively and with increasing success to model various types of sequential data. much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data such as long range dependency or localized attention phenomena. however while many sequential data such as video speech or language can have highly variable information flow most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step which can be detrimental to both speed and model capacity. in this paper we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step without prior knowledge of the sequence s time structure. we show experimentally that not only do our models require fewer operations they also lead to better performance overall on evaluation tasks.
learning to learn from weak supervision by full supervision
in this paper we propose a method for training neural networks when we have a large set of data with weak labels and a small amount of data with true labels. in our proposed model we train two neural networks a target network the learner and a confidence network the meta learner. the target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. we propose to control the magnitude of the gradient updates to the target network using the scores provided by the second confidence network which is trained on a small amount of supervised data. thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model.
smiles2vec an interpretable general purpose deep neural network for predicting chemical properties
chemical databases store information in text representations and the smiles format is a universal standard used in many cheminformatics software. encoded in each smiles string is structural information that can be used to predict complex chemical properties. in this work we develop smiles2vec a deep rnn that automatically learns features from smiles to predict chemical properties without the need for additional explicit feature engineering. using bayesian optimization methods to tune the network architecture we show that an optimized smiles2vec model can serve as a general purpose neural network for predicting distinct chemical properties including toxicity activity solubility and solvation energy while also outperforming contemporary mlp neural networks that uses engineered features. furthermore we demonstrate proof of concept of interpretability by developing an explanation mask that localizes on the most important characters used in making a prediction. when tested on the solubility dataset it identified specific parts of a chemical that is consistent with established first principles knowledge with an accuracy of 88 . our work demonstrates that neural networks can learn technically accurate chemical concept and provide state of the art accuracy making interpretable deep neural networks a useful tool of relevance to the chemical industry.
sample efficient deep reinforcement learning for dialogue systems with large action spaces
in spoken dialogue systems we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. a part of this effort is the policy optimisation task which attempts to find a policy describing how to respond to humans in the form of a function taking the current state of the dialogue and returning the response of the system. in this paper we investigate deep reinforcement learning approaches to solve this problem. particular attention is given to actor critic methods off policy reinforcement learning with experience replay and various methods aimed at reducing the bias and variance of estimators. when combined these methods result in the previously proposed acer algorithm that gave competitive results in gaming environments. these environments however are fully observable and have a relatively small action set so in this paper we examine the application of acer to dialogue policy optimisation. we show that this method beats the current state of the art in deep learning approaches for spoken dialogue systems. this not only leads to a more sample efficient algorithm that can train faster but also allows us to apply the algorithm in more difficult environments than before. we thus experiment with learning in a very large action space which has two orders of magnitude more actions than previously considered. we find that acer trains significantly faster than the current state of the art.
high dimensional vector semantics
in this paper we explore the vector semantics problem from the perspective of almost orthogonal property of high dimensional random vectors. we show that this intriguing property can be used to memorize random vectors by simply adding them and we provide an efficient probabilistic solution to the set membership problem. also we discuss several applications to word context vector embeddings document sentences similarity and spam filtering.
learning semantic script knowledge with event embeddings
induction of common sense knowledge about prototypical sequences of events has recently received much attention. instead of inducing this knowledge in the form of graphs as in much of the previous work in our method distributed representations of event realizations are computed based on distributed representations of predicates and their arguments and then these representations are used to predict prototypical event orderings. the parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from texts. we show that this approach results in a substantial boost in ordering performance with respect to previous methods.
mathematical language processing automatic grading and feedback for open response mathematical questions
while computer and communication technologies have provided effective means to scale up many aspects of education the submission and grading of assessments such as homework assignments and tests remains a weak link. in this paper we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in stem science technology engineering and mathematics courses. our data driven framework for mathematical language processing mlp leverages solution data from a large number of learners to evaluate the correctness of their solutions assign partial credit scores and provide feedback to each learner on the likely locations of any errors. mlp takes inspiration from the success of natural language processing for text data and comprises three main steps. first we convert each solution to an open response mathematical question into a series of numerical features. second we cluster the features from several solutions to uncover the structures of correct partially correct and incorrect solutions. we develop two different clustering approaches one that leverages generic clustering algorithms and one based on bayesian nonparametrics. third we automatically grade the remaining potentially large number of solutions based on their assigned cluster and one instructor provided grade per cluster. as a bonus we can track the cluster assignment of each step of a multistep solution and determine when it departs from a cluster of correct solutions which enables us to indicate the likely locations of errors to learners. we test and validate mlp on real world mooc data to demonstrate how it can substantially reduce the human effort required in large scale educational platforms.
nonparametric bayesian double articulation analyzer for direct language acquisition from continuous speech signals
human infants can discover words directly from unsegmented speech signals without any explicitly labeled data. in this paper we develop a novel machine learning method called nonparametric bayesian double articulation analyzer npb daa that can directly acquire language and acoustic models from observed continuous speech signals. for this purpose we propose an integrative generative model that combines a language model and an acoustic model into a single generative model called the hierarchical dirichlet process hidden language model hdp hlm . the hdp hlm is obtained by extending the hierarchical dirichlet process hidden semi markov model hdp hsmm proposed by johnson et al. an inference procedure for the hdp hlm is derived using the blocked gibbs sampler originally proposed for the hdp hsmm. this procedure enables the simultaneous and direct inference of language and acoustic models from continuous speech signals. based on the hdp hlm and its inference procedure we developed a novel double articulation analyzer. by assuming hdp hlm as a generative model of observed time series data and by inferring latent variables of the model the method can analyze latent double articulation structure i.e. hierarchically organized latent words and phonemes of the data in an unsupervised manner. the novel unsupervised double articulation analyzer is called npb daa. the npb daa can automatically estimate double articulation structure embedded in speech signals. we also carried out two evaluation experiments using synthetic data and actual human continuous speech signals representing japanese vowel sequences. in the word acquisition and phoneme categorization tasks the npb daa outperformed a conventional double articulation analyzer daa and baseline automatic speech recognition system whose acoustic model was trained in a supervised manner.
harnessing deep neural networks with logic rules
combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. we propose a general framework capable of enhancing various types of neural networks e.g. cnns and rnns with declarative first order logic rules. specifically we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. we deploy the framework on a cnn for sentiment analysis and an rnn for named entity recognition. with a few highly intuitive rules we obtain substantial improvements and achieve state of the art or comparable results to previous best performing systems.
toward controlled generation of text
generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. this paper aims at generating plausible natural language sentences whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. we propose a new neural generative model which combines variational auto encoders and holistic attribute discriminators for effective imposition of semantic structures. with differentiable approximation to discrete text samples explicit constraints on independent attribute controls and efficient collaborative learning of generator and discriminators our model learns highly interpretable representations from even only word annotations and produces realistic sentences with desired attributes. quantitative evaluation validates the accuracy of sentence and attribute generation.
adversarial connective exploiting networks for implicit discourse relation classification
implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues which motivates the use of annotated implicit connectives to improve the recognition. we propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives and thus encouraged to extract similarly salient features for accurate classification. we develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. our method effectively transfers discriminability of connectives to the implicit features and achieves state of the art performance on the pdtb benchmark.
abstract syntax networks for code generation and semantic parsing
tasks like code generation and semantic parsing require mapping unstructured or partially structured inputs to well formed executable outputs. we introduce abstract syntax networks a modeling framework for these problems. the outputs are represented as abstract syntax trees asts and constructed by a decoder with a dynamically determined modular structure paralleling the structure of the output tree. on the benchmark hearthstone dataset for code generation our model obtains 79.2 bleu and 22.7 exact match accuracy compared to previous state of the art values of 67.1 and 6.1 . furthermore we perform competitively on the atis jobs and geo semantic parsing datasets with no task specific engineering.
multimodal word distributions
word embeddings provide point representations of words containing useful semantic information. we introduce multimodal word distributions formed from gaussian mixtures for multiple word meanings entailment and rich uncertainty information. to learn these distributions we propose an energy based max margin objective. we show that the resulting approach captures uniquely expressive semantic information and outperforms alternatives such as word2vec skip grams and gaussian embeddings on benchmark datasets such as word similarity and entailment.
guiding reinforcement learning exploration using natural language
in this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments. this technique uses neural machine translation specifically the use of encoder decoder networks to learn associations between natural language behavior descriptions and state action information. we then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments. we evaluate this technique using the popular arcade game frogger under ideal and non ideal conditions. this evaluation shows that our modified policy shaping algorithm improves over a q learning agent as well as a baseline version of policy shaping.
robust task clustering for deep many task learning
we investigate task clustering for deep learning based multi task and few shot learning in a many task setting. we propose a new method to measure task similarities with cross task transfer performance matrix for the deep learning scenario. although this matrix provides us critical information regarding similarity between tasks its asymmetric property and unreliable performance scores can affect conventional clustering methods adversely. additionally the uncertain task pairs i.e. the ones with extremely asymmetric transfer scores may collectively mislead clustering algorithms to output an inaccurate task partition. to overcome these limitations we propose a novel task clustering algorithm by using the matrix completion technique. the proposed algorithm constructs a partially observed similarity matrix based on the certainty of cluster membership of the task pairs. we then use a matrix completion algorithm to complete the similarity matrix. our theoretical analysis shows that under mild constraints the proposed algorithm will perfectly recover the underlying true similarity matrix with a high probability. our results show that the new task clustering method can discover task clusters for training flexible and superior neural network models in a multi task learning setup for sentiment classification and dialog intent classification tasks. our task clustering approach also extends metric based few shot learning methods to adapt multiple metrics which demonstrates empirical advantages when the tasks are diverse.
natural language multitasking analyzing and improving syntactic saliency of hidden representations
we train multi task autoencoders on linguistic tasks and analyze the learned hidden sentence representations. the representations change significantly when translation and part of speech decoders are added. the more decoders a model employs the better it clusters sentences according to their syntactic similarity as the representation space becomes less entangled. we explore the structure of the representation space by interpolating between sentences which yields interesting pseudo english sentences many of which have recognizable syntactic structure. lastly we point out an interesting property of our models the difference vector between two sentences can be added to change a third sentence with similar features in a meaningful way.
multimodal sentiment analysis with word level fusion and reinforcement learning
with the increasing popularity of video sharing websites such as youtube and facebook multimodal sentiment analysis has received increasing attention from the scientific community. contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity we develop a novel deep architecture for multimodal sentiment analysis that performs modality fusion at the word level. in this paper we propose the gated multimodal embedding lstm with temporal attention gme lstm a model that is composed of 2 modules. the gated multimodal embedding alleviates the difficulties of fusion when there are noisy modalities. the lstm with temporal attention performs word level fusion at a finer fusion resolution between input modalities and attends to the most important time steps. as a result the gme lstm a is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. we demonstrate the effectiveness of this approach on the publicly available multimodal corpus of sentiment intensity and subjectivity analysis cmu mosi dataset by achieving state of the art sentiment classification and regression results. qualitative analysis on our model emphasizes the importance of the temporal attention layer in sentiment prediction because the additional acoustic and visual modalities are noisy. we also demonstrate the effectiveness of the gated multimodal embedding in selectively filtering these noisy modalities out. our results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.
a supervised approach to extractive summarisation of scientific papers
automatic summarisation is a popular approach to reduce a document to its main arguments. recent research in the area has focused on neural approaches to summarisation which can be very data hungry. however few large datasets exist and none for the traditionally popular domain of scientific publications which opens up challenging research avenues centered on encoding large complex documents. in this paper we introduce a new dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. we develop models on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best significantly outperforming well established baseline methods.
language models for image captioning the quirks and what works
two recent approaches have achieved state of the art results in image captioning. the first uses a pipelined process where a set of candidate words is generated by a convolutional neural network cnn trained on images and then a maximum entropy me language model is used to arrange these words into a coherent sentence. the second uses the penultimate activation layer of the cnn as input to a recurrent neural network rnn that then generates the caption sequence. in this paper we compare the merits of these different language modeling approaches for the first time by using the same state of the art cnn as input. we examine issues in the different approaches including linguistic irregularities caption repetition and data set overlap. by combining key aspects of the me and rnn methods we achieve a new record performance over previously published results on the benchmark coco dataset. however the gains we see in bleu do not translate to human judgments.
exploring models and data for image question answering
this work aims to address the problem of image based question answering qa with new models and datasets. in our work we propose to use neural networks and visual semantic embeddings without intermediate stages such as object detection and image segmentation to predict answers to simple questions about images. our model performs 1.8 times better than the only published results on an existing image qa dataset. we also present a question generation algorithm that converts image descriptions which are widely available into qa form. we used this algorithm to produce an order of magnitude larger dataset with more evenly distributed answers. a suite of baseline results on this new dataset are also presented.
making the v in vqa matter elevating the role of image understanding in visual question answering
problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. however inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities resulting in models that ignore visual information leading to an inflated sense of their capability. we propose to counter these language priors for the task of visual question answering vqa and make vision the v in vqa matter specifically we balance the popular vqa dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image but rather a pair of similar images that result in two different answers to the question. our dataset is by construction more balanced than the original vqa dataset and has approximately twice the number of image question pairs. our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the visual question answering dataset and challenge vqa v2.0 . we further benchmark a number of state of art vqa models on our balanced dataset. all models perform significantly worse on our balanced dataset suggesting that these models have indeed learned to exploit language priors. this finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. finally our data collection protocol for identifying complementary images enables us to develop a novel interpretable model which in addition to providing an answer to the given image question pair also provides a counter example based explanation. specifically it identifies an image that is similar to the original image but it believes has a different answer to the same question. this can help in building trust for machines among their users.
a multi world approach to question answering about real world scenes based on uncertain input
we propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. we combine discrete reasoning with uncertain predictions by a multi world approach that represents uncertainty about the perceived world in a bayesian framework. our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts object classes instances and lists of them. the system is directly trained from question answer pairs. we establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.
hard to cheat a turing test based on answering questions about images
progress in language and image understanding by machines has sparkled the interest of the research community in more open ended holistic tasks and refueled an old ai dream of building intelligent machines. we discuss a few prominent challenges that characterize such holistic tasks and argue for question answering about images as a particular appealing instance of such a holistic task. in particular we point out that it is a version of a turing test that is likely to be more robust to over interpretations and contrast it with tasks like grounding and generation of descriptions. finally we discuss tools to measure progress in this field.
analyzing the behavior of visual question answering models
recently a number of deep learning based models have been proposed for the task of visual question answering vqa . the performance of most models is clustered around 60 70 . in this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses and identifying the most fruitful directions for progress. we analyze two models one each from two major classes of vqa models with attention and without attention and show the similarities and differences in the behavior of these models. we also analyze the winning entry of the vqa challenge 2016. our behavior analysis reveals that despite recent progress today s vqa models are myopic tend to fail on sufficiently novel instances often jump to conclusions converge on a predicted answer after listening to just half the question and are stubborn do not change their answers across images .
sort story sorting jumbled images and captions into stories
temporal common sense has applications in ai tasks such as qa multi document summarization and human ai communication. we propose the task of sequencing given a jumbled set of aligned image caption pairs that belong to a story the task is to sort them such that the output sequence forms a coherent story. we present multiple approaches via unary position and pairwise order predictions and their ensemble based combinations achieving strong results on this task. we use both text based and image based features which depict complementary improvements. using qualitative examples we demonstrate that our models have learnt interesting aspects of temporal common sense.
mean box pooling a rich image representation and output embedding for the visual madlibs task
we present mean box pooling a novel visual representation that pools over cnn representations of a large number highly overlapping object proposals. we show that such representation together with ncca a successful multimodal embedding technique achieves state of the art performance on the visual madlibs task. moreover inspired by the ncca s objective function we extend classical cnn lstm approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. again such approach achieves a significant improvement over the prior work that also uses cnn lstm approach on visual madlibs.
learning to generalize to new compositions in image understanding
recurrent neural networks have recently been used for learning to describe images using natural language. however it has been observed that these models generalize poorly to scenes that were not observed during training possibly depending too strongly on the statistics of the text in the training data. here we propose to describe images using short structured representations aiming to capture the crux of a description. these structured representations allow us to tease out and evaluate separately two types of generalization standard generalization to new images with similar scenes and generalization to new combinations of known entities. we compare two learning approaches on the ms coco dataset a state of the art recurrent network based on an lstm show attend and tell and a simple structured prediction model on top of a deep network. we find that the structured model generalizes to new compositions substantially better than the lstm 7 times the accuracy of predicting structured representations. by providing a concrete method to quantify generalization for unseen combinations we argue that structured representations and compositional splits are a useful benchmark for image captioning and advocate compositional models that capture linguistic and visual structure.
measuring machine intelligence through visual question answering
as machines have become more intelligent there has been a renewed interest in methods for measuring their intelligence. a common approach is to propose tasks for which a human excels but one which machines find difficult. however an ideal task should also be easy to evaluate and not be easily gameable. we begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. an alternative and more promising task is visual question answering that tests a machine s ability to reason about language and vision. we describe a dataset unprecedented in size created for the task that contains over 760 000 human generated questions about images. using around 10 million human generated answers machines may be easily evaluated.
towards transparent ai systems interpreting visual question answering models
deep neural networks have shown striking progress and obtained state of the art results in many ai research fields in the recent years. however it is often unsatisfying to not know why they predict what they do. in this paper we address the problem of interpreting visual question answering vqa models. specifically we are interested in finding what part of the input pixels in images or words in questions the vqa model focuses on while answering the question. to tackle this problem we use two visualization techniques guided backpropagation and occlusion to find important words in the question and important regions in the image. we then present qualitative and quantitative analyses of these importance maps. we found that even without explicit attention mechanisms vqa models may sometimes be implicitly attending to relevant regions in the image and often to appropriate words in the question.
visual dialog
we introduce the task of visual dialog which requires an ai agent to hold a meaningful dialog with humans in natural conversational language about visual content. specifically given an image a dialog history and a question about the image the agent has to ground the question in image infer context from history and answer the question accurately. visual dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. we develop a novel two person chat data collection protocol to curate a large scale visual dialog dataset visdial . visdial v0.9 has been released and contains 1 dialog with 10 question answer pairs on 120k images from coco with a total of 1.2m dialog question answer pairs. we introduce a family of neural encoder decoder models for visual dialog with 3 encoders late fusion hierarchical recurrent encoder and memory network and 2 decoders generative and discriminative which outperform a number of sophisticated baselines. we propose a retrieval based evaluation protocol for visual dialog where the ai agent is asked to sort a set of candidate answers and evaluated on metrics such as mean reciprocal rank of human response. we quantify gap between machine and human performance on the visual dialog task via human studies. putting it all together we demonstrate the first visual chatbot our dataset code trained models and visual chatbot are available on https visualdialog.org
multi task learning of deep neural networks for audio visual automatic speech recognition
multi task learning mtl involves the simultaneous training of two or more related tasks over shared representations. in this work we apply mtl to audio visual automatic speech recognition av asr . our primary task is to learn a mapping between audio visual fused features and frame labels obtained from acoustic gmm hmm model. this is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual gmm hmm model. the mtl model is tested at various levels of babble noise and the results are compared with a base line hybrid dnn hmm av asr model. our results indicate that mtl is especially useful at higher level of noise. compared to base line upto 7 relative improvement in wer is reported at 3 snr db
learning cooperative visual dialog agents with deep reinforcement learning
we introduce the first goal driven training for visual question answering and dialog agents. specifically we pose a cooperative image guessing game between two agents qbot and abot who communicate in natural language dialog so that qbot can select an unseen image from a lineup of images. we use deep reinforcement learning rl to learn the policies of these agents end to end from pixels to multi agent multi round dialog to game reward. we demonstrate two experimental results. first as a sanity check demonstration of pure rl from scratch we show results on a synthetic world where the agents communicate in ungrounded vocabulary i.e. symbols with no pre specified meanings x y z . we find that two bots invent their own communication protocol and start using certain symbols to ask answer about certain visual attributes shape color style . thus we demonstrate the emergence of grounded language and communication among visual dialog agents with no human supervision. second we conduct large scale real image experiments on the visdial dataset where we pretrain with supervised dialog data and show that the rl fine tuned agents significantly outperform sl agents. interestingly the rl qbot learns to ask questions that abot is good at ultimately resulting in more informative dialog and a better team.
being negative but constructively lessons learnt from creating better visual question answering datasets
visual question answering qa has attracted a lot of attention lately seen essentially as a form of visual turing test that artificial intelligence should strive to achieve. in this paper we study a crucial component of this task how can we design good datasets for the task we focus on the design of multiple choice based datasets where the learner has to select the right answer from a set of candidate ones including the target i.e. the correct one and the decoys i.e. the incorrect ones . through careful analysis of the results attained by state of the art learning models and human annotators on existing datasets we show the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. in particular the resulting learner can ignore the visual information the question or the both while still doing well on the task. inspired by this we propose automatic procedures to remedy such design deficiencies. we apply the procedures to re construct decoy answers for two popular visual qa datasets as well as to create a new visual qa dataset from the visual genome project resulting in the largest dataset for this task. extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. the datasets are released and publicly available via http www.teds.usc.edu website vqa .
c vqa a compositional split of the visual question answering vqa v1.0 dataset
visual question answering vqa has received a lot of attention over the past couple of years. a number of deep learning models have been proposed for this task. however it has been shown that these models are heavily driven by superficial correlations in the training data and lack compositionality the ability to answer questions about unseen compositions of seen concepts. this compositionality is desirable and central to intelligence. in this paper we propose a new setting for visual question answering where the test question answer pairs are compositionally novel compared to training question answer pairs. to facilitate developing models under this setting we present a new compositional split of the vqa v1.0 dataset which we call compositional vqa c vqa . we analyze the distribution of questions and answers in the c vqa splits. finally we evaluate several existing vqa models under this new setting and show that the performances of these models degrade by a significant amount compared to the original vqa setting.
deep learning evaluation using deep linguistic processing
we discuss problems with the standard approaches to evaluation for tasks like visual question answering and argue that artificial data can be used to address these as a complement to current practice. we demonstrate that with the help of existing deep linguistic processing technology we are able to create challenging abstract datasets which enable us to investigate the language understanding abilities of multimodal deep learning models in detail.
meprop sparsified back propagation for accelerated deep learning with reduced overfitting
we propose a simple yet effective technique for neural network learning. the forward propagation is computed as usual. in back propagation only a small subset of the full gradient is computed to update the model parameters. the gradient vectors are sparsified in such a way that only the top k elements in terms of magnitude are kept. as a result only k rows or columns depending on the layout of the weight matrix are modified leading to a linear reduction k divided by the vector dimension in the computational cost. surprisingly experimental results demonstrate that we can update only 1 4 of the weights at each back propagation pass. this does not result in a larger number of training iterations. more interestingly the accuracy of the resulting models is actually improved rather than degraded and a detailed analysis is given. the code is available at https github.com jklj077 meprop
towards crafting text adversarial samples
adversarial samples are strategically modified samples which are crafted with the purpose of fooling a classifier at hand. an attacker introduces specially crafted adversarial samples to a deployed classifier which are being mis classified by the classifier. however the samples are perceived to be drawn from entirely different classes and thus it becomes hard to detect the adversarial samples. most of the prior works have been focused on synthesizing adversarial samples in the image domain. in this paper we propose a new method of crafting adversarial text samples by modification of the original samples. modifications of the original text samples are done by deleting or replacing the important or salient words in the text or by introducing new words in the text sample. our algorithm works best for the datasets which have sub categories within each of the classes of examples. while crafting adversarial samples one of the key constraint is to generate meaningful sentences which can at pass off as legitimate from language english viewpoint. experimental results on imdb movie review dataset for sentiment analysis and twitter dataset for gender detection show the efficiency of our proposed method.
reinforced video captioning with entailment rewards
sequence to sequence models have shown promising improvements on the temporal task of video captioning but they optimize word level cross entropy loss during training. first using policy gradient and mixed loss methods for reinforcement learning we directly optimize sentence level task based metrics as rewards achieving significant improvements over the baseline based on both automatic metrics and human evaluation on multiple datasets. next we propose a novel entailment enhanced reward cident that corrects phrase matching based metrics such as cider to only allow for logically implied partial matches and avoid contradictions achieving further significant improvements over the cider reward model. overall our cident reward model achieves the new state of the art on the msr vtt dataset.
hierarchically attentive rnn for album summarization and storytelling
we address the problem of end to end visual storytelling. given a photo album our model first selects the most representative summary photos and then composes a natural language story for the album. for this task we make use of the visual storytelling dataset and a model composed of three hierarchically attentive recurrent neural nets rnns to encode the album photos select representative summary photos and compose the story. automatic and human evaluations show our model achieves better performance on selection generation and retrieval than baselines.
generating natural adversarial examples
due to their complex nature it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. recent work on adversarial examples i.e. inputs with minor perturbations that result in substantially different model predictions is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. however these malicious perturbations are often unnatural not semantically meaningful and not applicable to complicated domains such as language. in this paper we propose a framework to generate natural and legible adversarial examples that lie on the data manifold by searching in semantic space of dense and continuous data representation utilizing the recent advances in generative adversarial networks. we present generated adversaries to demonstrate the potential of the proposed approach for black box classifiers for a wide range of applications such as image classification textual entailment and machine translation. we include experiments to show that the generated adversaries are natural legible to humans and useful in evaluating and analyzing black box classifiers.
training simplification and model simplification for deep learning a minimal effort back propagation method
we propose a simple yet effective technique to simplify the training and the resulting model of neural networks. in back propagation only a small subset of the full gradient is computed to update the model parameters. the gradient vectors are sparsified in such a way that only the top k elements in terms of magnitude are kept. as a result only k rows or columns depending on the layout of the weight matrix are modified leading to a linear reduction in the computational cost. based on the sparsified gradients we further simplify the model by eliminating the rows or columns that are seldom updated which will reduce the computational cost both in the training and decoding and potentially accelerate decoding in real world applications. surprisingly experimental results demonstrate that most of time we only need to update fewer than 5 of the weights at each back propagation pass. more interestingly the accuracy of the resulting models is actually improved rather than degraded and a detailed analysis is given. the model simplification results show that we could adaptively simplify the model which could often be reduced by around 9x without any loss on accuracy or even with improved accuracy.
embodied question answering
we present a new ai task embodied question answering embodiedqa where an agent is spawned at a random location in a 3d environment and asked a question what color is the car . in order to answer the agent must first intelligently navigate to explore the environment gather information through first person egocentric vision and then answer the question orange . this challenging task requires a range of ai skills active perception language understanding goal driven navigation commonsense reasoning and grounding of language into actions. in this work we develop the environments end to end trained reinforcement learning agents and evaluation protocols for embodiedqa.
don t just assume look and answer overcoming priors for visual question answering
a number of studies have found that today s visual question answering vqa models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. to encourage development of models geared towards the latter we propose a new setting for vqa where for every question type train and test sets have different prior distributions of answers. specifically we present new splits of the vqa v1 and vqa v2 datasets which we call visual question answering under changing priors vqa cp v1 and vqa cp v2 respectively . first we evaluate several existing vqa models under this new setting and show that their performance degrades significantly compared to the original vqa setting. second we propose a novel grounded visual question answering model gvqa that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from cheating by primarily relying on priors in the training data. specifically gvqa explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question enabling the model to more robustly generalize across different distributions of answers. gvqa is built off an existing vqa model stacked attention networks san . our experiments demonstrate that gvqa significantly outperforms san on both vqa cp v1 and vqa cp v2 datasets. interestingly it also outperforms more powerful vqa models such as multimodal compact bilinear pooling mcb in several cases. gvqa offers strengths complementary to san when trained and evaluated on the original vqa v1 and vqa v2 datasets. finally gvqa is more transparent and interpretable than existing vqa models.
codraw visual dialog for collaborative drawing
in this work we propose a goal driven collaborative task that contains vision language and action in a virtual environment as its core components. specifically we develop a collaborative image drawing game between two agents called codraw. our game is grounded in a virtual world that contains movable clip art objects. two players teller and drawer are involved. the teller sees an abstract scene containing multiple clip arts in a semantically meaningful configuration while the drawer tries to reconstruct the scene on an empty canvas using available clip arts. the two players communicate via two way communication using natural language. we collect the codraw dataset of 10k dialogs consisting of 138k messages exchanged between a teller and a drawer from amazon mechanical turk amt . we analyze our dataset and present three models to model the players behaviors including an attention model to describe and draw multiple clip arts at each round. the attention models are quantitatively compared to the other models to show how the conventional approaches work for this new task. we also present qualitative visualizations.
answerer in questioner s mind for goal oriented visual dialogue
goal oriented dialogue has been paid attention for its numerous applications in artificial intelligence. to solve this task deep learning and reinforcement learning have recently been applied. however these approaches struggle to find a competent recurrent neural questioner owing to the complexity of learning a series of sentences. motivated by theory of mind we propose answerer in questioner s mind aqm a novel algorithm for goal oriented dialogue. with aqm a questioner asks and infers based on an approximated probabilistic model of the answerer. the questioner figures out the answerer s intent via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question. we test our framework on two goal oriented visual dialogue tasks mnist counting dialog and guesswhat . in our experiments aqm outperforms comparative algorithms and makes human like dialogue. we further use aqm as a tool for analyzing the mechanism of deep reinforcement learning approach and discuss the future direction of practical goal oriented neural dialogue systems.
resource constrained structured prediction
we study the problem of structured prediction under test time budget constraints. we propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. our approach seeks to adaptively generate computationally costly features during test time in order to reduce the computational cost of prediction while maintaining prediction performance. we show that training the adaptive feature generation system can be reduced to a series of structured learning problems resulting in efficient training using existing structured learning algorithms. this framework provides theoretical justification for several existing heuristic approaches found in literature. we evaluate our proposed adaptive system on two structured prediction tasks optical character recognition ocr and dependency parsing and show strong performance in reduction of the feature costs without degrading accuracy.
listen attend and walk neural mapping of navigational instructions to action sequences
we propose a neural sequence to sequence model for direction following a task that is essential to realizing effective autonomous agents. our alignment based encoder decoder model with long short term memory recurrent neural networks lstm rnn translates natural language instructions to action sequences based upon a representation of the observable world state. we introduce a multi level aligner that empowers our model to focus on sentence regions salient to the current world state by using multiple abstractions of the input sentence. in contrast to existing methods our model uses no specialized linguistic resources e.g. parsers or task specific annotations e.g. seed lexicons . it is therefore generalizable yet still achieves the best results reported to date on a benchmark single sentence dataset and competitive results for the limited training multi sentence setting. we analyze our model through a series of ablations that elucidate the contributions of the primary components of our model.
coupling distributed and symbolic execution for natural language queries
building neural networks to query a knowledge base a table with natural language is an emerging research topic in deep learning. an executor for table querying typically requires multiple steps of execution because queries may have complicated structures. in previous studies researchers have developed either fully distributed executors or symbolic executors for table querying. a distributed executor can be trained in an end to end fashion but is weak in terms of execution efficiency and explicit interpretability. a symbolic executor is efficient in execution but is very difficult to train especially at initial stages. in this paper we propose to couple distributed and symbolic execution for natural language queries where the symbolic executor is pretrained with the distributed executor s intermediate execution results in a step by step fashion. experiments show that our approach significantly outperforms both distributed and symbolic executors exhibiting high accuracy high learning efficiency high execution efficiency and high interpretability.
an agent driven semantical identifier using radial basis neural networks and reinforcement learning
due to the huge availability of documents in digital form and the deception possibility raise bound to the essence of digital documents and the way they are spread the authorship attribution problem has constantly increased its relevance. nowadays authorship attribution for both information retrieval and analysis has gained great importance in the context of security trust and copyright preservation. this work proposes an innovative multi agent driven machine learning technique that has been developed for authorship attribution. by means of a preprocessing for word grouping and time period related analysis of the common lexicon we determine a bias reference level for the recurrence frequency of the words within analysed texts and then train a radial basis neural networks rbpnn based classifier to identify the correct author. the main advantage of the proposed approach lies in the generality of the semantic analysis which can be applied to different contexts and lexical domains without requiring any modification. moreover the proposed system is able to incorporate an external input meant to tune the classifier and then self adjust by means of continuous learning reinforcement.
where is my forearm clustering of body parts from simultaneous tactile and linguistic input using sequential mapping
humans and animals are constantly exposed to a continuous stream of sensory information from different modalities. at the same time they form more compressed representations like concepts or symbols. in species that use language this process is further structured by this interaction where a mapping between the sensorimotor concepts and linguistic elements needs to be established. there is evidence that children might be learning language by simply disambiguating potential meanings based on multiple exposures to utterances in different contexts cross situational learning . in existing models the mapping between modalities is usually found in a single step by directly using frequencies of referent and meaning co occurrences. in this paper we present an extension of this one step mapping and introduce a newly proposed sequential mapping algorithm together with a publicly available matlab implementation. for demonstration we have chosen a less typical scenario instead of learning to associate objects with their names we focus on body representations. a humanoid robot is receiving tactile stimulations on its body while at the same time listening to utterances of the body part names e.g. hand forearm and torso . with the goal at arriving at the correct body categories we demonstrate how a sequential mapping algorithm outperforms one step mapping. in addition the effect of data set size and noise in the linguistic input are studied.
improvements to deep convolutional neural networks for lvcsr
deep convolutional neural networks cnns are more powerful than deep neural networks dnn as they are able to better reduce spectral variation in the input signal. this has also been confirmed experimentally with cnns showing improvements in word error rate wer between 4 12 relative compared to dnns across a variety of lvcsr tasks. in this paper we describe different methods to further improve cnn performance. first we conduct a deep analysis comparing limited weight sharing and full weight sharing with state of the art features. second we apply various pooling strategies that have shown improvements in computer vision to an lvcsr speech task. third we introduce a method to effectively incorporate speaker adaptation namely fmllr into log mel features. fourth we introduce an effective strategy to use dropout during hessian free sequence training. we find that with these improvements particularly with fmllr and dropout we are able to achieve an additional 2 3 relative improvement in wer on a 50 hour broadcast news task over our previous best cnn baseline. on a larger 400 hour bn task we find an additional 4 5 relative improvement over our previous best cnn baseline.
collaborative deep learning for recommender systems
collaborative filtering cf is a successful approach commonly used by many recommender systems. conventional cf based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. however the ratings are often very sparse in many applications causing cf based methods to degrade significantly in their recommendation performance. to address this sparsity problem auxiliary information such as item content information may be utilized. collaborative topic regression ctr is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information. nevertheless the latent representation learned by ctr may not be very effective when the auxiliary information is very sparse. to address this problem we generalize recent advances in deep learning from i.i.d. input to non i.i.d. cf based input and propose in this paper a hierarchical bayesian model called collaborative deep learning cdl which jointly performs deep representation learning for the content information and collaborative filtering for the ratings feedback matrix. extensive experiments on three real world datasets from different domains show that cdl can significantly advance the state of the art.
explaining predictions of non linear classifiers in nlp
layer wise relevance propagation lrp is a recently proposed technique for explaining predictions of complex non linear classifiers in terms of input variables. in this paper we apply lrp for the first time to natural language processing nlp . more precisely we use it to explain the predictions of a convolutional neural network cnn trained on a topic categorization task. our analysis highlights which words are relevant for a specific prediction of the cnn. we compare our technique to standard sensitivity analysis both qualitatively and quantitatively using a word deleting perturbation experiment a pca analysis and various visualizations. all experiments validate the suitability of lrp for explaining the cnn predictions which is also in line with results reported in recent image classification studies.
tensor network language model
we propose a new statistical model suitable for machine learning of systems with long distance correlations such as natural languages. the model is based on directed acyclic graph decorated by multi linear tensor maps in the vertices and vector spaces in the edges called tensor network. such tensor networks have been previously employed for effective numerical computation of the renormalization group flow on the space of effective quantum field theories and lattice models of statistical mechanics. we provide explicit algebro geometric analysis of the parameter moduli space for tree graphs discuss model properties and applications such as statistical translation.
language as a matrix product state
we propose a statistical model for natural language that begins by considering language as a monoid then representing it in complex matrices with a compatible translation invariant probability measure. we interpret the probability measure as arising via the born rule from a translation invariant matrix product state.
accelerating hessian free optimization for deep neural networks by implicit preconditioning and sampling
hessian free training has become a popular parallel second or der optimization technique for deep neural network training. this study aims at speeding up hessian free training both by means of decreasing the amount of data used for training as well as through reduction of the number of krylov subspace solver iterations used for implicit estimation of the hessian. in this paper we develop an l bfgs based preconditioning scheme that avoids the need to access the hessian explicitly. since l bfgs cannot be regarded as a fixed point iteration we further propose the employment of flexible krylov subspace solvers that retain the desired theoretical convergence guarantees of their conventional counterparts. second we propose a new sampling algorithm which geometrically increases the amount of data utilized for gradient and krylov subspace iteration calculations. on a 50 hr english broadcast news task we find that these methodologies provide roughly a 1.5x speed up whereas on a 300 hr switchboard task these techniques provide over a 2.3x speedup with no loss in wer. these results suggest that even further speed up is expected as problems scale and complexity grows.
is a picture worth ten thousand words in a review dataset 
while textual reviews have become prominent in many recommendation based systems automated frameworks to provide relevant visual cues against text reviews where pictures are not available is a new form of task confronted by data mining and machine learning researchers. suggestions of pictures that are relevant to the content of a review could significantly benefit the users by increasing the effectiveness of a review. we propose a deep learning based framework to automatically 1 tag the images available in a review dataset 2 generate a caption for each image that does not have one and 3 enhance each review by recommending relevant images that might not be uploaded by the corresponding reviewer. we evaluate the proposed framework using the yelp challenge dataset. while a subset of the images in this particular dataset are correctly captioned the majority of the pictures do not have any associated text. moreover there is no mapping between reviews and images. each image has a corresponding business tag where the picture was taken though. the overall data setting and unavailability of crucial pieces required for a mapping make the problem of recommending images for reviews a major challenge. qualitative and quantitative evaluations indicate that our proposed framework provides high quality enhancements through automatic captioning tagging and recommendation for mapping reviews and images.
validation of nonlinear pca
linear principal component analysis pca can be extended to a nonlinear pca by using artificial neural networks. but the benefit of curved components requires a careful control of the model complexity. moreover standard techniques for model selection including cross validation and more generally the use of an independent test set fail when applied to nonlinear pca because of its inherent unsupervised characteristics. this paper presents a new approach for validating the complexity of nonlinear pca models by using the error in missing data estimation as a criterion for model selection. it is motivated by the idea that only the model of optimal complexity is able to predict missing values with the highest accuracy. while standard test set validation usually favours over fitted nonlinear pca models the proposed model validation approach correctly selects the optimal model complexity.
graph approximation and clustering on a budget
we consider the problem of learning from a similarity matrix such as spectral clustering and lowd imensional embedding when computing pairwise similarities are costly and only a limited number of entries can be observed. we provide a theoretical analysis using standard notions of graph approximation significantly generalizing previous results which focused on spectral clustering with two clusters . we also propose a new algorithmic approach based on adaptive sampling which experimentally matches or improves on previous methods while being considerably more general and computationally cheaper.
shareboost efficient multiclass learning with feature sharing
multiclass prediction is the problem of classifying an object into a relevant target class. we consider the problem of learning a multiclass predictor that uses only few features and in particular the number of used features should increase sub linearly with the number of possible classes. this implies that features should be shared by several classes. we describe and analyze the shareboost algorithm for learning a multiclass predictor that uses few shared features. we prove that shareboost efficiently finds a predictor that uses few shared features if such a predictor exists and that it has a small generalization error. we also describe how to use shareboost for learning a non linear predictor that has a fast evaluation time. in a series of experiments with natural data sets we demonstrate the benefits of shareboost and evaluate its success relatively to other state of the art approaches.
functional principal component analysis and randomized sparse clustering algorithm for medical image analysis
due to advances in sensors growing large and complex medical image data have the ability to visualize the pathological change in the cellular or even the molecular level or anatomical changes in tissues and organs. as a consequence the medical images have the potential to enhance diagnosis of disease prediction of clinical outcomes characterization of disease progression management of health care and development of treatments but also pose great methodological and computational challenges for representation and selection of features in image cluster analysis. to address these challenges we first extend one dimensional functional principal component analysis to the two dimensional functional principle component analyses 2dfpca to fully capture space variation of image signals. image signals contain a large number of redundant and irrelevant features which provide no additional or no useful information for cluster analysis. widely used methods for removing redundant and irrelevant features are sparse clustering algorithms using a lasso type penalty to select the features. however the accuracy of clustering using a lasso type penalty depends on how to select penalty parameters and a threshold for selecting features. in practice they are difficult to determine. recently randomized algorithms have received a great deal of attention in big data analysis. this paper presents a randomized algorithm for accurate feature selection in image cluster analysis. the proposed method is applied to ovarian and kidney cancer histology image data from the tcga database. the results demonstrate that the randomized feature selection method coupled with functional principal component analysis substantially outperforms the current sparse clustering algorithms in image cluster analysis.
jointly learning multiple measures of similarities from triplet comparisons
similarity between objects is multi faceted and it can be easier for human annotators to measure it when the focus is on a specific aspect. we consider the problem of mapping objects into view specific embeddings where the distance between them is consistent with the similarity comparisons of the form from the t th view object a is more similar to b than to c . our framework jointly learns view specific embeddings exploiting correlations between views. experiments on a number of datasets including one of multi view crowdsourced comparison on bird images show the proposed method achieves lower triplet generalization error when compared to both learning embeddings independently for each view and all views pooled into one view. our method can also be used to learn multiple measures of similarity over input features taking class labels into account and compares favorably to existing approaches for multi task metric learning on the isolet dataset.
variational inference for uncertainty on the inputs of gaussian process models
the gaussian process latent variable model gp lvm provides a flexible approach for non linear dimensionality reduction that has been widely applied. however the current approach for training gp lvms is based on maximum likelihood where the latent projection variables are maximized over rather than integrated out. in this paper we present a bayesian method for training gp lvms by introducing a non standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a gp lvm by maximizing an analytic lower bound on the exact marginal likelihood. we apply this method for learning a gp lvm from iid observations and for learning non linear dynamical systems where the observations are temporally correlated. we show that a benefit of the variational bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the nonlinear latent space. the resulting framework is generic flexible and easy to extend for other purposes such as gaussian process regression with uncertain inputs and semi supervised gaussian processes. we demonstrate our method on synthetic data and standard machine learning benchmarks as well as challenging real world datasets including high resolution video data.
conditional generative adversarial nets
generative adversarial nets 8 were recently introduced as a novel way to train generative models. in this work we introduce the conditional version of generative adversarial nets which can be constructed by simply feeding the data y we wish to condition on to both the generator and discriminator. we show that this model can generate mnist digits conditioned on class labels. we also illustrate how this model could be used to learn a multi modal model and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.
visual causal feature learning
we provide a rigorous definition of the visual cause of a behavior that is broadly applicable to the visually driven behavior in humans animals neurons robots and other perceiving systems. our framework generalizes standard accounts of causal learning to settings in which the causal variables need to be constructed from micro variables. we prove the causal coarsening theorem which allows us to gain causal knowledge from observational data with minimal experimental effort. the theorem provides a connection to standard inference techniques in machine learning that identify features of an image that correlate with but may not cause the target behavior. finally we propose an active learning scheme to learn a manipulator function that performs optimal manipulations on the image to automatically identify the visual cause of a target behavior. we illustrate our inference and learning algorithms in experiments based on both synthetic and real data.
in search of the real inductive bias on the role of implicit regularization in deep learning
we present experiments demonstrating that some other form of capacity control different from network size plays a central role in learning multilayer feed forward networks. we argue partially through analogy to matrix factorization that this is an inductive bias that can help shed light on deep learning.
domain generalization for object recognition with multi task autoencoders
the problem of domain generalization is to take knowledge acquired from a number of related domains where training data is available and to then successfully apply it to previously unseen domains. we propose a new feature learning algorithm multi task autoencoder mtae that provides good generalization performance for cross domain object recognition. our algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter domain variability in the appearance of objects. instead of reconstructing images from noisy versions mtae learns to transform the original image into analogs in multiple related domains. it thereby learns features that are robust to variations across domains. the learnt features are then used as inputs to a classifier. we evaluated the performance of the algorithm on benchmark image recognition datasets where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets. we found that denoising mtae outperforms alternative autoencoder based models as well as the current state of the art algorithms for domain generalization.
data efficient learning of feedback policies from image pixels using deep dynamical models
data efficient reinforcement learning rl in continuous state action spaces using very high dimensional observations remains a key challenge in developing fully autonomous systems. we consider a particularly important instance of this challenge the pixels to torques problem where an rl agent learns a closed loop control policy torques from pixel information only. we introduce a data efficient model based reinforcement learning algorithm that learns such a closed loop policy directly from pixel information. the key ingredient is a deep dynamical model for learning a low dimensional feature embedding of images jointly with a predictive model in this low dimensional feature space. joint learning is crucial for long term predictions which lie at the core of the adaptive nonlinear model predictive control strategy that we use for closed loop control. compared to state of the art rl methods for continuous states and actions our approach learns quickly scales to high dimensional state spaces is lightweight and an important step toward fully autonomous end to end learning from pixels to torques.
scatter component analysis a unified framework for domain adaptation and domain generalization
this paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from but related to the target. two closely related frameworks domain adaptation and domain generalization are concerned with such tasks where the only difference between those frameworks is the availability of the unlabeled target data domain adaptation can leverage unlabeled target information while domain generalization cannot. we propose scatter component analyis sca a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. sca is based on a simple geometrical measure i.e. scatter which operates on reproducing kernel hilbert space. sca finds a representation that trades between maximizing the separability of classes minimizing the mismatch between domains and maximizing the separability of data each of which is quantified through scatter. the optimization problem of sca can be reduced to a generalized eigenvalue problem which results in a fast and exact solution. comprehensive experiments on benchmark cross domain object recognition datasets verify that sca performs much faster than several state of the art algorithms and also provides state of the art classification accuracy in both domain adaptation and domain generalization. we also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation.
robust subspace clustering via tighter rank approximation
matrix rank minimization problem is in general np hard. the nuclear norm is used to substitute the rank function in many recent studies. nevertheless the nuclear norm approximation adds all singular values together and the approximation error may depend heavily on the magnitudes of singular values. this might restrict its capability in dealing with many practical problems. in this paper an arctangent function is used as a tighter approximation to the rank function. we use it on the challenging subspace clustering problem. for this nonconvex minimization problem we develop an effective optimization procedure based on a type of augmented lagrange multipliers alm method. extensive experiments on face clustering and motion segmentation show that the proposed method is effective for rank approximation.
recognizing semantic features in faces using deep learning
the human face constantly conveys information both consciously and subconsciously. however as basic as it is for humans to visually interpret this information it is quite a big challenge for machines. conventional semantic facial feature recognition and analysis techniques are already in use and are based on physiological heuristics but they suffer from lack of robustness and high computation time. this thesis aims to explore ways for machines to learn to interpret semantic information available in faces in an automated manner without requiring manual design of feature detectors using the approach of deep learning. this thesis provides a study of the effects of various factors and hyper parameters of deep neural networks in the process of determining an optimal network configuration for the task of semantic facial feature recognition. this thesis explores the effectiveness of the system to recognize the various semantic features like emotions age gender ethnicity etc. present in faces. furthermore the relation between the effect of high level concepts on low level features is explored through an analysis of the similarities in low level descriptors of different semantic features. this thesis also demonstrates a novel idea of using a deep network to generate 3 d active appearance models of faces from real world 2 d images. for a more detailed report on this work please see arxiv 1512.00743v1 .
deep reconstruction classification networks for unsupervised domain adaptation
in this paper we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition. specifically we design a new model called deep reconstruction classification network drcn which jointly learns a shared encoding representation for two tasks i supervised classification of labeled source data and ii unsupervised reconstruction of unlabeled target data.in this way the learnt representation not only preserves discriminability but also encodes useful information from the target domain. our new drcn model can be optimized by using backpropagation similarly as the standard neural networks. we evaluate the performance of drcn on a series of cross domain object recognition tasks where drcn provides a considerable improvement up to 8 in accuracy over the prior state of the art algorithms. interestingly we also observe that the reconstruction pipeline of drcn transforms images from the source domain into images whose appearance resembles the target dataset. this suggests that drcn s performance is due to constructing a single composite representation that encodes information about both the structure of target images and the classification of source images. finally we provide a formal analysis to justify the algorithm s objective in domain adaptation context.
a convolutional autoencoder for multi subject fmri data aggregation
finding the most effective way to aggregate multi subject fmri data is a long standing and challenging problem. it is of increasing interest in contemporary fmri studies of human cognition due to the scarcity of data per subject and the variability of brain anatomy and functional response across subjects. recent work on latent factor models shows promising results in this task but this approach does not preserve spatial locality in the brain. we examine two ways to combine the ideas of a factor model and a searchlight based analysis to aggregate multi subject fmri data while preserving spatial locality. we first do this directly by combining a recent factor method known as a shared response model with searchlight analysis. then we design a multi view convolutional autoencoder for the same task. both approaches preserve spatial locality and have competitive or better performance compared with standard searchlight analysis and the shared response model applied across the whole brain. we also report a system design to handle the computational challenge of training the convolutional autoencoder.
feedback controlled sequential lasso screening
one way to solve lasso problems when the dictionary does not fit into available memory is to first screen the dictionary to remove unneeded features. prior research has shown that sequential screening methods offer the greatest promise in this endeavor. most existing work on sequential screening targets the context of tuning parameter selection where one screens and solves a sequence of n lasso problems with a fixed grid of geometrically spaced regularization parameters. in contrast we focus on the scenario where a target regularization parameter has already been chosen via cross validated model selection and we then need to solve many lasso instances using this fixed value. in this context we propose and explore a feedback controlled sequential screening scheme. feedback is used at each iteration to select the next problem to be solved. this allows the sequence of problems to be adapted to the instance presented and the number of intermediate problems to be automatically selected. we demonstrate our feedback scheme using several datasets including a dictionary of approximate size 100 000 by 300 000.
the symmetry of a simple optimization problem in lasso screening
recently dictionary screening has been proposed as an effective way to improve the computational efficiency of solving the lasso problem which is one of the most commonly used method for learning sparse representations. to address today s ever increasing large dataset effective screening relies on a tight region bound on the solution to the dual lasso. typical region bounds are in the form of an intersection of a sphere and multiple half spaces. one way to tighten the region bound is using more half spaces which however adds to the overhead of solving the high dimensional optimization problem in lasso screening. this paper reveals the interesting property that the optimization problem only depends on the projection of features onto the subspace spanned by the normals of the half spaces. this property converts an optimization problem in high dimension to much lower dimension and thus sheds light on reducing the computation overhead of lasso screening based on tighter region bounds.
hard negative mining for metric learning based zero shot classification
zero shot learning has been shown to be an efficient strategy for domain adaptation. in this context this paper builds on the recent work of bucher et al. 1 which proposed an approach to solve zero shot classification problems zsc by introducing a novel metric learning based objective function. this objective function allows to learn an optimal embedding of the attributes jointly with a measure of similarity between images and attributes. this paper extends their approach by proposing several schemes to control the generation of the negative pairs resulting in a significant improvement of the performance and giving above state of the art results on three challenging zsc datasets.
pose selective max pooling for measuring similarity
in this paper we deal with two challenges for measuring the similarity of the subject identities in practical video based face recognition the variation of the head pose in uncontrolled environments and the computational expense of processing videos. since the frame wise feature mean is unable to characterize the pose diversity among frames we define and preserve the overall pose diversity and closeness in a video. then identity will be the only source of variation across videos since the pose varies even within a single video. instead of simply using all the frames we select those faces whose pose point is closest to the centroid of the k means cluster containing that pose point. then we represent a video as a bag of frame wise deep face features while the number of features has been reduced from hundreds to k. since the video representation can well represent the identity now we measure the subject similarity between two videos as the max correlation among all possible pairs in the two bags of features. on the official 5 000 video pairs of the youtube face dataset for face verification our algorithm achieves a comparable performance with vgg face that averages over deep features of all frames. other vision tasks can also benefit from the generic idea of employing geometric cues to improve the descriptiveness of deep features.
detecting unseen falls from wearable devices using channel wise ensemble of autoencoders
a fall is an abnormal activity that occurs rarely so it is hard to collect real data for falls. it is therefore difficult to use supervised learning methods to automatically detect falls. another challenge in using machine learning methods to automatically detect falls is the choice of engineered features. in this paper we propose to use an ensemble of autoencoders to extract features from different channels of wearable sensor data trained only on normal activities. we show that the traditional approach of choosing a threshold as the maximum of the reconstruction error on the training normal data is not the right way to identify unseen falls. we propose two methods for automatic tightening of reconstruction error from only the normal activities for better identification of unseen falls. we present our results on two activity recognition datasets and show the efficacy of our proposed method against traditional autoencoder models and two standard one class classification methods.
generalization error of invariant classifiers
this paper studies the generalization error of invariant classifiers. in particular we consider the common scenario where the classification task is invariant to certain transformations of the input and that the classifier is constructed or learned to be invariant to these transformations. our approach relies on factoring the input space into a product of a base space and a set of transformations. we show that whereas the generalization error of a non invariant classifier is proportional to the complexity of the input space the generalization error of an invariant classifier is proportional to the complexity of the base space. we also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. our analysis applies to general classifiers such as convolutional neural networks. we demonstrate the implications of the developed theory for such classifiers with experiments on the mnist and cifar 10 datasets.
universal adversarial perturbations
given a state of the art deep neural network classifier we show the existence of a universal image agnostic and very small perturbation vector that causes natural images to be misclassified with high probability. we propose a systematic algorithm for computing universal perturbations and show that state of the art deep neural networks are highly vulnerable to such perturbations albeit being quasi imperceptible to the human eye. we further empirically analyze these universal perturbations and show in particular that they generalize very well across neural networks. the surprising existence of universal perturbations reveals important geometric correlations among the high dimensional decision boundary of classifiers. it further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.
linear disentangled representation learning for facial actions
limited annotated data available for the recognition of facial expression and action units embarrasses the training of deep networks which can learn disentangled invariant features. however a linear model with just several parameters normally is not demanding in terms of training data. in this paper we propose an elegant linear model to untangle confounding factors in challenging realistic multichannel signals such as 2d face videos. the simple yet powerful model does not rely on huge training data and is natural for recognizing facial actions without explicitly disentangling the identity. base on well understood intuitive linear models such as sparse representation based classification src previous attempts require a prepossessing of explicit decoupling which is practically inexact. instead we exploit the low rank property across frames to subtract the underlying neutral faces which are modeled jointly with sparse representation on the action components with group sparsity enforced. on the extended cohn kanade dataset ck our one shot automatic method on raw face videos performs as competitive as src applied on manually prepared action components and performs even better than src in terms of true positive rate. we apply the model to the even more challenging task of facial action unit recognition verified on the mpi face video database mpi vdb achieving a decent performance. all the programs and data have been made publicly available.
on detecting adversarial perturbations
machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. however it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi imperceptible to a human. in this work we propose to augment deep neural networks with a small detector subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. our method is orthogonal to prior work on addressing adversarial perturbations which has mostly focused on making the classification network itself more robust. we show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi imperceptible to humans. moreover while the detectors have been trained to detect only a specific adversary they generalize to similar and weaker adversaries. in addition we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.
activation maximization generative adversarial nets
class labels have been empirically shown useful in improving the sample quality of generative adversarial nets gans . in this paper we mathematically study the properties of the current variants of gans that make use of class label information. with class aware gradient and cross entropy decomposition we reveal how class labels and associated losses influence gan s training. based on that we propose activation maximization generative adversarial networks am gan as an advanced solution. comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution where am gan outperforms other strong baselines and achieves state of the art inception score 8.91 on cifar 10. in addition we demonstrate that with the inception imagenet classifier inception score mainly tracks the diversity of the generator and there is however no reliable evidence that it can reflect the true sample quality. we thus propose a new metric called am score to provide more accurate estimation on the sample quality. our proposed model also outperforms the baseline methods in the new metric.
interpretable explanations of black boxes by meaningful perturbation
as machine learning algorithms are increasingly applied to high impact yet high risk tasks such as medical diagnosis or autonomous driving it is critical that researchers can explain how such algorithms arrived at their predictions. in recent years a number of image saliency methods have been developed to summarize where highly complex neural networks look in an image for evidence for their predictions. however these techniques are limited by their heuristic nature and architectural constraints. in this paper we make two main contributions first we propose a general framework for learning different kinds of explanations for any black box algorithm. second we specialise the framework to find the part of an image most responsible for a classifier decision. unlike previous works our method is model agnostic and testable because it is grounded in explicit and interpretable image perturbations.
a general theory for training learning machine
though the deep learning is pushing the machine learning to a new stage basic theories of machine learning are still limited. the principle of learning the role of the a prior knowledge the role of neuron bias and the basis for choosing neural transfer function and cost function etc. are still far from clear. in this paper we present a general theoretical framework for machine learning. we classify the prior knowledge into common and problem dependent parts and consider that the aim of learning is to maximally incorporate them. the principle we suggested for maximizing the former is the design risk minimization principle while the neural transfer function the cost function as well as pretreatment of samples are endowed with the role for maximizing the latter. the role of the neuron bias is explained from a different angle. we develop a monte carlo algorithm to establish the input output responses and we control the input output sensitivity of a learning machine by controlling that of individual neurons. applications of function approaching and smoothing pattern recognition and classification are provided to illustrate how to train general learning machines based on our theory and algorithm. our method may in addition induce new applications such as the transductive inference.
a generalization of convolutional neural networks to graph structured data
this paper introduces a generalization of convolutional neural networks cnns from low dimensional grid data such as images to graph structured data. we propose a novel spatial convolution utilizing a random walk to uncover the relations within the input analogous to the way the standard convolution uses the spatial neighborhood of a pixel on the grid. the convolution has an intuitive interpretation is efficient and scalable and can also be used on data with varying graph structure. furthermore this generalization can be applied to many standard regression or classification problems by learning the the underlying graph. we empirically demonstrate the performance of the proposed cnn on mnist and challenge the state of the art on merck molecular activity data set.
formal guarantees on the robustness of a classifier against adversarial manipulation
recent work has shown that state of the art classifiers are quite brittle in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. this raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety critical systems. we show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance specific lower bounds on the norm of the input manipulation required to change the classifier decision. based on this analysis we propose the cross lipschitz regularization functional. we show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier without any loss in prediction performance.
classification regions of deep neural networks
the goal of this paper is to analyze the geometric properties of deep neural network classifiers in the input space. we specifically study the topology of classification regions created by deep networks as well as their associated decision boundary. through a systematic empirical investigation we show that state of the art deep nets learn connected classification regions and that the decision boundary in the vicinity of datapoints is flat along most directions. we further draw an essential connection between two seemingly unrelated properties of deep networks their sensitivity to additive perturbations in the inputs and the curvature of their decision boundary. the directions where the decision boundary is curved in fact remarkably characterize the directions to which the classifier is the most vulnerable. we finally leverage a fundamental asymmetry in the curvature of the decision boundary of deep nets and propose a method to discriminate between original images and images perturbed with small adversarial examples. we show the effectiveness of this purely geometric approach for detecting small adversarial perturbations in images and for recovering the labels of perturbed images.
analysis of universal adversarial perturbations
deep networks have recently been shown to be vulnerable to universal perturbations there exist very small image agnostic perturbations that cause most natural images to be misclassified by such classifiers. in this paper we propose the first quantitative analysis of the robustness of classifiers to universal perturbations and draw a formal link between the robustness to universal perturbations and the geometry of the decision boundary. specifically we establish theoretical bounds on the robustness of classifiers under two decision boundary models flat and curved models . we show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature there exists shared directions along which the decision boundary of deep networks is systematically positively curved. under such conditions we prove the existence of small universal perturbations. our analysis further provides a novel geometric method for computing universal perturbations in addition to explaining their properties.
bayesian gan
generative adversarial networks gans can implicitly learn rich distributions over images audio and data which are hard to model with an explicit likelihood. we present a practical bayesian formulation for unsupervised and semi supervised learning with gans. within this framework we use stochastic gradient hamiltonian monte carlo to marginalize the weights of the generator and discriminator networks. the resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching or mini batch discrimination. by exploring an expressive posterior over the parameters of the generator the bayesian gan avoids mode collapse produces interpretable and diverse candidate samples and provides state of the art quantitative results for semi supervised learning on benchmarks including svhn celeba and cifar 10 outperforming dcgan wasserstein gans and dcgan ensembles.
unsupervised learning of disentangled representations from video
we present a new model drnet that learns disentangled image representations from video. our approach leverages the temporal coherence of video and a novel adversarial loss to learn a representation that factorizes each frame into a stationary part and a temporally varying component. the disentangled representation can be used for a range of tasks. for example applying a standard lstm to the time vary components enables prediction of future frames. we evaluate our approach on a range of synthetic and real videos demonstrating the ability to coherently generate hundreds of steps into the future.
dualing gans
generative adversarial nets gans are a promising technique for modeling a distribution from samples. it is however well known that gan training suffers from instability due to the nature of its maximin formulation. in this paper we explore ways to tackle the instability problem by dualizing the discriminator. we start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem such that both the generator and the discriminator of this dualing gan act in concert. we then demonstrate how to extend this intuition to non linear formulations. for gans with linear discriminators our approach is able to remove the instability in training while for gans with nonlinear discriminators our approach provides an alternative to the commonly used gan training algorithm.
wavelet residual network for low dose ct via deep convolutional framelets
model based iterative reconstruction mbir algorithms for low dose x ray ct are computationally expensive. to address this problem we recently proposed the world first deep convolutional neural network cnn for low dose x ray ct and won the second place in 2016 aapm low dose ct grand challenge. however some of the texture were not fully recovered. to cope with this problem here we propose a deep residual learning approach in directional wavelet domain. the proposed method is motivated by an observation that a deep convolutional neural network can be interpreted as a multilayer convolutional framelets expansion using non local basis convolved with data driven local basis. we further extend the idea to derive a deep convolutional framelet expansion by combining global redundant transforms and signal boosting from multiple signal representations. extensive experimental results confirm that the proposed network has significantly improved performance and preserves the detail texture of the original images
3d prnn generating shape primitives with recurrent neural networks
the success of various applications including robotics digital content creation and visualization demand a structured and abstract representation of the 3d world from limited sensor data. inspired by the nature of human perception of 3d shapes as a collection of simple parts we explore such an abstract shape representation based on primitives. given a single depth image of an object we present 3d prnn a generative recurrent neural network that synthesizes multiple plausible shapes composed of a set of primitives. our generative model encodes symmetry characteristics of common man made objects preserves long range structural coherence and describes objects of varying complexity with a compact representation. we also propose a method based on gaussian fields to generate a large scale dataset of primitive based shape representations to train our network. we evaluate our approach on a wide range of examples and show that it outperforms nearest neighbor based shape retrieval methods and is on par with voxel based generative models while using a significantly reduced parameter space.
inception score label smoothing gradient vanishing and log d x alternative
in this article we mathematically study several gan related topics including inception score label smoothing gradient vanishing and the log d x alternative. an advanced version is included in arxiv 1703.02000 activation maximization generative adversarial nets . please refer section 6 in 1703.02000 for detailed analysis on inception score and refer its appendix for the discussions on label smoothing gradient vanishing and log d x alternative. 
a brief survey of deep reinforcement learning
deep reinforcement learning is poised to revolutionise the field of ai and represents a step towards building autonomous systems with a higher level understanding of the visual world. currently deep learning is enabling reinforcement learning to scale to problems that were previously intractable such as learning to play video games directly from pixels. deep reinforcement learning algorithms are also applied to robotics allowing control policies for robots to be learned directly from camera inputs in the real world. in this survey we begin with an introduction to the general field of reinforcement learning then progress to the main streams of value based and policy based methods. our survey will cover central algorithms in deep reinforcement learning including the deep q network trust region policy optimisation and asynchronous advantage actor critic. in parallel we highlight the unique advantages of deep neural networks focusing on visual understanding via reinforcement learning. to conclude we describe several current areas of research within the field.
circnn accelerating and compressing deep neural networks using block circulantweight matrices
large scale deep neural networks dnns are both compute and memory intensive. as the size of dnns continues to grow it is critical to improve the energy efficiency and performance while maintaining accuracy. for dnns the model size is an important factor affecting performance scalability and energy efficiency. weight pruning achieves good compression ratios but suffers from three drawbacks 1 the irregular network structure after pruning 2 the increased training complexity and 3 the lack of rigorous guarantee of compression ratio and inference accuracy. to overcome these limitations this paper proposes circnn a principled approach to represent weights and process neural networks using block circulant matrices. circnn utilizes the fast fourier transform fft based fast multiplication simultaneously reducing the computational complexity both in inference and training from o n2 to o nlogn and the storage complexity from o n2 to o n with negligible accuracy loss. compared to other approaches circnn is distinct due to its mathematical rigor it can converge to the same effectiveness as dnns without compression. the circnn architecture a universal dnn inference engine that can be implemented on various hardware software platforms with configurable network architecture. to demonstrate the performance and energy efficiency we test circnn in fpga asic and embedded processors. our results show that circnn architecture achieves very high energy efficiency and performance with a small hardware footprint. based on the fpga implementation and asic synthesis results circnn achieves 6 102x energy efficiency improvements compared with the best state of the art results.
xflow 1d 2d cross modal deep neural networks for audiovisual classification
we propose two multimodal deep learning architectures that allow for cross modal dataflow xflow between the feature extractors thereby extracting more interpretable features and obtaining a better representation than through unimodal learning for the same amount of training data. these models can usefully exploit correlations between audio and visual data which have a different dimensionality and are therefore nontrivially exchangeable. our work improves on existing multimodal deep learning metholodogies in two essential ways 1 it presents a novel method for performing cross modality before features are learned from individual modalities and 2 extends the previously proposed cross connections which only transfer information between streams that process compatible data. both cross modal architectures outperformed their baselines by up to 7.5 when evaluated on the avletters dataset.
context embedding networks
low dimensional embeddings that capture the main variations of interest in collections of data are important for many applications. one way to construct these embeddings is to acquire estimates of similarity from the crowd. however similarity is a multi dimensional concept that varies from individual to individual. existing models for learning embeddings from the crowd typically make simplifying assumptions such as all individuals estimate similarity using the same criteria the list of criteria is known in advance or that the crowd workers are not influenced by the data that they see. to overcome these limitations we introduce context embedding networks cens . in addition to learning interpretable embeddings from images cens also model worker biases for different attributes along with the visual context i.e. the visual attributes highlighted by a set of images. experiments on two noisy crowd annotated datasets show that modeling both worker bias and visual context results in more interpretable embeddings compared to existing approaches.
how much chemistry does a deep neural network need to know to make accurate predictions 
the meteoric rise of deep learning models in computer vision research having achieved human level accuracy in image recognition tasks is firm evidence of the impact of representation learning of deep neural networks. in the chemistry domain recent advances have also led to the development of similar cnn models such as chemception that is trained to predict chemical properties using images of molecular drawings. in this work we investigate the effects of systematically removing and adding localized domain specific information to the image channels of the training data. by augmenting images with only 3 additional basic information and without introducing any architectural changes we demonstrate that an augmented chemception augchemception outperforms the original model in the prediction of toxicity activity and solvation free energy. then by altering the information content in the images and examining the resulting model s performance we also identify two distinct learning patterns in predicting toxicity activity as compared to solvation free energy. these patterns suggest that chemception is learning about its tasks in the manner that is consistent with established knowledge. thus our work demonstrates that advanced chemical knowledge is not a pre requisite for deep learning models to accurately predict complex chemical properties.
variational inference of disentangled latent concepts from unlabeled observations
disentangled representations where the higher level data generative factors are reflected in disjoint latent dimensions offer several benefits such as ease of deriving invariant representations transferability to other tasks interpretability etc. we consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations and propose a variational inference based approach to infer disentangled latent factors. we introduce a regularizer on the expectation of the approximate posterior over observed data that encourages the disentanglement. we evaluate the proposed approach using several quantitative metrics and empirically observe significant gains over existing methods in terms of both disentanglement and data likelihood reconstruction quality .
three factors influencing minima in sgd
we study the properties of the endpoint of stochastic gradient descent sgd . by approximating sgd as a stochastic differential equation sde we consider the boltzmann gibbs equilibrium distribution of that sde under the assumption of isotropic variance in loss gradients. through this analysis we find that three factors learning rate batch size and the variance of the loss gradients control the trade off between the depth and width of the minima found by sgd with wider minima favoured by a higher ratio of learning rate to batch size. we have direct control over the learning rate and batch size while the variance is determined by the choice of model architecture model parameterization and dataset. in the equilibrium distribution only the ratio of learning rate to batch size appears implying that the equilibrium distribution is invariant under a simultaneous rescaling of learning rate and batch size by the same amount. we then explore experimentally how learning rate and batch size affect sgd from two perspectives the endpoint of sgd and the dynamics that lead up to it. for the endpoint the experiments suggest the endpoint of sgd is invariant under simultaneous rescaling of batch size and learning rate and also that a higher ratio leads to flatter minima both findings are consistent with our theoretical analysis. we note experimentally that the dynamics also seem to be invariant under the same rescaling of learning rate and batch size which we explore showing that one can exchange batch size and learning rate for cyclical learning rate schedule. next we illustrate how noise affects memorization showing that high noise levels lead to better generalization. finally we find experimentally that the invariance under simultaneous rescaling of learning rate and batch size breaks down if the learning rate gets too large or the batch size gets too small.
learning to play othello with deep neural networks
achieving superhuman playing level by alphago corroborated the capabilities of convolutional neural architectures cnns for capturing complex spatial patterns. this result was to a great extent due to several analogies between go board states and 2d images cnns have been designed for in particular translational invariance and a relatively large board. in this paper we verify whether cnn based move predictors prove effective for othello a game with significantly different characteristics including a much smaller board size and complete lack of translational invariance. we compare several cnn architectures and board encodings augment them with state of the art extensions train on an extensive database of experts moves and examine them with respect to move prediction accuracy and playing strength. the empirical evaluation confirms high capabilities of neural move predictors and suggests a strong correlation between prediction accuracy and playing strength. the best cnns not only surpass all other 1 ply othello players proposed to date but defeat 2 ply edax the best open source othello player.
deep learning can reverse photon migration for diffuse optical tomography
can artificial intelligence ai learn complicated non linear physics here we propose a novel deep learning approach that learns non linear photon scattering physics and obtains accurate 3d distribution of optical anomalies. in contrast to the traditional black box deep learning approaches to inverse problems our deep network learns to invert the lippmann schwinger integral equation which describes the essential physics of photon migration of diffuse near infrared nir photons in turbid media. as an example for clinical relevance we applied the method to our prototype diffuse optical tomography dot . we show that our deep neural network trained with only simulation data can accurately recover the location of anomalies within biomimetic phantoms and live animals without the use of an exogenous contrast agent.
using rule based labels for weak supervised learning a chemnet for transferable chemical property prediction
with access to large datasets deep neural networks dnn have achieved human level accuracy in image and speech recognition tasks. however in chemistry data is inherently small and fragmented. in this work we develop an approach of using rule based knowledge for training chemnet a transferable and generalizable deep neural network for chemical property prediction that learns in a weak supervised manner from large unlabeled chemical databases. when coupled with transfer learning approaches to predict other smaller datasets for chemical properties that it was not originally trained on we show that chemnet s accuracy outperforms contemporary dnn models that were trained using conventional supervised learning. furthermore we demonstrate that the chemnet pre training approach is equally effective on both cnn chemception and rnn smiles2vec models indicating that this approach is network architecture agnostic and is effective across multiple data modalities. our results indicate a pre trained chemnet that incorporates chemistry domain knowledge enables the development of generalizable neural networks for more accurate prediction of novel chemical properties.
deep learning in rf sub sampled b mode ultrasound imaging
in portable three dimensional and ultra fast ultrasound us imaging systems there is an increasing need to reconstruct high quality images from a limited number of rf data from receiver rx or scan line sc sub sampling. however due to the severe side lobe artifacts from rf sub sampling the standard beam former often produces blurry images with less contrast that are not suitable for diagnostic purpose. to address this problem some researchers have studied compressed sensing cs to exploit the sparsity of the image or rf data in some domains. however the existing cs approaches require either hardware changes or computationally expensive algorithms. to overcome these limitations here we propose a novel deep learning approach that directly interpolates the missing rf data by utilizing redundancy in the rx sc plane. in particular the network design principle derives from a novel interpretation of the deep neural network as a cascaded convolution framelets that learns the data driven bases for hankel matrix decomposition. our extensive experimental results from sub sampled rf data from a real us system confirmed that the proposed method can effectively reduce the data rate without sacrificing the image quality.
deep learning interior tomography for region of interest reconstruction
interior tomography for the region of interest roi imaging has advantages of using a small detector and reducing x ray radiation dose. however standard analytic reconstruction suffers from severe cupping artifacts due to existence of null space in the truncated radon transform. existing penalized reconstruction methods may address this problem but they require extensive computations due to the iterative reconstruction. inspired by the recent deep learning approaches to low dose and sparse view ct here we propose a deep learning architecture that removes null space signals from the fbp reconstruction. experimental results have shown that the proposed method provides near perfect reconstruction with about 7 10 db improvement in psnr over existing methods in spite of significantly reduced run time complexity.
deep learning reconstruction for 9 view dual energy ct baggage scanner
for homeland and transportation security applications 2d x ray explosive detection system eds have been widely used but they have limitations in recognizing 3d shape of the hidden objects. among various types of 3d computed tomography ct systems to address this issue this paper is interested in a stationary ct using fixed x ray sources and detectors. however due to the limited number of projection views analytic reconstruction algorithms produce severe streaking artifacts. inspired by recent success of deep learning approach for sparse view ct reconstruction here we propose a novel image and sinogram domain deep learning architecture for 3d reconstruction from very sparse view measurement. the algorithm has been tested with the real data from a prototype 9 view dual energy stationary ct eds carry on baggage scanner developed by gemss medical systems korea which confirms the superior reconstruction performance over the existing approaches.
effective building block design for deep convolutional neural networks using search
deep learning has shown promising results on many machine learning tasks but dl models are often complex networks with large number of neurons and layers and recently complex layer structures known as building blocks. finding the best deep model requires a combination of finding both the right architecture and the correct set of parameters appropriate for that architecture. in addition this complexity in terms of layer types number of neurons and number of layers also present problems with generalization since larger networks are easier to overfit to the data. in this paper we propose a search framework for finding effective architectural building blocks for convolutional neural networks cnn . our approach is much faster at finding models that are close to state of the art in performance. in addition the models discovered by our approach are also smaller than models discovered by similar techniques. we achieve these twin advantages by designing our search space in such a way that it searches over a reduced set of state of the art building blocks for cnns including residual block inception block inception residual block resnext block and many others. we apply this technique to generate models for multiple image datasets and show that these models achieve performance comparable to state of the art and even surpassing the state of the art in one case . we also show that learned models are transferable between datasets.
tvae triplet based variational autoencoder using metric learning
deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity by relying on the embedding learned from metric learning. at the same time variational autoencoder vae has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. however for traditional vae the data label or feature information are intractable. similarly traditional representation learning approaches fail to represent many salient aspects of the data. in this project we propose a novel integrated framework to learn latent embedding in vae by incorporating deep metric learning. the features are learned by optimizing a triplet loss on the mean vectors of vae in conjunction with standard evidence lower bound elbo of vae. this approach which we call triplet based variational autoencoder tvae allows us to capture more fine grained information in the latent embedding. our model is tested on mnist data set and achieves a high triplet accuracy of 95.60 while the traditional vae kingma welling 2013 achieves triplet accuracy of 75.08 .
learning to play with intrinsically motivated self aware agents
infants are experts at playing with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. we seek to mathematically formalize these abilities using a neural network that implements curiosity driven intrinsic motivation. using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees we propose a world model network that learns to predict the dynamic consequences of the agent s actions. simultaneously we train a separate explicit self model that allows the agent to track the error map of its own world model and then uses the self model to adversarially challenge the developing world model. we demonstrate that this policy causes the agent to explore novel and informative interactions with its environment leading to the generation of a spectrum of complex behaviors including ego motion prediction object attention and object gathering. moreover the world model that the agent learns supports improved performance on object dynamics prediction detection localization and recognition tasks. taken together our results are initial steps toward creating flexible autonomous agents that self supervise in complex novel physical environments.
emergence of structured behaviors from curiosity based intrinsic motivation
infants are experts at playing with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. we seek to replicate some of these abilities with a neural network that implements curiosity driven intrinsic motivation. using a simple but ecologically naturalistic simulated environment in which the agent can move and interact with objects it sees the agent learns a world model predicting the dynamic consequences of its actions. simultaneously the agent learns to take actions that adversarially challenge the developing world model pushing the agent to explore novel and informative interactions with its environment. we demonstrate that this policy leads to the self supervised emergence of a spectrum of complex behaviors including ego motion prediction object attention and object gathering. moreover the world model that the agent learns supports improved performance on object dynamics prediction and localization tasks. our results are a proof of principle that computational models of intrinsic motivation might account for key features of developmental visuomotor learning in infants.
stochastic video generation with a learned prior
generating video frames that accurately predict future world states is challenging. existing approaches either fail to capture the full distribution of outcomes or yield blurry generations or both. in this paper we introduce an unsupervised video generation model that learns a prior model of uncertainty in a given environment. video frames are generated by drawing samples from this prior and combining them with a deterministic estimate of the future frame. the approach is simple and easily trained end to end on a variety of datasets. sample generations are both varied and sharp even many frames into the future and compare favorably to those from existing approaches.
multi evidence filtering and fusion for multi label classification object detection and semantic segmentation based on weakly supervised learning
supervised object detection and semantic segmentation require object or even pixel level annotations. when there exist image level labels only it is challenging for weakly supervised algorithms to achieve accurate predictions. the accuracy achieved by top weakly supervised algorithms is still significantly lower than their fully supervised counterparts. in this paper we propose a novel weakly supervised curriculum learning pipeline for multi label object recognition detection and semantic segmentation. in this pipeline we first obtain intermediate object localization and pixel labeling results for the training images and then use such results to train task specific deep networks in a fully supervised manner. the entire process consists of four stages including object localization in the training images filtering and fusing object instances pixel labeling for the training images and task specific network training. to obtain clean object instances in the training images we propose a novel algorithm for filtering fusing and classifying object instances collected from multiple solution mechanisms. in this algorithm we incorporate both metric learning and density based clustering to filter detected object instances. experiments show that our weakly supervised pipeline achieves state of the art results in multi label image classification as well as weakly supervised object detection and very competitive results in weakly supervised semantic segmentation on ms coco pascal voc 2007 and pascal voc 2012.
neural networks should be wide enough to learn disconnected decision regions
in the recent literature the important role of depth in deep learning has been emphasized. in this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. it turns out that for a class of activation functions including leaky relu neural networks having a pyramidal structure that is no layer has more hidden units than the input dimension produce necessarily connected decision regions. this implies that a sufficiently wide layer is necessary to produce disconnected decision regions. we discuss the implications of this result for the construction of neural networks in particular the relation to the problem of adversarial manipulation of classifiers.
visual explanations from deep 3d convolutional neural networks for alzheimer s disease classification
we develop three efficient approaches for generating visual explanations from 3d convolutional neural networks 3d cnns for alzheimer s disease classification. one approach conducts sensitivity analysis on hierarchical 3d image segmentation and the other two visualize network activations on a spatial map. visual checks and a quantitative localization benchmark indicate that all approaches identify important brain parts for alzheimer s disease diagnosis. comparative analysis show that the sensitivity analysis based approach has difficulty handling loosely distributed cerebral cortex and approaches based on visualization of activations are constrained by the resolution of the convolutional layer. the complementarity of these methods improves the understanding of 3d cnns in alzheimer s disease classification from different perspectives.
averaging weights leads to wider optima and better generalization
deep neural networks are typically trained by optimizing a loss function with an sgd variant in conjunction with a decaying learning rate until convergence. we show that simple averaging of multiple points along the trajectory of sgd with a cyclical or constant learning rate leads to better generalization than conventional training. we also show that this stochastic weight averaging swa procedure finds much broader optima than sgd and approximates the recent fast geometric ensembling fge approach with a single model. using swa we achieve notable improvement in test accuracy over conventional sgd training on a range of state of the art residual networks pyramidnets densenets and shake shake networks on cifar 10 cifar 100 and imagenet. in short swa is extremely easy to implement improves generalization and has almost no computational overhead.
senns sparse extraction neural networks for feature extraction
by drawing on ideas from optimisation theory artificial neural networks ann graph embeddings and sparse representations i develop a novel technique termed senns sparse extraction neural networks aimed at addressing the feature extraction problem. the proposed method uses preferably deep anns for projecting input attribute vectors to an output space wherein pairwise distances are maximized for vectors belonging to different classes but minimized for those belonging to the same class while simultaneously enforcing sparsity on the ann outputs. the vectors that result from the projection can then be used as features in any classifier of choice. mathematically i formulate the proposed method as the minimisation of an objective function which can be interpreted in the ann output space as a negative factor of the sum of the squares of the pair wise distances between output vectors belonging to different classes added to a positive factor of the sum of squares of the pair wise distances between output vectors belonging to the same classes plus sparsity and weight decay terms. to derive an algorithm for minimizing the objective function via gradient descent i use the multi variate version of the chain rule to obtain the partial derivatives of the function with respect to ann weights and biases and find that each of the required partial derivatives can be expressed as a sum of six terms. as it turns out four of those six terms can be computed using the standard back propagation algorithm the fifth can be computed via a slight modification of the standard backpropagation algorithm while the sixth one can be computed via simple arithmetic. finally i propose experiments on the arabase arabic corpora of digits and letters the cmu pie database of faces the mnist digits database and other standard machine learning databases.
generative models and model criticism via optimized maximum mean discrepancy
we propose a method to optimize the representation and distinguishability of samples from two probability distributions by maximizing the estimated power of a statistical test based on the maximum mean discrepancy mmd . this optimized mmd is applied to the setting of unsupervised learning by generative adversarial networks gan in which a model attempts to generate realistic samples and a discriminator attempts to tell these apart from data samples. in this context the mmd may be used in two roles first as a discriminator either directly on the samples or on features of the samples. second the mmd can be used to evaluate the performance of a generative model by testing the model s samples against a reference data set. in the latter role the optimized mmd is particularly helpful as it gives an interpretable indication of how the model and data distributions differ even in cases where individual model samples are not easily distinguished either by eye or by classifier.
deep learning approximation for stochastic control problems
many real world stochastic control problems suffer from the curse of dimensionality . to overcome this difficulty we develop a deep learning approach that directly solves high dimensional stochastic control problems based on monte carlo sampling. we approximate the time dependent controls as feedforward neural networks and stack these networks together through model dynamics. the objective function for the control problem plays the role of the loss function for the deep neural network. we test this approach using examples from the areas of optimal trading and energy storage. our results suggest that the algorithm presented here achieves satisfactory accuracy and at the same time can handle rather high dimensional problems.
generating focussed molecule libraries for drug discovery with recurrent neural networks
in de novo drug design computational strategies are used to generate novel molecules with good affinity to the desired biological target. in this work we show that recurrent neural networks can be trained as generative models for molecular structures similar to statistical language models in natural language processing. we demonstrate that the properties of the generated molecules correlate very well with the properties of the molecules used to train the model. in order to enrich libraries with molecules active towards a given biological target we propose to fine tune the model with small sets of molecules which are known to be active against that target. against staphylococcus aureus the model reproduced 14 of 6051 hold out test molecules that medicinal chemists designed whereas against plasmodium falciparum malaria it reproduced 28 of 1240 test molecules. when coupled with a scoring function our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.
parameter space noise for exploration
deep reinforcement learning rl methods generally engage in exploratory behavior through noise injection in the action space. an alternative is to add noise directly to the agent s parameters which can lead to more consistent exploration and a richer set of behaviors. methods such as evolutionary strategies use parameter perturbations but discard all temporal structure in the process and require significantly more samples. combining parameter noise with traditional rl methods allows to combine the best of both worlds. we demonstrate that both off and on policy methods benefit from this approach through experimental comparison of dqn ddpg and trpo on high dimensional discrete action environments as well as continuous control tasks. our results show that rl with parameter noise learns more efficiently than traditional rl with action space noise and evolutionary strategies individually.
on the robustness of a neural network
with the development of neural networks based machine learning and their usage in mission critical applications voices are rising against the textit black box aspect of neural networks as it becomes crucial to understand their limits and capabilities. with the rise of neuromorphic hardware it is even more critical to understand how a neural network as a distributed system tolerates the failures of its computing nodes neurons and its communication channels synapses. experimentally assessing the robustness of neural networks involves the quixotic venture of testing all the possible failures on all the possible inputs which ultimately hits a combinatorial explosion for the first and the impossibility to gather all the possible inputs for the second. in this paper we prove an upper bound on the expected error of the output when a subset of neurons crashes. this bound involves dependencies on the network parameters that can be seen as being too pessimistic in the average case. it involves a polynomial dependency on the lipschitz coefficient of the neurons activation function and an exponential dependency on the depth of the layer where a failure occurs. we back up our theoretical results with experiments illustrating the extent to which our prediction matches the dependencies between the network parameters and robustness. our results show that the robustness of neural networks to the average crash can be estimated without the need to neither test the network on all failure configurations nor access the training set used to train the network both of which are practically impossible requirements.
zhusuan a library for bayesian deep learning
in this paper we introduce zhusuan a python probabilistic programming library for bayesian deep learning which conjoins the complimentary advantages of bayesian methods and deep learning. zhusuan is built upon tensorflow. unlike existing deep learning libraries which are mainly designed for deterministic neural networks and supervised tasks zhusuan is featured for its deep root into bayesian inference thus supporting various kinds of probabilistic models including both the traditional hierarchical bayesian models and recent deep generative models. we use running examples to illustrate the probabilistic programming on zhusuan including bayesian logistic regression variational auto encoders deep sigmoid belief networks and bayesian recurrent neural networks.
using parameterized black box priors to scale up model based policy search for robotics
the most data efficient algorithms for reinforcement learning in robotics are model based policy search algorithms which alternate between learning a dynamical model of the robot and optimizing a policy to maximize the expected return given the model and its uncertainties. among the few proposed approaches the recently introduced black drops algorithm exploits a black box optimization algorithm to achieve both high data efficiency and good computation times when several cores are used nevertheless like all model based policy search approaches black drops does not scale to high dimensional state action spaces. in this paper we introduce a new model learning procedure in black drops that leverages parameterized black box priors to 1 scale up to high dimensional systems and 2 be robust to large inaccuracies of the prior information. we demonstrate the effectiveness of our approach with the pendubot swing up task in simulation and with a physical hexapod robot 48d state space 18d action space that has to walk forward as fast as possible. the results show that our new algorithm is more data efficient than previous model based policy search algorithms with and without priors and that it can allow a physical 6 legged robot to learn new gaits in only 16 to 30 seconds of interaction time.
bayesian optimization with automatic prior selection for data efficient direct policy search
one of the most interesting features of bayesian optimization for direct policy search is that it can leverage priors e.g. from simulation or from previous tasks to accelerate learning on a robot. in this paper we are interested in situations for which several priors exist but we do not know in advance which one fits best the current situation. we tackle this problem by introducing a novel acquisition function called most likely expected improvement mlei that combines the likelihood of the priors and the expected improvement. we evaluate this new acquisition function on a transfer learning task for a 5 dof planar arm and on a possibly damaged 6 legged robot that has to learn to walk on flat ground and on stairs with priors corresponding to different stairs and different kinds of damages. our results show that mlei effectively identifies and exploits the priors even when there is no obvious match between the current situations and the priors.
bounding and counting linear regions of deep neural networks
in this paper we study the representational power of deep neural networks dnn that belong to the family of piecewise linear pwl functions based on pwl activation units such as rectifier or maxout. we investigate the complexity of such networks by studying the number of linear regions of the pwl function. typically a pwl function from a dnn can be seen as a large family of linear functions acting on millions of such regions. we directly build upon the work of montufar et al. 2014 montufar 2017 and raghu et al. 2017 by refining the upper and lower bounds on the number of linear regions for rectified and maxout networks. in addition to achieving tighter bounds we also develop a novel method to perform exact enumeration or counting of the number of linear regions with a mixed integer linear formulation that maps the input space to output. we use this new capability to visualize how the number of linear regions change while training dnns.
deep rewiring training very sparse deep networks
neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. but also generic hardware and software implementations of deep learning run more efficiently for sparse networks. several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. we present an algorithm deep r that enables us to train directly a sparsely connected neural network. deep r automatically rewires the network during supervised training so that connections are there where they are most needed for the task while its total number is all the time strictly bounded. we demonstrate that deep r can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. deep r is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior.
comparing heterogeneous entities using artificial neural networks of trainable weighted structural components and machine learned activation functions
to compare entities of differing types and structural components the artificial neural network paradigm was used to cross compare structural components between heterogeneous documents. trainable weighted structural components were input into machine learned activation functions of the neurons. the model was used for matching news articles and videos where the inputs and activation functions respectively consisted of term vectors and cosine similarity measures between the weighted structural components. the model was tested with different weights achieving as high as 59.2 accuracy for matching videos to news articles. a mobile application user interface for recommending related videos for news articles was developed to demonstrate consumer value including its potential usefulness for cross selling products from unrelated categories.
active learning of inverse models with intrinsically motivated goal exploration in robots
we introduce the self adaptive goal generation robust intelligent adaptive curiosity sagg riac architecture as an intrinsi cally motivated goal exploration mechanism which allows active learning of inverse models in high dimensional redundant robots. this allows a robot to efficiently and actively learn distributions of parameterized motor skills policies that solve a corresponding distribution of parameterized tasks goals. the architecture makes the robot sample actively novel parameterized tasks in the task space based on a measure of competence progress each of which triggers low level goal directed learning of the motor policy pa rameters that allow to solve it. for both learning and generalization the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task and based on the previously learnt correspondences between policy and task parameters. we present experiments with high dimensional continuous sensorimotor spaces in three different robotic setups 1 learning the inverse kinematics in a highly redundant robotic arm 2 learning omnidirectional locomotion with motor primitives in a quadruped robot 3 an arm learning to control a fishing rod with a flexible wire. we show that 1 exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots 2 selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly as well as more efficient than different standard active motor babbling methods 3 this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot.
end to end tracking and semantic segmentation using recurrent neural networks
in this work we present a novel end to end framework for tracking and classifying a robot s surroundings in complex dynamic and only partially observable real world environments. the approach deploys a recurrent neural network to filter an input stream of raw laser measurements in order to directly infer object locations along with their identity in both visible and occluded areas. to achieve this we first train the network using unsupervised deep tracking a recently proposed theoretical framework for end to end space occupancy prediction. we show that by learning to track on a large amount of unsupervised data the network creates a rich internal representation of its environment which we in turn exploit through the principle of inductive transfer of knowledge to perform the task of it s semantic classification. as a result we show that only a small amount of labelled data suffices to steer the network towards mastering this additional task. furthermore we propose a novel recurrent neural network architecture specifically tailored to tracking and semantic classification in real world robotics applications. we demonstrate the tracking and classification performance of the method on real world data collected at a busy road junction. our evaluation shows that the proposed end to end framework compares favourably to a state of the art model free tracking solution and that it outperforms a conventional one shot training scheme for semantic classification.
deep tracking seeing beyond seeing using recurrent neural networks
this paper presents to the best of our knowledge the first end to end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or system identification in the form of plant or sensor models. specifically our system accepts a stream of raw sensor data at one end and in real time produces an estimate of the entire environment state at the output including even occluded objects. we achieve this by framing the problem as a deep learning task and exploit sequence models in the form of recurrent neural networks to learn a mapping from sensor measurements to object tracks. in particular we propose a learning method based on a form of input dropout which allows learning in an unsupervised manner only based on raw occluded sensor data without access to ground truth annotations. we demonstrate our approach using a synthetic dataset designed to mimic the task of tracking objects in 2d laser data as commonly encountered in robotics applications and show that it learns to track many dynamic objects despite occlusions and the presence of sensor noise.
deep predictive coding networks for video prediction and unsupervised learning
while great strides have been made in using deep learning algorithms to solve supervised learning tasks the problem of unsupervised learning leveraging unlabeled examples to learn about the structure of a domain remains a difficult unsolved challenge. here we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. we describe a predictive neural network prednet architecture that is inspired by the concept of predictive coding from the neuroscience literature. these networks learn to predict future frames in a video sequence with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. we show that these networks are able to robustly learn to predict the movement of synthetic rendered objects and that in doing so the networks learn internal representations that are useful for decoding latent object parameters e.g. pose that support object recognition with fewer training views. we also show that these networks can scale to complex natural image streams car mounted camera videos capturing key aspects of both egocentric movement and the movement of objects in the visual scene and the representation learned in this setting is useful for estimating the steering angle. altogether these results suggest that prediction represents a powerful framework for unsupervised learning allowing for implicit learning of object and scene structure.
vote3deep fast object detection in 3d point clouds using efficient convolutional neural networks
this paper proposes a computationally efficient approach to detecting objects natively in 3d point clouds using convolutional neural networks cnns . in particular this is achieved by leveraging a feature centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. to this end we examine the trade off between accuracy and speed for different architectures and additionally propose to use an l1 penalty on the filter activations to further encourage sparsity in the intermediate representations. to the best of our knowledge this is the first work to propose sparse convolutional layers and l1 regularisation for efficient large scale processing of 3d data. we demonstrate the efficacy of our approach on the kitti object detection benchmark and show that vote3deep models with as few as three layers outperform the previous state of the art in both laser and laser vision based approaches by margins of up to 40 while remaining highly competitive in terms of processing time.
on convergence and stability of gans
we propose studying gan training dynamics as regret minimization which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. we analyze the convergence of gan training from this new point of view to understand why mode collapse happens. we hypothesize the existence of undesirable local equilibria in this non convex game to be responsible for mode collapse. we observe that these local equilibria often exhibit sharp gradients of the discriminator function around some real data points. we demonstrate that these degenerate local equilibria can be avoided with a gradient penalty scheme called dragan. we show that dragan enables faster training achieves improved stability with fewer mode collapses and leads to generator networks with better modeling performance across a variety of architectures and objective functions.
imitation from observation learning to imitate behaviors from raw video via context translation
imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable using supervision provided as demonstrations from an expert typically a human operator. however standard imitation learning methods assume that the agent receives examples of observation action tuples that could be provided for instance to a supervised learning algorithm. this stands in contrast to how humans and animals imitate we observe another person performing some behavior and then figure out which actions will realize that behavior compensating for changes in viewpoint surroundings and embodiment. we term this kind of imitation learning as imitation from observation and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. this lifts the assumption in imitation learning that the demonstration should consist of observations and actions in the same environment and enables a variety of interesting applications including learning robotic skills that involve tool use simply by observing videos of human tool use. our experimental results show that our approach can perform imitation from observation for a variety of real world robotic tasks modeled on common household chores acquiring skills such as sweeping from videos of a human demonstrator. videos can be found at https sites.google.com site imitationfromobservation
convergence rates for pretraining and dropout guiding learning parameters using network structure
unsupervised pretraining and dropout have been well studied especially with respect to regularization and output consistency. however our understanding about the explicit convergence rates of the parameter estimates and their dependence on the learning like denoising and dropout rate and structural like depth and layer lengths aspects of the network is less mature. an interesting question in this context is to ask if the network structure could guide the choices of such learning parameters. in this work we explore these gaps between network structure the learning mechanisms and their interaction with parameter convergence rates. we present a way to address these issues based on the backpropagation convergence rates for general nonconvex objectives using first order information. we then incorporate two learning mechanisms into this general framework denoising autoencoder and dropout and subsequently derive the convergence rates of deep networks. building upon these bounds we provide insights into the choices of learning parameters and network sizes that achieve certain levels of convergence accuracy. the results derived here support existing empirical observations and we also conduct a set of experiments to evaluate them.
learning discriminative features via label consistent neural network
deep convolutional neural networks cnn enforces supervised information only at the output layer and hidden layers are trained by back propagating the prediction error from the output layer without explicit supervision. we propose a supervised feature learning approach label consistent neural network which enforces direct supervision in late hidden layers. we associate each neuron in a hidden layer with a particular class label and encourage it to be activated for input signals from the same class. more specifically we introduce a label consistency regularization called discriminative representation error loss for late hidden layers and combine it with classification error loss to build our overall objective function. this label consistency constraint alleviates the common problem of gradient vanishing and tends to faster convergence it also makes the features derived from late hidden layers discriminative enough for classification even using a simple k nn classifier since input signals from the same class will have very similar representations. experimental results demonstrate that our approach achieves state of the art performances on several public benchmarks for action and object category recognition.
out of sample extension for dimensionality reduction of noisy time series
this paper proposes an out of sample extension framework for a global manifold learning algorithm isomap that uses temporal information in out of sample points in order to make the embedding more robust to noise and artifacts. given a set of noise free training data and its embedding the proposed framework extends the embedding for a noisy time series. this is achieved by adding a spatio temporal compactness term to the optimization objective of the embedding. to the best of our knowledge this is the first method for out of sample extension of manifold embeddings that leverages timing information available for the extension set. experimental results demonstrate that our out of sample extension algorithm renders a more robust and accurate embedding of sequentially ordered image data in the presence of various noise and artifacts when compared to other timing aware embeddings. additionally we show that an out of sample extension framework based on the proposed algorithm outperforms the state of the art in eye gaze estimation.
adversarial examples for semantic image segmentation
machine learning methods in general and deep neural networks in particular have shown to be vulnerable to adversarial perturbations. so far this phenomenon has mainly been studied in the context of whole image classification. in this contribution we analyse how adversarial perturbations can affect the task of semantic segmentation. we show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations that lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.
decision based adversarial attacks reliable attacks against black box machine learning models
many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. so far it was unclear how much risk adversarial perturbations carry for the safety of real world machine learning applications because most methods used to generate such perturbations rely either on detailed model information gradient based attacks or on confidence scores such as class probabilities score based attacks neither of which are available in most real world scenarios. in many such cases one currently needs to retreat to transfer based attacks which rely on cumbersome substitute models need access to the training data and can be defended against. here we emphasise the importance of attacks which solely rely on the final model decision. such decision based attacks are 1 applicable to real world black box models such as autonomous cars 2 need less knowledge and are easier to apply than transfer based attacks and 3 are more robust to simple defences than gradient or score based attacks. previous attacks in this category were limited to simple models or simple datasets. here we introduce the boundary attack a decision based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. the attack is conceptually simple requires close to no hyperparameter tuning does not rely on substitute models and is competitive with the best gradient based attacks in standard computer vision tasks like imagenet. we apply the attack on two black box algorithms from clarifai.com. the boundary attack in particular and the class of decision based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. an implementation of the attack is available as part of foolbox at https github.com bethgelab foolbox .
towards building an intelligent anti malware system a deep learning approach using support vector machine svm for malware classification
effective and efficient mitigation of malware is a long time endeavor in the information security community. the development of an anti malware system that can counteract an unknown malware is a prolific activity that may benefit several sectors. we envision an intelligent anti malware system that utilizes the power of deep learning dl models. using such models would enable the detection of newly released malware through mathematical generalization. that is finding the relationship between a given malware x and its corresponding malware family y f x mapsto y . to accomplish this feat we used the malimg dataset nataraj et al. 2011 which consists of malware images that were processed from malware binaries and then we trained the following dl models 1 to classify each malware family cnn svm tang 2013 gru svm agarap 2017 and mlp svm. empirical evidence has shown that the gru svm stands out among the dl models with a predictive accuracy of 84.92 . this stands to reason for the mentioned model had the relatively most sophisticated architecture design among the presented models. the exploration of an even more optimal dl svm model is the next stage towards the engineering of an intelligent anti malware system.
feature extraction using latent dirichlet allocation and neural networks a case study on movie synopses
feature extraction has gained increasing attention in the field of machine learning as in order to detect patterns extract information or predict future observations from big data the urge of informative features is crucial. the process of extracting features is highly linked to dimensionality reduction as it implies the transformation of the data from a sparse high dimensional space to higher level meaningful abstractions. this dissertation employs neural networks for distributed paragraph representations and latent dirichlet allocation to capture higher level features of paragraph vectors. although neural networks for distributed paragraph representations are considered the state of the art for extracting paragraph vectors we show that a quick topic analysis model such as latent dirichlet allocation can provide meaningful features too. we evaluate the two methods on the cmu movie summary corpus a collection of 25 203 movie plot summaries extracted from wikipedia. finally for both approaches we use k nearest neighbors to discover similar movies and plot the projected representations using t distributed stochastic neighbor embedding to depict the context similarities. these similarities expressed as movie distances can be used for movies recommendation. the recommended movies of this approach are compared with the recommended movies from imdb which use a collaborative filtering recommendation approach to show that our two models could constitute either an alternative or a supplementary recommendation approach.
a survey of available corpora for building data driven dialogue systems
during the past decade several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data driven models. in the area of dialogue systems the trend is less obvious and most practical systems are still built through significant engineering and expert knowledge. nevertheless several recent results suggest that data driven approaches are feasible and quite promising. to facilitate research in this area we have carried out a wide survey of publicly available datasets suitable for data driven learning of dialogue systems. we discuss important characteristics of these datasets how they can be used to learn diverse dialogue strategies and their other potential uses. we also examine methods for transfer learning between datasets and the use of external knowledge. finally we discuss appropriate choice of evaluation metrics for the learning objective.
generative topic embedding a continuous representation of documents extended version with proofs 
word embedding maps words into a low dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. on the other hand topic modeling maps documents onto a low dimensional topic space by utilizing the global word collocation patterns in the same document. these two types of patterns are complementary. in this paper we propose a generative topic embedding model to combine the two types of patterns. in our model topics are represented by embedding vectors and are shared across documents. the probability of each word is influenced by both its local context and its topic. a variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. jointly they represent the document in a low dimensional continuous space. in two document classification tasks our method performs better than eight existing methods with fewer features. in addition we illustrate with an example that our method can generate coherent topics even based on only one document.
fine grained entity typing with high multiplicity assignments
as entity type systems become richer and more fine grained we expect the number of types assigned to a given entity to increase. however most fine grained typing work has focused on datasets that exhibit a low degree of type multiplicity. in this paper we consider the high multiplicity regime inherent in data sources such as wikipedia that have semi open type systems. we introduce a set prediction approach to this problem and show that our model outperforms unstructured baselines on a new wikipedia based fine grained typing corpus.
towards a visual turing challenge
as language and visual understanding by machines progresses rapidly we are observing an increasing interest in holistic architectures that tightly interlink both modalities in a joint learning and inference process. this trend has allowed the community to progress towards more challenging and open tasks and refueled the hope at achieving the old ai dream of building machines that could pass a turing test in open domains. in order to steadily make progress towards this goal we realize that quantifying performance becomes increasingly difficult. therefore we ask how we can precisely define such challenges and how we can evaluate different algorithms on this open tasks in this paper we summarize and discuss such challenges as well as try to give answers where appropriate options are available in the literature. we exemplify some of the solutions on a recently presented dataset of question answering task based on real world indoor images that establishes a visual turing challenge. finally we argue despite the success of unique ground truth annotation we likely have to step away from carefully curated dataset and rather rely on social consensus as the main driving force to create suitable benchmarks. providing coverage in this inherently ambiguous output space is an emerging challenge that we face in order to make quantifiable progress in this area.
interactive robot learning of gestures language and affordances
a growing field in robotics and artificial intelligence ai research is human robot collaboration whose target is to enable effective teamwork between humans and robots. however in many situations human teams are still superior to human robot teams primarily because human teams can easily agree on a common goal with language and the individual members observe each other effectively leveraging their shared motor repertoire and sensorimotor resources. this paper shows that for cognitive robots it is possible and indeed fruitful to combine knowledge acquired from interacting with elements of the environment affordance exploration with the probabilistic observation of another agent s actions. we propose a model that unites i learning robot affordances and word descriptions with ii statistical recognition of human gestures with vision sensors. we discuss theoretical motivations possible implementations and we show initial results which highlight that after having acquired knowledge of its surrounding environment a humanoid robot can generalize this knowledge to the case when it observes another agent human partner performing the same motor actions previously executed during training.
visual features for context aware speech recognition
automatic transcriptions of consumer generated multi media content such as youtube videos still exhibit high word error rates. such data typically occupies a very broad domain has been recorded in challenging conditions with cheap hardware and a focus on the visual modality and may have been post processed or edited. in this paper we extend our earlier work on adapting the acoustic model of a dnn based speech recognition system to an rnn language model and show how both can be adapted to the objects and scenes that can be automatically detected in the video. we are working on a corpus of how to videos from the web and the idea is that an object that can be seen car or a scene that is being detected kitchen can be used to condition both models on the context of the recording thereby reducing perplexity and improving transcription. we achieve good improvements in both cases and compare and analyze the respective reductions in word error rate. we expect that our results can be used for any type of speech processing in which context information is available for example in robotics man machine interaction or when indexing large audio visual archives and should ultimately help to bring together the video to text and speech to text communities.
examining cooperation in visual dialog models
in this work we propose a blackbox intervention method for visual dialog models with the aim of assessing the contribution of individual linguistic or visual components. concretely we conduct structured or randomized interventions that aim to impair an individual component of the model and observe changes in task performance. we reproduce a state of the art visual dialog model and demonstrate that our methodology yields surprising insights namely that both dialog and image information have minimal contributions to task performance. the intervention method presented here can be applied as a sanity check for the strength and robustness of each component in visual dialog systems.
video highlight prediction using audience chat reactions
sports channel video portals offer an exciting domain for research on multimodal multilingual analysis. we present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real world audience discourse with complex slang in both english and traditional chinese. we present a novel dataset based on league of legends championships recorded from north american and taiwanese twitch.tv channels will be released for further research and demonstrate strong results on these using multimodal character level cnn rnn model architectures.
invariant representations for noisy speech recognition
modern automatic speech recognition asr systems need to be robust under acoustic variability arising from environmental speaker channel and recording conditions. ensuring such robustness to variability is a challenge in modern day neural network based asr systems especially when all types of variability are not seen during training. we attempt to address this problem by encouraging the neural network acoustic model to learn invariant feature representations. we use ideas from recent research on image generation using generative adversarial networks and domain adaptation ideas extending adversarial gradient based training. a recent work from ganin et al. proposes to use adversarial training for image domain adaptation by using an intermediate representation from the main target classification network to deteriorate the domain classifier performance through a separate neural network. our work focuses on investigating neural architectures which produce representations invariant to noise conditions for asr. we evaluate the proposed architecture on the aurora 4 task a popular benchmark for noise robust asr. we show that our method generalizes better than the standard multi condition training especially when only a few noise categories are seen during training.
self supervised vision based detection of the active speaker as a prerequisite for socially aware language acquisition
this paper presents a self supervised method for detecting the active speaker in a multi person spoken interaction scenario. we argue that this capability is a fundamental prerequisite for any artificial cognitive system attempting to acquire language in social settings. our methods are able to detect an arbitrary number of possibly overlapping active speakers based exclusively on visual information about their face. our methods do not rely on external annotations thus complying with cognitive development. instead they use information from the auditory modality to support learning in the visual domain. the methods have been extensively evaluated on a large multi person face to face interaction dataset. the results reach an accuracy of 80 on a multi speaker setting. we believe this system represents an essential component of any artificial cognitive system or robotic platform engaging in social interaction.
product characterisation towards personalisation learning attributes from unstructured data to recommend fashion products
in this paper we describe a solution to tackle a common set of challenges in e commerce which arise from the fact that new products are continually being added to the catalogue. the challenges involve properly personalising the customer experience forecasting demand and planning the product range. we argue that the foundational piece to solve all of these problems is having consistent and detailed information about each product information that is rarely available or consistent given the multitude of suppliers and types of products. we describe in detail the architecture and methodology implemented at asos one of the world s largest fashion e commerce retailers to tackle this problem. we then show how this quantitative understanding of the products can be leveraged to improve recommendations in a hybrid recommender system approach.
the self organization of speech sounds
the speech code is a vehicle of language it defines a set of forms used by a community to carry information. such a code is necessary to support the linguistic interactions that allow humans to communicate. how then may a speech code be formed prior to the existence of linguistic interactions moreover the human speech code is discrete and compositional shared by all the individuals of a community but different across communities and phoneme inventories are characterized by statistical regularities. how can a speech code with these properties form we try to approach these questions in the paper using the methodology of the artificial . we build a society of artificial agents and detail a mechanism that shows the formation of a discrete speech code without pre supposing the existence of linguistic capacities or of coordinated interactions. the mechanism is based on a low level model of sensory motor interactions. we show that the integration of certain very simple and non language specific neural devices leads to the formation of a speech code that has properties similar to the human speech code. this result relies on the self organizing properties of a generic coupling between perception and production within agents and on the interactions between agents. the artificial system helps us to develop better intuitions on how speech might have appeared by showing how self organization might have helped natural selection to find speech.
what the f measure doesn t measure features flaws fallacies and fixes
the f measure or f score is one of the most commonly used single number measures in information retrieval natural language processing and machine learning but it is based on a mistake and the flawed assumptions render it unsuitable for use in most contexts fortunately there are better alternatives.
a machine learning perspective on predictive coding with paq
paq8 is an open source lossless data compression algorithm that currently achieves the best compression rates on many benchmarks. this report presents a detailed description of paq8 from a statistical machine learning perspective. it shows that it is possible to understand some of the modules of paq8 and use this understanding to improve the method. however intuitive statistical explanations of the behavior of other modules remain elusive. we hope the description in this report will be a starting point for discussions that will increase our understanding lead to improvements to paq8 and facilitate a transfer of knowledge from paq8 to other machine learning methods such a recurrent neural networks and stochastic memoizers. finally the report presents a broad range of new applications of paq to machine learning tasks including language modeling and adaptive text prediction adaptive game playing classification and compression using features from the field of deep learning.
a novel frank wolfe algorithm. analysis and applications to large scale svm training
recently there has been a renewed interest in the machine learning community for variants of a sparse greedy approximation procedure for concave optimization known as the frank wolfe fw method . in particular this procedure has been successfully applied to train large scale instances of non linear support vector machines svms . specializing fw to svm training has allowed to obtain efficient algorithms but also important theoretical results including convergence analysis of training algorithms and new characterizations of model sparsity. in this paper we present and analyze a novel variant of the fw method based on a new way to perform away steps a classic strategy used to accelerate the convergence of the basic fw procedure. our formulation and analysis is focused on a general concave maximization problem on the simplex. however the specialization of our algorithm to quadratic forms is strongly related to some classic methods in computational geometry namely the gilbert and mdm algorithms. on the theoretical side we demonstrate that the method matches the guarantees in terms of convergence rate and number of iterations obtained by using classic away steps. in particular the method enjoys a linear rate of convergence a result that has been recently proved for mdm on quadratic forms. on the practical side we provide experiments on several classification datasets and evaluate the results using statistical tests. experiments show that our method is faster than the fw method with classic away steps and works well even in the cases in which classic away steps slow down the algorithm. furthermore these improvements are obtained without sacrificing the predictive accuracy of the obtained svm model.
semi supervised vocabulary informed learning
despite significant progress in object categorization in recent years a number of important challenges remain mainly ability to learn from limited labeled data and ability to recognize object classes within large potentially open set of labels. zero shot learning is one way of addressing these challenges but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes allowing former to inform the latter but not vice versa. we propose the notion of semi supervised vocabulary informed learning to alleviate the above mentioned challenges and address problems of supervised zero shot and open set recognition using a unified framework. specifically we propose a maximum margin framework for semantic manifold based recognition that incorporates distance constraints from both supervised and unsupervised vocabulary atoms ensuring that labeled samples are projected closest to their correct prototypes in the embedding space than to others. we show that resulting model shows improvements in supervised zero shot and large open set recognition with up to 310k class vocabulary on awa and imagenet datasets.
submodular meets structured finding diverse subsets in exponentially large structured item sets
to cope with the high level of ambiguity faced in domains such as computer vision or natural language processing robust prediction methods often search for a diverse set of high quality candidate solutions or proposals. in structured prediction problems this becomes a daunting task as the solution space image labelings sentence parses etc. is exponentially large. we study greedy algorithms for finding a diverse subset of solutions in structured output spaces by drawing new connections between submodular functions over combinatorial item sets and high order potentials hops studied for graphical models. specifically we show via examples that when marginal gains of submodular diversity functions allow structured representations this enables efficient sub linear time approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed hops. we discuss benefits tradeoffs and show that our constructions lead to significantly better proposals.
zm net real time zero shot image manipulation network
many problems in image processing and computer vision e.g. colorization style transfer can be posed as manipulating an input image into a corresponding output image given a user specified guiding signal. a holy grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals even signals unseen during training such as diverse paintings and arbitrary descriptive attributes. however existing methods are either inefficient to simultaneously process multiple signals let alone generalize to unseen signals or unable to handle signals from other modalities. in this paper we make the first attempt to address the zero shot image manipulation task. we cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal even unseen ones . to this end we propose the zero shot manipulation net zm net a fully differentiable architecture that jointly optimizes an image transformation network tnet and a parameter network pnet . the pnet learns to generate key transformation parameters for the tnet given any guiding signal while the tnet performs fast zero shot image manipulation according to both signal dependent parameters from the pnet and signal invariant parameters from the tnet itself. extensive experiments show that our zm net can perform high quality image manipulation conditioned on different forms of guiding signals e.g. style images and attributes in real time tens of milliseconds per image even for unseen signals. moreover a large scale style dataset with over 20 000 style images is also constructed to promote further research.
multi agent diverse generative adversarial networks
we propose an intuitive generalization to the generative adversarial networks gans and its conditional variants to address the well known mode collapse problem. firstly we propose a multi agent gan architecture incorporating multiple generators and one discriminator. secondly to enforce different generators to capture diverse high probability modes we modify discriminator s objective function where along with finding the real and fake samples the discriminator has to identify the generator that generated the fake sample. intuitively to succeed in this task the discriminator must learn to push different generators towards different identifiable modes. our framework mad gan is generalizable in the sense that it can be easily combined with other existing variants of gans to produce diverse samples. we perform extensive experiments on synthetic and real datasets and compare mad gan with different variants of gan. we show high quality diverse sample generations for the challenging tasks such as image to image translation known to learn delta distribution and face generation. in addition we show that mad gan is able to disentangle different modalities even when trained using highly challenging multi view dataset mixture of forests icebergs bedrooms etc . in the end we also show its efficacy for the unsupervised feature representation task. in the appendix we introduce a similarity based competing objective which encourages the different generators to generate varied samples judged by a user defined similarity metric. we show extensive evaluations on a 1 d setting of mixture of gaussians for non parametric density estimation. the theoretical proofs back the efficacy of the framework and explains why various generators are pushed towards distinct clusters of modes.
geometric gan
generative adversarial nets gans represent an important milestone for effective generative models which has inspired numerous variants seemingly different from each other. one of the main contributions of this paper is to reveal a unified geometric structure in gan and its variants. specifically we show that the adversarial generative model training can be decomposed into three geometric steps separating hyperplane search discriminator parameter update away from the separating hyperplane and the generator update along the normal vector direction of the separating hyperplane. this geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric gan using svm separating hyperplane that maximizes the margin. our theoretical analysis shows that the geometric gan converges to a nash equilibrium between the discriminator and generator. in addition extensive numerical results show that the superior performance of geometric gan.
a data and model parallel distributed and scalable framework for training of deep networks in apache spark
training deep networks is expensive and time consuming with the training period increasing with data size and growth in model parameters. in this paper we provide a framework for distributed training of deep networks over a cluster of cpus in apache spark. the framework implements both data parallelism and model parallelism making it suitable to use for deep networks which require huge training data and model parameters which are too big to fit into the memory of a single machine. it can be scaled easily over a cluster of cheap commodity hardware to attain significant speedup and obtain better results making it quite economical as compared to farm of gpus and supercomputers. we have proposed a new algorithm for training of deep networks for the case when the network is partitioned across the machines model parallelism along with detailed cost analysis and proof of convergence of the same. we have developed implementations for fully connected feedforward networks convolutional neural networks recurrent neural networks and long short term memory architectures. we present the results of extensive simulations demonstrating the speedup and accuracy obtained by our framework for different sizes of the data and model parameters with variation in the number of worker cores partitions thereby showing that our proposed framework can achieve significant speedup upto 11x for cnn and is also quite scalable.
understanding and comparing deep neural networks for age and gender classification
recently deep neural networks have demonstrated excellent performances in recognizing the age and gender on human face images. however these models were applied in a black box manner with no information provided about which facial features are actually used for prediction and how these features depend on image preprocessing model initialization and architecture choice. we present a study investigating these different effects. in detail our work compares four popular neural network architectures studies the effect of pretraining evaluates the robustness of the considered alignment preprocessings via cross method test set swapping and intuitively visualizes the model s prediction strategies in given preprocessing conditions using the recent layer wise relevance propagation lrp algorithm. our evaluations on the challenging adience benchmark show that suitable parameter initialization leads to a holistic perception of the input compensating artefactual data representations. with a combination of simple preprocessing steps we reach state of the art performance in gender recognition.
when is a convolutional filter easy to learn 
we analyze the convergence of stochastic gradient descent algorithm for learning a convolutional filter with rectified linear unit relu activation function. our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of relu in contrast with previous works that are restricted to standard gaussian input. we show that stochastic gradient descent with random initialization can learn the convolutional filter in polynomial time and the convergence rate depends on the smoothness of the input distribution and the closeness of patches. to the best of our knowledge this is the first recovery guarantee of gradient based algorithms for convolutional filter on non gaussian input distributions. our theory also justifies the two stage learning rate strategy in deep neural networks. while our focus is theoretical we also present experiments that illustrate our theoretical findings.
learning sparse visual representations with leaky capped norm regularizers
sparsity inducing regularization is an important part for learning over complete visual representations. despite the popularity of ell 1 regularization in this paper we investigate the usage of non convex regularizations in this problem. our contribution consists of three parts. first we propose the leaky capped norm regularization lcnr which allows model weights below a certain threshold to be regularized more strongly as opposed to those above therefore imposes strong sparsity and only introduces controllable estimation bias. we propose a majorization minimization algorithm to optimize the joint objective function. second our study over monocular 3d shape recovery and neural networks with lcnr outperforms ell 1 and other non convex regularizations achieving state of the art performance and faster convergence. third we prove a theoretical global convergence speed on the 3d recovery problem. to the best of our knowledge this is the first convergence analysis of the 3d recovery problem.
convnets and imagenet beyond accuracy explanations bias detection adversarial examples and model criticism
convnets and imagenet have driven the recent success of deep learning for image classification. however the marked slowdown in performance improvement the recent studies on the lack of robustness of neural networks to adversarial examples and their tendency to exhibit undesirable biases e.g racial biases questioned the reliability and the sustained development of these methods. this work investigates these questions from the perspective of the end user by using human subject studies and explanations. we experimentally demonstrate that the accuracy and robustness of convnets measured on imagenet are underestimated. we show that explanations can mitigate the impact of misclassified adversarial examples from the perspective of the end user and we introduce a novel tool for uncovering the undesirable biases learned by a model. these contributions also show that explanations are a promising tool for improving our understanding of convnets predictions and for designing more reliable models
gradient descent learns one hidden layer cnn don t be afraid of spurious local minima
we consider the problem of learning a one hidden layer neural network with non overlapping convolutional layer and relu activation function i.e. f mathbf z mathbf w mathbf a sum j a j sigma mathbf w top mathbf z j in which both the convolutional weights mathbf w and the output weights mathbf a are parameters to be learned. we prove that with gaussian input mathbf z there is a spurious local minimum that is not a global mininum. surprisingly in the presence of local minimum starting from randomly initialized weights gradient descent with weight normalization can still be proven to recover the true parameters with constant probability which can be boosted to arbitrarily high accuracy with multiple restarts . we also show that with constant probability the same procedure could also converge to the spurious local minimum showing that the local minimum plays a non trivial role in the dynamics of gradient descent. furthermore a quantitative analysis shows that the gradient descent dynamics has two phases it starts off slow but converges much faster after several iterations.
curiosity driven exploration by self supervised prediction
in many real world scenarios rewards extrinsic to the agent are extremely sparse or absent altogether. in such cases curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. we formulate curiosity as the error in an agent s ability to predict the consequence of its own actions in a visual feature space learned by a self supervised inverse dynamics model. our formulation scales to high dimensional continuous state spaces like images bypasses the difficulties of directly predicting pixels and critically ignores the aspects of the environment that cannot affect the agent. the proposed approach is evaluated in two environments vizdoom and super mario bros. three broad settings are investigated 1 sparse extrinsic reward where curiosity allows for far fewer interactions with the environment to reach the goal 2 exploration with no extrinsic reward where curiosity pushes the agent to explore more efficiently and 3 generalization to unseen scenarios e.g. new levels of the same game where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. demo video and code available at https pathak22.github.io noreward rl 
houdini fooling deep structured prediction models
generating adversarial examples is a critical step for evaluating and improving the robustness of learning machines. so far most existing methods only work for classification and are not designed to alter the true performance measure of the problem at hand. we introduce a novel flexible approach named houdini for generating adversarial examples specifically tailored for the final performance measure of the task considered be it combinatorial and non decomposable. we successfully apply houdini to a range of applications such as speech recognition pose estimation and semantic segmentation. in all cases the attacks based on houdini achieve higher success rate than those based on the traditional surrogates used to train the models while using a less perceptible adversarial perturbation.
recent advances in zero shot recognition
with the recent renaissance of deep convolution neural networks encouraging breakthroughs have been achieved on the supervised recognition tasks where each class has sufficient training data and fully annotated training data. however to scale the recognition to a large number of classes with few or now training samples for each class remains an unsolved problem. one approach to scaling up the recognition is to develop models capable of recognizing unseen categories without any training instances or zero shot recognition learning. this article provides a comprehensive review of existing zero shot recognition techniques covering various aspects ranging from representations of models and from datasets and evaluation settings. we also overview related recognition tasks including one shot and open set recognition which can be used as natural extensions of zero shot recognition when limited number of class samples become available or when zero shot recognition is implemented in a real world setting. importantly we highlight the limitations of existing approaches and point out future research directions in this existing new research area.
the loss surface and expressivity of deep convolutional neural networks
we analyze the expressiveness and loss surface of practical deep convolutional neural networks cnns with shared weights and max pooling layers. we show that such cnns produce linearly independent features at a wide layer which has more neurons than the number of training samples. this condition holds e.g. for the vgg network. furthermore we provide for such wide cnns necessary and sufficient conditions for global minima with zero training error. for the case where the wide layer is followed by a fully connected layer we show that almost every critical point of the empirical loss is a global minimum with zero training error. our analysis suggests that both depth and width are very important in deep learning. while depth brings more representational power and allows the network to learn high level features width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network has a well behaved loss surface with potentially no bad local minima.
physics guided neural networks pgnn an application in lake temperature modeling
this paper introduces a novel framework for combining scientific knowledge of physics based models with neural networks to advance scientific discovery. this framework termed as physics guided neural network pgnn leverages the output of physics based model simulations along with observational features to generate predictions using a neural network architecture. further this paper presents a novel framework for using physics based loss functions in the learning objective of neural networks to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set. we illustrate the effectiveness of pgnn for the problem of lake temperature modeling where physical relationships between the temperature density and depth of water are used to design a physics based loss function. by using scientific knowledge to guide the construction and learning of neural networks we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results.
unified spectral clustering with optimal graph
spectral clustering has found extensive use in many areas. most traditional spectral clustering algorithms work in three separate steps similarity graph construction continuous labels learning discretizing the learned labels by k means clustering. such common practice has two potential flaws which may lead to severe information loss and performance degradation. first predefined similarity graph might not be optimal for subsequent clustering. it is well accepted that similarity graph highly affects the clustering results. to this end we propose to automatically learn similarity information from data and simultaneously consider the constraint that the similarity matrix has exact c connected components if there are c clusters. second the discrete solution may deviate from the spectral solution since k means method is well known as sensitive to the initialization of cluster centers. in this work we transform the candidate solution into a new one that better approximates the discrete one. finally those three subtasks are integrated into a unified framework with each subtask iteratively boosted by using the results of the others towards an overall optimal solution. it is known that the performance of a kernel method is largely determined by the choice of kernels. to tackle this practical problem of how to select the most suitable kernel for a particular data set we further extend our model to incorporate multiple kernel learning ability. extensive experiments demonstrate the superiority of our proposed method as compared to existing clustering approaches.
on the inductive bias of dropout
dropout is a simple but effective technique for learning in neural networks and other settings. a sound theoretical understanding of dropout is needed to determine when dropout should be applied and how to use it most effectively. in this paper we continue the exploration of dropout as a regularizer pioneered by wager et.al. we focus on linear classification where a convex proxy to the misclassification loss i.e. the logistic loss used in logistic regression is minimized. we show a when the dropout regularized criterion has a unique minimizer b when the dropout regularization penalty goes to infinity with the weights and when it remains bounded c that the dropout regularization can be non monotonic as individual weights increase from 0 and d that the dropout regularization penalty may not be convex. this last point is particularly surprising because the combination of dropout regularization with any convex loss proxy is always a convex function. in order to contrast dropout regularization with l 2 regularization we formalize the notion of when different sources are more compatible with different regularizers. we then exhibit distributions that are provably more compatible with dropout regularization than l 2 regularization and vice versa. these sources provide additional insight into how the inductive biases of dropout and l 2 regularization differ. we provide some similar results for l 1 regularization.
surprising properties of dropout in deep networks
we analyze dropout in deep networks with rectified linear units and the quadratic loss. our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. for example on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. this provides a counterpoint to the suggestion that dropout discourages co adaptation of weights. we also show that the dropout penalty can grow exponentially in the depth of the network while the weight decay penalty remains essentially linear and that dropout is insensitive to various re scalings of the input features outputs and network weights. this last insensitivity implies that there are no isolated local minima of the dropout training criterion. our work uncovers new properties of dropout extends our understanding of why dropout succeeds and lays the foundation for further progress.
training probabilistic spiking neural networks with first to spike decoding
third generation neural networks or spiking neural networks snns aim at harnessing the energy efficiency of spike domain processing by building on computing elements that operate on and exchange spikes. in this paper the problem of training a two layer snn is studied for the purpose of classification under a generalized linear model glm probabilistic neural model that was previously considered within the computational neuroscience literature. conventional classification rules for snns operate offline based on the number of output spikes at each output neuron. in contrast a novel training method is proposed here for a first to spike decoding rule whereby the snn can perform an early classification decision once spike firing is detected at an output neuron. numerical results bring insights into the optimal parameter selection for the glm neuron and on the accuracy complexity trade off performance of conventional and first to spike decoding.
a novel clustering algorithm based on quantum games
enormous successes have been made by quantum algorithms during the last decade. in this paper we combine the quantum game with the problem of data clustering and then develop a quantum game based clustering algorithm in which data points in a dataset are considered as players who can make decisions and implement quantum strategies in quantum games. after each round of a quantum game each player s expected payoff is calculated. later he uses a link removing and rewiring lrr function to change his neighbors and adjust the strength of links connecting to them in order to maximize his payoff. further algorithms are discussed and analyzed in two cases of strategies two payoff matrixes and two lrr functions. consequently the simulation results have demonstrated that data points in datasets are clustered reasonably and efficiently and the clustering algorithms have fast rates of convergence. moreover the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.
exact solutions to the nonlinear dynamics of learning in deep linear neural networks
despite the widespread practical success of deep learning methods our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. we attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. despite the linearity of their input output map such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. we show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks including long plateaus followed by rapid transitions to lower error solutions and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. we provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity learning speed can nevertheless remain finite for a special class of initial conditions on the weights very deep networks incur only a finite depth independent delay in learning speed relative to shallow networks. we show that under certain conditions on the training data unsupervised pretraining can find this special class of initial conditions while scaled random gaussian initializations cannot. we further exhibit a new class of random orthogonal initial conditions on weights that like unsupervised pre training enjoys depth independent learning times. we further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks as long as they operate in a special regime known as the edge of chaos.
entropy of overcomplete kernel dictionaries
in signal analysis and synthesis linear approximation theory considers a linear decomposition of any given signal in a set of atoms collected into a so called dictionary. relevant sparse representations are obtained by relaxing the orthogonality condition of the atoms yielding overcomplete dictionaries with an extended number of atoms. more generally than the linear decomposition overcomplete kernel dictionaries provide an elegant nonlinear extension by defining the atoms through a mapping kernel function e.g. the gaussian kernel . models based on such kernel dictionaries are used in neural networks gaussian processes and online learning with kernels. the quality of an overcomplete dictionary is evaluated with a diversity measure the distance the approximation the coherence and the babel measures. in this paper we develop a framework to examine overcomplete kernel dictionaries with the entropy from information theory. indeed a higher value of the entropy is associated to a further uniform spread of the atoms over the space. for each of the aforementioned diversity measures we derive lower bounds on the entropy. several definitions of the entropy are examined with an extensive analysis in both the input space and the mapped feature space.
rotation invariant convolutional neural networks for galaxy morphology prediction
measuring the morphological parameters of galaxies is a key requirement for studying their formation and evolution. surveys such as the sloan digital sky survey sdss have resulted in the availability of very large collections of images which have permitted population wide analyses of galaxy morphology. morphological analysis has traditionally been carried out mostly via visual inspection by trained experts which is time consuming and does not scale to large gtrsim10 4 numbers of images. although attempts have been made to build automated classification systems these have not been able to achieve the desired level of accuracy. the galaxy zoo project successfully applied a crowdsourcing strategy inviting online users to classify images by answering a series of questions. unfortunately even this approach does not scale well enough to keep up with the increasing availability of galaxy images. we present a deep neural network model for galaxy morphology classification which exploits translational and rotational symmetry. it was developed in the context of the galaxy challenge an international competition to build the best model for morphology classification based on annotated images from the galaxy zoo project. for images with high agreement among the galaxy zoo participants our model is able to reproduce their consensus with near perfect accuracy 99 for most questions. confident model predictions are highly accurate which makes the model suitable for filtering large collections of images and forwarding challenging images to experts for manual annotation. this approach greatly reduces the experts workload without affecting accuracy. the application of these algorithms to larger sets of training data will be critical for analysing results from future surveys such as the lsst.
kernel nonnegative matrix factorization without the curse of the pre image application to unmixing hyperspectral images
the nonnegative matrix factorization nmf is widely used in signal and image processing including bio informatics blind source separation and hyperspectral image analysis in remote sensing. a great challenge arises when dealing with a nonlinear formulation of the nmf. within the framework of kernel machines the models suggested in the literature do not allow the representation of the factorization matrices which is a fallout of the curse of the pre image. in this paper we propose a novel kernel based model for the nmf that does not suffer from the pre image problem by investigating the estimation of the factorization matrices directly in the input space. for different kernel functions we describe two schemes for iterative algorithms an additive update rule based on a gradient descent scheme and a multiplicative update rule in the same spirit as in the lee and seung algorithm. within the proposed framework we develop several extensions to incorporate constraints including sparseness smoothness and spatial regularization with a total variation like penalty. the effectiveness of the proposed method is demonstrated with the problem of unmixing hyperspectral images using well known real images and results with state of the art techniques.
approximation errors of online sparsification criteria
many machine learning frameworks such as resource allocating networks kernel based methods gaussian processes and radial basis function networks require a sparsification scheme in order to address the online learning paradigm. for this purpose several online sparsification criteria have been proposed to restrict the model definition on a subset of samples. the most known criterion is the linear approximation criterion which discards any sample that can be well represented by the already contributing samples an operation with excessive computational complexity. several computationally efficient sparsification criteria have been introduced in the literature such as the distance the coherence and the babel criteria. in this paper we provide a framework that connects these sparsification criteria to the issue of approximating samples by deriving theoretical bounds on the approximation errors. moreover we investigate the error of approximating any feature by proposing upper bounds on the approximation error for each of the aforementioned sparsification criteria. two classes of features are described in detail the empirical mean and the principal axes in the kernel principal component analysis.
discrete deep feature extraction a theory and new architectures
first steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made for the continuous time case in mallat 2012 and wiatowski and b olcskei 2015. this paper considers the discrete case introduces new convolutional neural network architectures and proposes a mathematical framework for their analysis. specifically we establish deformation and translation sensitivity results of local and global nature and we investigate how certain structural properties of the input signal are reflected in the corresponding feature vectors. our theory applies to general filters and general lipschitz continuous non linearities and pooling operators. experiments on handwritten digit classification and facial landmark detection including feature importance evaluation complement the theoretical findings.
neural responding machine for short text conversation
we propose neural responding machine nrm a neural network based response generator for short text conversation. nrm takes the general encoder decoder framework it formalizes the generation of response as a decoding process based on the latent representation of the input text while both encoding and decoding are realized with recurrent neural networks rnn . the nrm is trained with a large amount of one round conversation data collected from a microblogging service. empirical study shows that nrm can generate grammatically correct and content wise appropriate responses to over 75 of the input text outperforming state of the arts in the same setting including retrieval based and smt based models.
deep active learning for dialogue generation
we propose an online end to end neural generative conversational model for open domain dialogue. it is trained using a unique combination of offline two phase supervised learning and online human in the loop active learning. while most existing research proposes offline supervision or hand crafted reward functions for online reinforcement we devise a novel interactive learning mechanism based on hamming diverse beam search for response generation and one character user feedback at each step. experiments show that our model inherently promotes the generation of semantically relevant and interesting responses and can be used to train agents with customized personas moods and conversational styles.
teaching machines to read and comprehend
teaching machines to read natural language documents remains an elusive challenge. machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen but until now large scale training and test datasets have been missing for this type of evaluation. in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.
syntax aware multi sense word embeddings for deep compositional models of meaning
deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of nlp research. we detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. furthermore each word is associated with a number of senses the most plausible of which is selected dynamically during the composition process. we evaluate the produced vectors qualitatively and quantitatively with positive results. at the sentence level the effectiveness of the framework is demonstrated on the msrpar task for which we report results within the state of the art range.
a deep architecture for semantic matching with multiple positional sentence representations
matching natural language sentences is central for many applications such as information retrieval and question answering. existing deep models rely on a single sentence representation or multiple granularity representations for matching. however such methods cannot well capture the contextualized local information in the matching process. to tackle this problem we present a new deep architecture to match two sentences with multiple positional sentence representations. specifically each positional sentence representation is a sentence representation at this position generated by a bidirectional long short term memory bi lstm . the matching score is finally produced by aggregating interactions between these different positional sentence representations through k max pooling and a multi layer perceptron. our model has several advantages 1 by using bi lstm rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation 2 by matching with multiple positional sentence representations it is flexible to aggregate different important contextualized local information in a sentence to support the matching 3 experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model.
lstm neural reordering feature for statistical machine translation
artificial neural networks are powerful models which have been widely applied into many aspects of machine translation such as language modeling and translation modeling. though notable improvements have been made in these areas the reordering problem still remains a challenge in statistical machine translations. in this paper we present a novel neural reordering model that directly models word pairs and alignment. by utilizing lstm recurrent neural networks much longer context could be learned for reordering prediction. experimental results on nist openmt12 arabic english and chinese english 1000 best rescoring task show that our lstm neural reordering feature is robust and achieves significant improvements over various baseline systems.
learning natural language inference with lstm
natural language inference nli is a fundamentally important task in natural language processing that has many applications. the recently released stanford natural language inference snli corpus has made it possible to develop and evaluate learning centered methods such as deep neural networks for natural language inference nli . in this paper we propose a special long short term memory lstm architecture for nli. our model builds on top of a recently proposed neural attention model for nli but is based on a significantly different idea. instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification our solution uses a match lstm to perform word by word matching of the hypothesis with the premise. this lstm is able to place more emphasis on important word level matching results. in particular we observe that this lstm remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. on the snli corpus our model achieves an accuracy of 86.1 outperforming the state of the art.
quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive lstms
recursive neural networks rnn and their recently proposed extension recursive long short term memory networks rlstm are models that compute representations for sentences by recursively combining word embeddings according to an externally provided parse tree. both models thus unlike recurrent networks explicitly make use of the hierarchical structure of a sentence. in this paper we demonstrate that rnns nevertheless suffer from the vanishing gradient and long distance dependency problem and that rlstms greatly improve over rnn s on these problems. we present an artificial learning task that allows us to quantify the severity of these problems for both models. we further show that a ratio of gradients at the root node and a focal leaf node is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. this paper thus provides an explanation for existing superior results of rlstms on tasks such as sentiment analysis and suggests that the benefits of including hierarchical structure and of including lstm style gating are complementary.
implicit discourse relation classification via multi task neural networks
without discourse connectives classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. previous research usually makes use of one kind of discourse framework such as pdtb or rst to improve the classification performance on discourse relations. actually under different discourse annotation frameworks there exist multiple corpora which have internal connections. to exploit the combination of different discourse corpora we design related discourse classification tasks specific to a corpus and propose a novel convolutional neural network embedded multi task learning system to synthesize these tasks by learning both unique and shared representations for each task. the experimental results on the pdtb implicit discourse relation classification task demonstrate that our model achieves significant gains over baseline systems.
enhancing sentence relation modeling with auxiliary character level embedding
neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. however the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. to address this challenge we propose a new deep neural network architecture that jointly leverage pre trained word embedding and auxiliary character embedding to learn sentence meanings. the two kinds of word sequence representations as inputs into multi layer bidirectional lstm to learn enhanced sentence representation. after that we construct matching features followed by another temporal cnn to learn high level hidden matching feature representations. experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets.
automatic open knowledge acquisition via long short term memory networks with feedback negative sampling
previous studies in open information extraction open ie are mainly based on extraction patterns. they manually define patterns or automatically learn them from a large corpus. however these approaches are limited when grasping the context of a sentence and they fail to capture implicit relations. in this paper we address this problem with the following methods. first we exploit long short term memory lstm networks to extract higher level features along the shortest dependency paths connecting headwords of relations and arguments. the path level features from lstm networks provide useful clues regarding contextual information and the validity of arguments. second we constructed samples to train lstm networks without the need for manual labeling. in particular feedback negative sampling picks highly negative samples among non positive samples through a model trained with positive samples. the experimental results show that our approach produces more precise and abundant extractions than state of the art open ie systems. to the best of our knowledge this is the first work to apply deep learning to open ie.
question answering over knowledge base with neural attention combining global knowledge information
with the rapid growth of knowledge bases kbs on the web how to take full advantage of them becomes increasingly important. knowledge base based question answering kb qa is one of the most promising approaches to access the substantial knowledge. meantime as the neural network based nn based methods develop nn based kb qa has already achieved impressive results. however previous work did not put emphasis on question representation and the question is converted into a fixed vector regardless of its candidate answers. this simple representation strategy is unable to express the proper information of the question. hence we present a neural attention based model to represent the questions dynamically according to the different focuses of various candidate answer aspects. in addition we leverage the global knowledge inside the underlying kb aiming at integrating the rich kb information into the representation of the answers. and it also alleviates the out of vocabulary oov problem which helps the attention model to represent the question more precisely. the experimental results on webquestions demonstrate the effectiveness of the proposed approach.
generating natural language inference chains
the ability to reason with natural language is a fundamental prerequisite for many nlp tasks such as information extraction machine translation and question answering. to quantify this ability systems are commonly tested whether they can recognize textual entailment i.e. whether one sentence can be inferred from another one. however in most nlp applications only single source sentences instead of sentence pairs are available. hence we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. we take entailment pairs of the stanford natural language inference corpus and train an lstm with attention. on a manually annotated test set we found that 82 of generated sentences are correct an improvement of 10.3 over an lstm baseline. a qualitative analysis shows that this model is not only capable of shortening input sentences but also inferring new statements via paraphrasing and phrase entailment. we then apply this model recursively to input output pairs thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. finally by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence.
mufuru the multi function recurrent unit
recurrent neural networks such as the gru and lstm found wide adoption in natural language processing and achieve state of the art results for many tasks. these models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. however they only cover a small subset of potentially useful compositions. we propose multi function recurrent units mufurus that allow for arbitrary differentiable functions as composition operations. furthermore mufurus allow for an input and state dependent choice of these composition operations that is learned. our experiments demonstrate that the additional functionality helps in different sequence modeling tasks including the evaluation of propositional logic formulae language modeling and sentiment analysis.
lstmvis a tool for visual analysis of hidden state dynamics in recurrent neural networks
recurrent neural networks and in particular long short term memory lstm networks are a remarkably effective tool for sequence modeling that learn a dense black box hidden representation of their sequential input. researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. in this work we present lstmvis a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. the tool allows users to select a hypothesis input range to focus on local state changes to match these states changes to similar patterns in a large data set and to align these results with structural annotations from their domain. we show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting phrase structure and chord progressions and demonstrate how the tool can be used to isolate patterns for further statistical analysis. we characterize the domain the different stakeholders and their goals and tasks.
compression of neural machine translation models via pruning
neural machine translation nmt like many other deep learning domains typically suffers from over parameterization resulting in large storage sizes. this paper examines three simple magnitude based pruning schemes to compress nmt models namely class blind class uniform and class distribution which differ in terms of how pruning thresholds are computed for the different classes of weights in the nmt architecture. we demonstrate the efficacy of weight pruning as a compression technique for a state of the art nmt system. we show that an nmt model with over 200 million parameters can be pruned by 40 with very little performance loss as measured on the wmt 14 english german translation task. this sheds light on the distribution of redundancy in the nmt architecture. our main result is that with retraining we can recover and even surpass the original performance with an 80 pruned model.
constructing a natural language inference dataset using generative neural networks
natural language inference is an important task for natural language understanding. it is concerned with classifying the logical relation between two sentences. in this paper we propose several text generative neural networks for generating text hypothesis which allows construction of new natural language inference datasets. to evaluate the models we propose a new metric the accuracy of the classifier trained on the generated dataset. the accuracy obtained by our best generative model is only 2.7 lower than the accuracy of the classifier trained on the original human crafted dataset. furthermore the best generated dataset combined with the original dataset achieves the highest accuracy. the best model learns a mapping embedding for each training example. by comparing various metrics we show that datasets that obtain higher rouge or meteor scores do not necessarily yield higher classification accuracies. we also provide analysis of what are the characteristics of a good dataset including the distinguishability of the generated datasets from the original one.
dataset and neural recurrent sequence labeling model for open domain factoid question answering
while question answering qa with neural network i.e. neural qa has achieved promising results in recent years lacking of large scale real word qa dataset is still a challenge for developing and evaluating neural qa system. to alleviate this problem we propose a large scale human annotated real world qa dataset webqa with more than 42k questions and 556k evidences. as existing neural qa methods resolve qa either as sequence generation or classification ranking problem they face challenges of expensive softmax computation unseen answers handling or separate candidate answer generation component. in this work we cast neural qa as a sequence labeling problem and propose an end to end sequence labeling model which overcomes all the above challenges. experimental results on webqa show that our model outperforms the baselines significantly with an f1 score of 74.69 with word based input and the performance drops only 3.72 f1 points with more challenging character based input.
tweet2vec learning tweet embeddings using character level cnn lstm encoder decoder
we present tweet2vec a novel method for generating general purpose vector representation of tweets. the model learns tweet embeddings using character level cnn lstm encoder decoder. we trained our model on 3 million randomly selected english language tweets. the model was evaluated using two methods tweet semantic similarity and tweet sentiment categorization outperforming the previous state of the art in both tasks. the evaluations demonstrate the power of the tweet embeddings generated by our model for various tweet categorization tasks. the vector representations generated by our model are generic and hence can be applied to a variety of tasks. though the model presented in this paper is trained on english language tweets the method presented can be used to learn tweet embeddings for different languages.
online segment to segment neural transduction
we introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. by independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. our model tackles the bottleneck of vanilla encoder decoders that have to read and memorize the entire input sequence in their fixed length hidden states before producing any output. it is different from previous attentive models in that instead of treating the attention weights as output of a deterministic function our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder decoders.
semantic parsing with semi supervised sequential autoencoders
we present a novel semi supervised approach for sequence transduction and apply it to semantic parsing. the unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. we apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms.
exploiting sentence and context representations in deep neural models for spoken language understanding
this paper presents a deep learning architecture for the semantic decoder component of a statistical spoken dialogue system. in a slot filling dialogue the semantic decoder predicts the dialogue act and a set of slot value pairs from a set of n best hypotheses returned by the automatic speech recognition. most current models for spoken language understanding assume i word aligned semantic annotations as in sequence taggers and ii delexicalisation or a mapping of input words to domain specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation e.g. morphology synonyms paraphrasing . in this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation. the proposed architecture uses a convolutional neural network for the sentence representation and a long short term memory network for the context representation. results are presented for the publicly available dstc2 corpus and an in car corpus which is similar to dstc2 but has a significantly higher word error rate wer .
the neural noisy channel
we formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. unlike direct models which can suffer from explaining away effects during training noisy channel models must produce outputs that explain their inputs and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol we obtain a tractable and effective beam search decoder. experimental results on abstractive sentence summarisation morphological inflection and machine translation show that noisy channel models outperform direct models and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.
generative deep neural networks for dialogue a short review
researchers have recently started investigating deep neural networks for dialogue applications. in particular generative sequence to sequence seq2seq models have shown promising results for unstructured tasks such as word level dialogue response generation. the hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies while requiring a minimum amount of domain knowledge and hand crafting. an important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses. in support of this goal we review recently proposed models based on generative encoder decoder neural network architectures and show that these models have better ability to incorporate long term dialogue history to model uncertainty and ambiguity in dialogue and to generate responses with high level compositional structure.
learning python code suggestion with a sparse pointer network
to enhance developer productivity all modern integrated development environments ides include code suggestion functionality that proposes likely next tokens at the cursor. while current ides work well for statically typed languages their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically typed languages. moreover suggestion engines in modern ides do not propose expressions or multi statement idiomatic code. recent work has shown that language models can improve code suggestion systems by learning from software repositories. this paper introduces a neural language model with a sparse pointer network aimed at capturing very long range dependencies. we release a large scale code suggestion corpus of 41m lines of python code crawled from github. on this corpus we found standard neural language models to perform well at suggesting local phenomena but struggle to refer to identifiers that are introduced many tokens in the past. by augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an lstm baseline. in fact this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. furthermore a qualitative analysis shows this model indeed captures interesting long range dependencies like referring to a class member defined over 60 tokens in the past.
opennmt open source toolkit for neural machine translation
we describe an open source toolkit for neural machine translation nmt . the toolkit prioritizes efficiency modularity and extensibility with the goal of supporting nmt research into model architectures feature representations and source modalities while maintaining competitive performance and reasonable training requirements. the toolkit consists of modeling and translation support as well as detailed pedagogical documentation about the underlying techniques.
making neural qa as simple as possible but not simpler
recent development of large scale question answering qa datasets triggered a substantial amount of research into end to end neural architectures for qa. increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. in this work we propose a simple heuristic that guides the development of neural baseline systems for the extractive qa task. we find that there are two ingredients necessary for building a high performing neural qa system first the awareness of question words while processing the context and second a composition function that goes beyond simple bag of words modeling such as recurrent neural networks. our results show that fastqa a system that meets these two requirements can achieve very competitive performance compared with existing models. we argue that this surprising finding puts results of previous systems and the complexity of recent qa datasets into perspective.
survey of the state of the art in natural language generation core tasks applications and evaluation
this paper surveys the current state of the art in natural language generation nlg defined as the task of generating text or speech from non linguistic input. a survey of nlg is timely in view of the changes that the field has undergone over the past decade or so especially in relation to new usually data driven methods as well as new applications of nlg technology. this survey therefore aims to a give an up to date synthesis of research on the core tasks in nlg and the architectures adopted in which such tasks are organised b highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between nlg and other areas of artificial intelligence c draw attention to the challenges in nlg evaluation relating them to similar challenges faced in other areas of natural language processing with an emphasis on different evaluation methods and the relationships between them.
a constrained sequence to sequence neural model for sentence simplification
sentence simplification reduces semantic complexity to benefit people with language impairments. previous simplification studies on the sentence level and word level have achieved promising results but also meet great challenges. for sentence level studies sentences after simplification are fluent but sometimes are not really simplified. for word level studies words are simplified but also have potential grammar errors due to different usages of words before and after simplification. in this paper we propose a two step simplification framework by combining both the word level and the sentence level simplifications making use of their corresponding advantages. based on the two step framework we implement a novel constrained neural generation model to simplify sentences given simplified words. the final results on wikipedia and simple wikipedia aligned datasets indicate that our method yields better performance than various baselines.
improved neural relation detection for knowledge base question answering
relation detection is a core component for many nlp applications including knowledge base question answering kbqa . in this paper we propose a hierarchical recurrent neural network enhanced by residual learning that detects kb relations given an input question. our method uses deep residual bidirectional lstms to compare questions and relation names via different hierarchies of abstraction. additionally we propose a simple kbqa system that integrates entity linking and our proposed relation detector to enable one enhance another. experimental results evidence that our approach achieves not only outstanding relation detection performance but more importantly it helps our kbqa system to achieve state of the art accuracy for both single relation simplequestions and multi relation webqsp qa benchmarks.
asr error management for improving spoken language understanding
this paper addresses the problem of automatic speech recognition asr error detection and their use for improving spoken language understanding slu systems. in this study the slu task consists in automatically extracting from asr transcriptions semantic concepts and concept values pairs in a e.g touristic information system. an approach is proposed for enriching the set of semantic labels with error specific labels and by using a recently proposed neural approach based on word embeddings to compute well calibrated asr confidence measures. experimental results are reported showing that it is possible to decrease significantly the concept value error rate with a state of the art system outperforming previously published results performance on the same experimental data. it also shown that combining an slu approach based on conditional random fields with a neural encoder decoder attention based architecture it is possible to effectively identifying confidence islands and uncertain semantic output segments useful for deciding appropriate error handling actions by the dialogue manager strategy .
dynamic integration of background knowledge in neural nlu systems
common sense or background knowledge is required to understand natural language but in most neural natural language understanding nlu systems the requisite background knowledge is indirectly acquired from static corpora. we develop a new reading architecture for the dynamic integration of explicit background knowledge in nlu models. a new task agnostic reading module provides refined word representations to a task specific nlu architecture by processing background knowledge in the form of free text statements together with the task specific inputs. strong performance on the tasks of document question answering dqa and recognizing textual entailment rte demonstrate the effectiveness and flexibility of our approach. analysis shows that our models learn to exploit knowledge selectively and in a semantically appropriate way.
rethinking skip thought a neighborhood based approach
we study the skip thought model with neighborhood information as weak supervision. more specifically we propose a skip thought neighbor model to consider the adjacent sentences as a neighborhood. we train our skip thought neighbor model on a large corpus with continuous sentences and then evaluate the trained model on 7 tasks which include semantic relatedness paraphrase detection and classification benchmarks. both quantitative comparison and qualitative investigation are conducted. we empirically show that our skip thought neighbor model performs as well as the skip thought model on evaluation tasks. in addition we found that incorporating an autoencoder path in our model didn t aid our model to perform better while it hurts the performance of the skip thought model.
neural domain adaptation for biomedical question answering
factoid question answering qa has recently benefited from the development of deep learning dl systems. neural network models outperform traditional approaches in domains where large datasets exist such as squad ca. 100 000 questions for wikipedia articles. however these systems have not yet been applied to qa in more specific domains such as biomedicine because datasets are generally too small to train a dl system from scratch. for example the bioasq dataset for biomedical qa comprises less then 900 factoid single answer and list multiple answers qa instances. in this work we adapt a neural qa system trained on a large open domain dataset squad source to a biomedical dataset bioasq target by employing various transfer learning techniques. our network architecture is based on a state of the art qa system extended with biomedical word embeddings and a novel mechanism to answer list questions. in contrast to existing biomedical qa systems our system does not rely on domain specific ontologies parsers or entity taggers which are expensive to create. despite this fact our systems achieve state of the art results on factoid questions and competitive results on list questions.
neural models for key phrase detection and question generation
we propose a two stage neural model to tackle question generation from documents. our model first estimates the probability that word sequences in a document compose interesting answers using a neural model trained on a question answering corpus. we thus take a data driven approach to interestingness. predicted key phrases then act as target answers that condition a sequence to sequence question generation model with a copy mechanism. empirically our neural key phrase detection model significantly outperforms an entity tagging baseline system and existing rule based approaches. we demonstrate that the question generator formulates good quality natural language questions from extracted key phrases and a human study indicates that our system s generated question answer pairs are competitive with those of an earlier approach. we foresee our system being used in an educational setting to assess reading comprehension and also as a data augmentation technique for semi supervised learning.
neural question answering at bioasq 5b
this paper describes our submission to the 2017 bioasq challenge. we participated in task b phase b which is concerned with biomedical question answering qa . we focus on factoid and list question using an extractive qa model that is we restrict our system to output substrings of the provided text snippets. at the core of our system we use fastqa a state of the art neural qa system. we extended it with biomedical word embeddings and changed its answer layer to be able to answer list questions in addition to factoid questions. we pre trained the model on a large scale open domain qa dataset squad and then fine tuned the parameters on the bioasq training set. with our approach we achieve state of the art results on factoid questions and competitive results on list questions.
a deep network with visual text composition behavior
while natural languages are compositional how state of the art neural models achieve compositionality is still unclear. we propose a deep network which not only achieves competitive accuracy for text classification but also exhibits compositional behavior. that is while creating hierarchical representations of a piece of text such as a sentence the lower layers of the network distribute their layer specific attention weights to individual words. in contrast the higher layers compose meaningful phrases and clauses whose lengths increase as the networks get deeper until fully composing the sentence.
semi supervised emotion lexicon expansion with label propagation and specialized word embeddings
there exist two main approaches to automatically extract affective orientation lexicon based and corpus based. in this work we argue that these two methods are compatible and show that combining them can improve the accuracy of emotion classifiers. in particular we introduce a novel variant of the label propagation algorithm that is tailored to distributed word representations we apply batch gradient descent to accelerate the optimization of label propagation and to make the optimization feasible for large graphs and we propose a reproducible method for emotion lexicon expansion. we conclude that label propagation can expand an emotion lexicon in a meaningful way and that the expanded emotion lexicon can be leveraged to improve the accuracy of an emotion classifier.
modelling protagonist goals and desires in first person narrative
many genres of natural language text are narratively structured a testament to our predilection for organizing our experiences as narratives. there is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes. however to date there has been limited work on computational models for this problem. we introduce a new dataset desiredb which includes gold standard labels for identifying statements of desire textual evidence for desire fulfillment and annotations for whether the stated desire is fulfilled given the evidence in the narrative context. we report experiments on tracking desire fulfillment using different methods and show that lstm skip thought model achieves f measure of 0.7 on our corpus.
understanding grounded language learning agents
neural network based systems can now learn to locate the referents of words and phrases in images answer questions about visual scenes and even execute symbolic instructions as first person actors in partially observable worlds. to achieve this so called grounded language learning models must overcome certain well studied learning challenges that are also fundamental to infants learning their first words. while it is notable that models with no meaningful prior knowledge overcome these learning obstacles ai researchers and practitioners currently lack a clear understanding of exactly how they do so. here we address this question as a way of achieving a clearer general understanding of grounded language learning both to inform future research and to improve confidence in model predictions. for maximum control and generality we focus on a simple neural network based language learning agent trained via policy gradient methods to interpret synthetic linguistic instructions in a simulated 3d world. we apply experimental paradigms from developmental psychology to this agent exploring the conditions under which established human biases and learning effects emerge. we further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.
just ask building an architecture for extensible self service spoken language understanding
this paper presents the design of the machine learning architecture that underlies the alexa skills kit ask a large scale spoken language understanding slu software development kit sdk that enables developers to extend the capabilities of amazon s virtual assistant alexa. at amazon the infrastructure powers over 25 000 skills deployed through the ask as well as aws s amazon lex slu service. the ask emphasizes flexibility predictability and a rapid iteration cycle for third party developers. it imposes inductive biases that allow it to learn robust slu models from extremely small and sparse datasets and in doing so removes significant barriers to entry for software developers and dialogue systems researchers.
the narrativeqa reading comprehension challenge
reading comprehension rc in contrast to information retrieval requires integrating information and reasoning about events entities and their relations across a full document. question answering is conventionally used to assess rc ability in both artificial agents and children learning to read. however existing rc datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information e.g. local context similarity or global term frequency they thus fail to test for the essential integrative aspect of rc. to encourage progress on deeper comprehension of language we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. these tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. we show that although humans solve the tasks easily standard rc models struggle on the tasks presented here. we provide an analysis of the dataset and the challenges it presents.
cognitive database a step towards endowing relational databases with artificial intelligence capabilities
we propose cognitive databases an approach for transparently enabling artificial intelligence ai capabilities in relational databases. a novel aspect of our design is to first view the structured data source as meaningful unstructured text and then use the text to build an unsupervised neural network model using a natural language processing nlp technique called word embedding. this model captures the hidden inter intra column relationships between database tokens of different types. for each database token the model includes a vector that encodes contextual semantic relationships. we seamlessly integrate the word embedding model into existing sql query infrastructure and use it to enable a new class of sql based analytics queries called cognitive intelligence ci queries. ci queries use the model vectors to enable complex queries such as semantic matching inductive reasoning queries such as analogies predictive queries using entities not present in a database and more generally using knowledge from external sources. we demonstrate unique capabilities of cognitive databases using an apache spark based prototype to execute inductive reasoning ci queries over a multi modal database containing text and images. we believe our first of a kind system exemplifies using ai functionality to endow relational databases with capabilities that were previously very hard to realize in practice.
feudal reinforcement learning for dialogue management in large domains
reinforcement learning rl is a promising approach to solve dialogue policy optimisation. traditional rl algorithms however fail to scale to large domains due to the curse of dimensionality. we propose a novel dialogue management architecture based on feudal rl which decomposes the decision into two steps a first step where a master policy selects a subset of primitive actions and a second step where a primitive action is chosen from the selected subset. the structural information included in the domain ontology is used to abstract the dialogue state space taking the decisions at each step using different parts of the abstracted state. this combined with an information sharing mechanism between slots increases the scalability to large domains. we show that an implementation of this approach based on deep q networks significantly outperforms previous state of the art in several dialogue domains and environments without the need of any additional reward signal.
an analysis of neural language modeling at multiple scales
many of the leading approaches in language modeling introduce novel complex and specialized architectures. we take existing state of the art word level language models based on lstms and qrnns and extend them to both larger vocabularies as well as character level granularity. when properly tuned lstms and qrnns achieve state of the art results on character level penn treebank enwik8 and word level wikitext 103 datasets respectively. results are obtained in only 12 hours wikitext 103 to 2 days enwik8 using a single modern gpu.
spatial diffuseness features for dnn based speech recognition in noisy and reverberant environments
we propose a spatial diffuseness feature for deep neural network dnn based automatic speech recognition to improve recognition accuracy in reverberant and noisy environments. the feature is computed in real time from multiple microphone signals without requiring knowledge or estimation of the direction of arrival and represents the relative amount of diffuse noise in each time and frequency bin. it is shown that using the diffuseness feature as an additional input to a dnn based acoustic model leads to a reduced word error rate for the reverb challenge corpus both compared to logmelspec features extracted from noisy signals and features enhanced by spectral subtraction.
character aware neural language models
we describe a simple neural language model that relies only on character level inputs. predictions are still made at the word level. our model employs a convolutional neural network cnn and a highway network over characters whose output is given to a long short term memory lstm recurrent neural network language model rnn lm . on the english penn treebank the model is on par with the existing state of the art despite having 60 fewer parameters. on languages with rich morphology arabic czech french german spanish russian the model outperforms word level morpheme level lstm baselines again with fewer parameters. the results suggest that on many languages character inputs are sufficient for language modeling. analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode from characters only both semantic and orthographic information.
neural based machine translation for medical text domain. based on european medicines agency leaflet texts
the quality of machine translation is rapidly evolving. today one can find several machine translation systems on the web that provide reasonable translations although the systems are not perfect. in some specific domains the quality may decrease. a recently proposed approach to this domain is neural machine translation. it aims at building a jointly tuned single neural network that maximizes translation performance a very different approach from traditional statistical machine translation. recently proposed neural machine translation models often belong to the encoder decoder family in which a source sentence is encoded into a fixed length vector that is in turn decoded to generate a translation. the present research examines the effects of different training methods on a polish english machine translation system used for medical data. the european medicines agency parallel text corpus was used as the basis for training of neural and statistical network based translation systems. the main machine translation evaluation metrics have also been used in analysis of the systems. a comparison and implementation of a real time medical translator is the main focus of our experiments.
conditional generation and snapshot learning in neural dialogue systems
recently a variety of lstm based conditional language models lm have been applied across a range of language generation tasks. in this work we study various model architectures and different ways to represent and aggregate the source information in an end to end neural dialogue system framework. a method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross entropy objective function to the conditioning vector. the experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the lm and the differing architectures provide different trade offs between the two. secondly the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. thirdly snapshot learning leads to consistent performance improvements independent of which architecture is used.
dialog state tracking a machine reading approach using memory network
in an end to end dialog system the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules. this paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an end to end memory network memn2n a memory enhanced neural network architecture. we evaluate the proposed approach on the second dialog state tracking challenge dstc 2 dataset. the corpus has been converted for the occasion in order to frame the hidden state variable inference as a question answering task based on a sequence of utterances extracted from a dialog. we show that the proposed tracker gives encouraging results. then we propose to extend the dstc 2 dataset with specific reasoning capabilities requirement like counting list maintenance yes no question answering and indefinite knowledge management. finally we present encouraging results using our proposed memn2n based tracking model.
a physical metaphor to study semantic drift
in accessibility tests for digital preservation over time we experience drifts of localized and labelled content in statistical models of evolving semantics represented as a vector field. this articulates the need to detect measure interpret and model outcomes of knowledge dynamics. to this end we employ a high performance machine learning algorithm for the training of extremely large emergent self organizing maps for exploratory data analysis. the working hypothesis we present here is that the dynamics of semantic drifts can be modeled on a relaxed version of newtonian mechanics called social mechanics. by using term distances as a measure of semantic relatedness vs. their pagerank values indicating social importance and applied as variable term mass gravitation as a metaphor to express changes in the semantic content of a vector field lends a new perspective for experimentation. from term gravitation over time one can compute its generating potential whose fluctuations manifest modifications in pairwise term similarity vs. social importance thereby updating osgood s semantic differential. the dataset examined is the public catalog metadata of tate galleries london.
optimizing neural network hyperparameters with gaussian processes for dialog act classification
systems based on artificial neural networks anns have achieved state of the art results in many natural language processing tasks. although anns do not require manually engineered features anns have many hyperparameters to be optimized. the choice of hyperparameters significantly impacts models performances. however the ann hyperparameters are typically chosen by manual grid or random search which either requires expert experiences or is computationally expensive. recent approaches based on bayesian optimization using gaussian processes gps is a more systematic way to automatically pinpoint optimal or near optimal machine learning hyperparameters. using a previously published ann model yielding state of the art results for dialog act classification we demonstrate that optimizing hyperparameters using gp further improves the results and reduces the computational time by a factor of 4 compared to a random search. therefore it is a useful technique for tuning ann models to yield the best performances for natural language processing tasks.
a survey of voice translation methodologies acoustic dialect decoder
speech translation has always been about giving source text or audio input and waiting for system to give translated output in desired form. in this paper we present the acoustic dialect decoder add a voice to voice ear piece translation device. we introduce and survey the recent advances made in the field of speech engineering to employ in the add particularly focusing on the three major processing steps of recognition translation and synthesis. we tackle the problem of machine understanding of natural language by designing a recognition unit for source audio to text a translation unit for source language text to target language text and a synthesis unit for target language text to target language speech. speech from the surroundings will be recorded by the recognition unit present on the ear piece and translation will start as soon as one sentence is successfully read. this way we hope to give translated output as and when input is being read. the recognition unit will use hidden markov models hmms based tool kit htk hybrid rnn systems with gated memory cells and the synthesis unit hmm based speech synthesis system hts. this system will initially be built as an english to tamil translation device.
learning to reason with adaptive computation
multi hop inference is necessary for machine learning systems to successfully solve tasks such as recognising textual entailment and machine reading. in this work we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. we introduce the first model involving adaptive computation time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model.
feature augmented neural networks for patient note de identification
patient notes contain a wealth of information of potentially great interest to medical investigators. however to protect patients privacy protected health information phi must be removed from the patient notes before they can be legally released a process known as patient note de identification. the main objective for a de identification system is to have the highest possible recall. recently the first neural network based de identification system has been proposed yielding state of the art results. unlike other systems it does not rely on human engineered features which allows it to be quickly deployed but does not leverage knowledge from human experts or from electronic health records ehrs . in this work we explore a method to incorporate human engineered features as well as features derived from ehrs to a neural network based de identification system. our results show that the addition of features especially the ehr derived features further improves the state of the art in patient note de identification including for some of the most sensitive phi types such as patient names. since in a real life setting patient notes typically come with ehrs we recommend developers of de identification systems to leverage the information ehrs contain.
direct acoustics to word models for english conversational speech recognition
recent work on end to end automatic speech recognition asr has shown that the connectionist temporal classification ctc loss can be used to convert acoustics to phone or character sequences. such systems are used with a dictionary and separately trained language model lm to produce word sequences. however they are not truly end to end in the sense of mapping acoustics directly to words without an intermediate phone representation. in this paper we present the first results employing direct acoustics to word ctc models on two well known public benchmark tasks switchboard and callhome. these models do not require an lm or even a decoder at run time and hence recognize speech with minimal complexity. however due to the large number of word output units ctc word models require orders of magnitude more data to train reliably compared to traditional systems. we present some techniques to mitigate this issue. our ctc word model achieves a word error rate of 13.0 18.8 on the hub5 2000 switchboard callhome test sets without any lm or decoder compared with 9.6 16.0 for phone based ctc with a 4 gram lm. we also present rescoring results on ctc word model lattices to quantify the performance benefits of a lm and contrast the performance of word and phone ctc models.
factorization tricks for lstm networks
we present two simple ways of reducing the number of parameters and accelerating the training of large long short term memory lstm networks the first one is matrix factorization by design of lstm matrix into the product of two smaller matrices and the second one is partitioning of lstm matrix its inputs and states into the independent groups. both approaches allow us to train large lstm networks significantly faster to the near state of the art perplexity while using significantly less rnn parameters.
neuroner an easy to use program for named entity recognition based on neural networks
named entity recognition ner aims at identifying entities of interest in a text. artificial neural networks anns have recently been shown to outperform existing ner systems. however anns remain challenging to use for non expert users. in this paper we present neuroner an easy to use named entity recognition tool based on anns. users can annotate entities using a graphical web based user interface brat the annotations are then used to train an ann which in turn predict entities locations and categories in new texts. neuroner makes this annotation training prediction flow smooth and accessible to anyone.
syllable aware neural language models a failure to beat character aware ones
syllabification does not seem to improve word level rnn language modeling quality when compared to character based segmentation. however our best syllable aware language model achieving performance comparable to the competitive character aware model has 18 33 fewer parameters and is trained 1.2 2.2 times faster.
a benchmarking environment for reinforcement learning based task oriented dialogue management
dialogue assistants are rapidly becoming an indispensable daily aid. to avoid the significant effort needed to hand craft the required dialogue flow the dialogue management dm module can be cast as a continuous markov decision process mdp and trained through reinforcement learning rl . several rl models have been investigated over recent years. however the lack of a common benchmarking framework makes it difficult to perform a fair comparison between different models and their capability to generalise to different environments. therefore this paper proposes a set of challenging simulated environments for dialogue model development and evaluation. to provide some baselines we investigate a number of representative parametric algorithms namely deep reinforcement learning algorithms dqn a2c and natural actor critic and compare them to a non parametric model gp sarsa. both the environments and policy models are implemented using the publicly available pydial toolkit and released on line in order to establish a testbed framework for further experiments and to facilitate experimental reproducibility.
reusing weights in subword aware neural language models
we propose several ways of reusing subword embeddings and other weights in subword aware neural language models. the proposed techniques do not benefit a competitive character aware model but some of them improve the performance of syllable and morpheme aware models while showing significant reductions in model sizes. we discover a simple hands on principle in a multi layer input embedding model layers should be tied consecutively bottom up if reused at output. our best morpheme aware model with properly reused weights beats the competitive word level model by a large margin across multiple languages and has 20 87 fewer parameters.
multi task learning of pairwise sequence classification tasks over disparate label spaces
we combine multi task learning and semi supervised learning by inducing a joint embedding space between disparate label spaces and learning transfer functions between label embeddings enabling us to jointly leverage unlabelled data and auxiliary annotated datasets. we evaluate our approach on a variety of sequence classification tasks with disparate label spaces. we outperform strong single and multi task baselines and achieve a new state of the art for aspect and topic based sentiment analysis.
dynamic memory networks for visual and textual question answering
neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. one such architecture the dynamic memory network dmn obtained high accuracy on a variety of language tasks. however it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. based on an analysis of the dmn we propose several improvements to its memory and input modules. together with these changes we introduce a novel input module for images in order to be able to answer visual questions. our new dmn model improves the state of the art on both the visual question answering dataset and the babi 10k text question answering dataset without supporting fact supervision.
picture it in your mind generating high level visual representations from textual descriptions
in this paper we tackle the problem of image search when the query is a short textual description of the image the user is looking for. we choose to implement the actual search process as a similarity search in a visual feature space by learning to translate a textual query into a visual representation. searching in the visual feature space has the advantage that any update to the translation model does not require to reprocess the typically huge image collection on which the search is performed. we propose text2vis a neural network that generates a visual representation in the visual feature space of the fc6 fc7 layers of imagenet from a short descriptive text. text2vis optimizes two loss functions using a stochastic loss selection method. a visual focused loss is aimed at learning the actual text to visual feature mapping while a text focused loss is aimed at modeling the higher level semantic concepts expressed in language and countering the overfit on non relevant visual components of the visual loss. we report preliminary results on the ms coco dataset.
where to put the image in an image caption generator
when a recurrent neural network language model is used for caption generation the image information can be fed to the neural network either by directly incorporating it in the rnn conditioning the language model by injecting image features or in a layer following the rnn conditioning the language model by merging image features. while both options are attested in the literature there is as yet no systematic comparison between the two. in this paper we empirically show that it is not especially detrimental to performance whether one architecture is used or another. the merge architecture does have practical advantages as conditioning by merging allows the rnn s hidden state vector to shrink in size by up to four times. our results suggest that the visual and linguistic modalities for caption generation need not be jointly encoded by the rnn as that yields large memory intensive models with few tangible advantages in performance rather the multimodal integration should be delayed to a subsequent stage.
a focused dynamic attention model for visual question answering
visual question and answering vqa problems are attracting increasing interest from multiple research disciplines. solving vqa problems requires techniques from both computer vision for understanding the visual contents of a presented image or video as well as the ones from natural language processing for understanding semantics of the question and generating the answers. regarding visual content modeling most of existing vqa methods adopt the strategy of extracting global features from the image or video which inevitably fails in capturing fine grained information such as spatial configuration of multiple objects. extracting features from auto generated regions as some region based image recognition methods do cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question. in this work we propose a novel focused dynamic attention fda model to provide better aligned image content representation with proposed questions. being aware of the key words in the question fda employs off the shelf object detector to identify important regions and fuse the information from the regions and global features via an lstm unit. such question driven representations are then combined with question representation and fed into a reasoning unit for generating the answers. extensive evaluation on a large scale benchmark dataset vqa clearly demonstrate the superior performance of fda over well established baselines.
simple image description generator via a linear phrase based approach
generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. in this paper we present a simple model that is able to generate descriptive sentences given a sample image. this model has a strong focus on the syntax of the descriptions. we train a purely bilinear model that learns a metric between an image representation generated from a previously trained convolutional neural network and phrases that are used to described them. the system is then able to infer phrases from a given image sample. based on caption syntax statistics we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. our approach which is considerably simpler than state of the art models achieves comparable results on the recently release microsoft coco dataset.
multimodal convolutional neural networks for matching image and sentence
in this paper we propose multimodal convolutional neural networks m cnns for matching image and sentence. our m cnn provides an end to end framework with convolutional architectures to exploit image representation word composition and the matching relations between the two modalities. more specifically it consists of one image cnn encoding the image content and one matching cnn learning the joint representation of image and sentence. the matching cnn composes words to different semantic fragments and learns the inter modal relations between image and the composed fragments at different levels thus fully exploit the matching relations between image and sentence. experimental results on benchmark databases of bidirectional image and sentence retrieval demonstrate that the proposed m cnns can effectively capture the information necessary for image and sentence matching. specifically our proposed m cnns for bidirectional image and sentence retrieval on flickr30k and microsoft coco databases achieve the state of the art performances.
learning to compose neural networks for question answering
we describe a question answering model that applies to both images and structured knowledge bases. the model uses natural language strings to automatically assemble neural networks from a collection of composable modules. parameters for these modules are learned jointly with network assembly parameters via reinforcement learning with only world question answer triples as supervision. our approach which we term a dynamic neural model network achieves state of the art results on benchmark datasets in both visual and structured domains.
signer independent fingerspelling recognition with deep neural network adaptation
we study the problem of recognition of fingerspelled letter sequences in american sign language in a signer independent setting. fingerspelled sequences are both challenging and important to recognize as they are used for many content words such as proper nouns and technical terms. previous work has shown that it is possible to achieve almost 90 accuracies on fingerspelling recognition in a signer dependent setting. however the more realistic signer independent setting presents challenges due to significant variations among signers coupled with the dearth of available training data. we investigate this problem with approaches inspired by automatic speech recognition. we start with the best performing approaches from prior work based on tandem models and segmental conditional random fields scrfs with features based on deep neural network dnn classifiers of letters and phonological features. using dnn adaptation we find that it is possible to bridge a large part of the gap between signer dependent and signer independent performance. using only about 115 transcribed words for adaptation from the target signer we obtain letter accuracies of up to 82.7 with frame level adaptation labels and 69.7 with only word labels.
full network embedding in a multimodal embedding pipeline
the current state of the art for image annotation and image retrieval tasks is obtained through deep neural networks which combine an image representation and a text representation into a shared embedding space. in this paper we evaluate the impact of using the full network embedding in this setting replacing the original image representation in a competitive multimodal embedding generation scheme. unlike the one layer image embeddings typically used by most approaches the full network embedding provides a multi scale representation of images which results in richer characterizations. to measure the influence of the full network embedding we evaluate its performance on three different datasets and compare the results with the original multimodal embedding generation scheme when using a one layer image embedding and with the rest of the state of the art. results for image annotation and image retrieval tasks indicate that the full network embedding is consistently superior to the one layer embedding. these results motivate the integration of the full network embedding on any multimodal embedding generation scheme something feasible thanks to the flexibility of the approach.
what is the role of recurrent neural networks rnns in an image caption generator 
in neural image captioning systems a recurrent neural network rnn is typically viewed as the primary generation component. this view suggests that the image features should be injected into the rnn. this is in fact the dominant view in the literature. alternatively the rnn can instead be viewed as only encoding the previously generated words. this view suggests that the rnn should only be used to encode linguistic features and that only the final representation should be merged with the image features at a later stage. this paper compares these two architectures. we find that in general late merging outperforms injection suggesting that rnns are better viewed as encoders rather than generators.
a fixed size encoding method for variable length sequences with its application to neural network language models
in this paper we propose the new fixed size ordinally forgetting encoding fofe method which can almost uniquely encode any variable length sequence of words into a fixed size representation. fofe can model the word order in a sequence using a simple ordinally forgetting mechanism according to the positions of words. in this work we have applied fofe to feedforward neural network language models fnn lms . experimental results have shown that without using any recurrent feedbacks fofe based fnn lms can significantly outperform not only the standard fixed input fnn lms but also the popular rnn lms.
transition based dependency parsing with stack long short term memory
we propose a technique for learning representations of parser states in transition based dependency parsers. our primary innovation is a new control structure for sequence to sequence neural networks the stack lstm. like the conventional stack data structures used in transition based parsing elements can be pushed to or popped from the top of the stack in constant time but in addition an lstm maintains a continuous space embedding of the stack contents. this lets us formulate an efficient parsing model that captures three facets of a parser s state i unbounded look ahead into the buffer of incoming words ii the complete history of actions taken by the parser and iii the complete contents of the stack of partially built tree fragments including their internal structures. standard backpropagation techniques are used for training and yield state of the art parsing performance.
a semisupervised approach for language identification based on ladder networks
in this study we address the problem of training a neuralnetwork for language identification using both labeled and unlabeled speech samples in the form of i vectors. we propose a neural network architecture that can also handle out of set languages. we utilize a modified version of the recently proposed ladder network semisupervised training procedure that optimizes the reconstruction costs of a stack of denoising autoencoders. we show that this approach can be successfully applied to the case where the training dataset is composed of both labeled and unlabeled acoustic data. the results show enhanced language identification on the nist 2015 language identification dataset.
first pass large vocabulary continuous speech recognition using bi directional recurrent dnns
we present a method to perform first pass large vocabulary continuous speech recognition using only a neural network and language model. deep neural network acoustic models are now commonplace in hmm based speech recognition systems but building such systems is a complex domain specific task. recent work demonstrated the feasibility of discarding the hmm sequence modeling framework by directly predicting transcript text from audio. this paper extends this approach in two ways. first we demonstrate that a straightforward recurrent neural network architecture can achieve a high level of accuracy. second we propose and evaluate a modified prefix search decoding algorithm. this approach to decoding enables first pass speech recognition with a language model completely unaided by the cumbersome infrastructure of hmm based systems. experiments on the wall street journal corpus demonstrate fairly competitive word error rates and the importance of bi directional network recurrence.
applying deep learning techniques on medical corpora from the world wide web a prototypical system and evaluation
background the amount of biomedical literature is rapidly growing and it is becoming increasingly difficult to keep manually curated knowledge bases and ontologies up to date. in this study we applied the word2vec deep learning toolkit to medical corpora to test its potential for identifying relationships from unstructured text. we evaluated the efficiency of word2vec in identifying properties of pharmaceuticals based on mid sized unstructured medical text corpora available on the web. properties included relationships to diseases may treat or physiological processes has physiological effect . we compared the relationships identified by word2vec with manually curated information from the national drug file reference terminology ndf rt ontology as a gold standard. results our results revealed a maximum accuracy of 49.28 which suggests a limited ability of word2vec to capture linguistic regularities on the collected medical corpora compared with other published results. we were able to document the influence of different parameter settings on result accuracy and found and unexpected trade off between ranking quality and accuracy. pre processing corpora to reduce syntactic variability proved to be a good strategy for increasing the utility of the trained vector models. conclusions word2vec is a very efficient implementation for computing vector representations and for its ability to identify relationships in textual data without any prior domain knowledge. we found that the ranking and retrieved results generated by word2vec were not of sufficient quality for automatic population of knowledge bases and ontologies but could serve as a starting point for further manual curation.
syntax based deep matching of short texts
many tasks in natural language processing ranging from machine translation to question answering can be reduced to the problem of matching two sentences or more generally two short texts. we propose a new approach to the problem called deep match tree deepmatch tree under a general setting. the approach consists of two components 1 a mining algorithm to discover patterns for matching two short texts defined in the product space of dependency trees and 2 a deep neural network for matching short texts using the mined patterns as well as a learning algorithm to build the network having a sparse structure. we test our algorithm on the problem of matching a tweet and a response in social media a hard matching problem proposed in wang et al. 2013 and show that deepmatch tree can outperform a number of competitor models including one without using dependency trees and one based on word embedding all with large margins
ensemble of generative and discriminative techniques for sentiment analysis of movie reviews
sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document typically a consumer review . in the simplest settings we discriminate only between positive and negative sentiment turning the task into a standard binary classification problem. we compare several ma chine learning approaches to this problem and combine them to achieve the best possible results. we show how to use for this task the standard generative lan guage models which are slightly complementary to the state of the art techniques. we achieve strong results on a well known dataset of imdb movie reviews. our results are easily reproducible as we publish also the code needed to repeat the experiments. this should simplify further advance of the state of the art as other researchers can combine their techniques with ours with little effort.
diverse embedding neural network language models
we propose diverse embedding neural network denn a novel architecture for language models lms . a dennlm projects the input word history vector onto multiple diverse low dimensional sub spaces instead of a single higher dimensional sub space as in conventional feed forward neural network lms. we encourage these sub spaces to be diverse during network training through an augmented loss function. our language modeling experiments on the penn treebank data set show the performance benefit of using a dennlm.
learning linearly separable features for speech recognition using convolutional neural networks
automatic speech recognition systems usually rely on spectral based features such as mfcc of plp. these features are extracted based on prior knowledge such as speech perception or and speech production. recently convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data driven manner i.e. using directly temporal raw speech signal as input. this system was shown to yield similar or better performance than hmm ann based system on phoneme recognition task and on large scale continuous speech recognition task using less parameters. motivated by these studies we investigate the use of simple linear classifier in the cnn based framework. thus the network learns linearly separable features from raw speech. we show that such system yields similar or better performance than mlp based system using cepstral based features as input.
learning to transduce with unbounded memory
recently strong results have been demonstrated by deep recurrent neural networks on natural language transduction problems. in this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. these experiments lead us to propose new memory based recurrent networks that implement continuously differentiable analogues of traditional data structures such as stacks queues and deques. we show that these architectures exhibit superior generalisation performance to deep rnns and are often able to learn the underlying generating algorithms in our transduction experiments.
feedforward sequential memory neural networks without recurrent feedback
we introduce a new structure for memory neural networks called feedforward sequential memory networks fsmn which can learn long term dependency without using recurrent feedback. the proposed fsmn is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. in this work we have applied fsmn to several language modeling lm tasks. experimental results have shown that the memory blocks in fsmn can learn effective representations of long history. experiments have shown that fsmn based language models can significantly outperform not only feedforward neural network fnn based lms but also the popular recurrent neural network rnn lms.
towards structured deep neural network for automatic speech recognition
in this paper we propose the structured deep neural network structured dnn as a structured and deep learning framework. this approach can learn to find the best structured object such as a label sequence given a structured input such as a vector sequence by globally considering the mapping relationships between the structures rather than item by item. when automatic speech recognition is viewed as a special case of such a structured learning problem where we have the acoustic vector sequence as the input and the phoneme label sequence as the output it becomes possible to comprehensively learn utterance by utterance as a whole rather than frame by frame. structured support vector machine structured svm was proposed to perform asr with structured learning previously but limited by the linear nature of svm. here we propose structured dnn to use nonlinear transformations in multi layers as a structured and deep learning approach. this approach was shown to beat structured svm in preliminary experiments on timit.
character level incremental speech recognition with recurrent neural networks
in real time speech recognition applications the latency is an important issue. we have developed a character level incremental speech recognition isr system that responds quickly even during the speech where the hypotheses are gradually improved while the speaking proceeds. the algorithm employs a speech to character unidirectional recurrent neural network rnn which is end to end trained with connectionist temporal classification ctc and an rnn based character level language model lm . the output values of the ctc trained rnn are character level probabilities which are processed by beam search decoding. the rnn lm augments the decoding by providing long term dependency information. we propose tree based online beam search with additional depth pruning which enables the system to process infinitely long input speech with low latency. this system not only responds quickly on speech but also can dictate out of vocabulary oov words according to pronunciation. the proposed model achieves the word error rate wer of 8.90 on the wall street journal wsj nov 92 20k evaluation set when trained on the wsj si 284 training set.
globally normalized transition based neural networks
we introduce a globally normalized transition based neural network model that achieves state of the art part of speech tagging dependency parsing and sentence compression results. our model is a simple feed forward neural network that operates on a task specific transition system yet achieves comparable or better accuracies than recurrent models. we discuss the importance of global as opposed to local normalization a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.
clinical information extraction via convolutional neural network
we report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. our approach uses context words and their part of speech tags and shape information as features. then we hire temporal 1d convolutional neural network to learn hidden feature representations. finally we use multilayer perceptron mlp to predict event spans. the empirical evaluation demonstrates that our approach significantly outperforms baselines.
zoneout regularizing rnns by randomly preserving hidden activations
we propose zoneout a novel method for regularizing rnns. at each timestep zoneout stochastically forces some hidden units to maintain their previous values. like dropout zoneout uses random noise to train a pseudo ensemble improving generalization. but by preserving instead of dropping hidden units gradient information and state information are more readily propagated through time as in feedforward stochastic depth networks. we perform an empirical investigation of various rnn regularizers and find that zoneout gives significant performance improvements across tasks. we achieve competitive results with relatively simple models in character and word level language modelling on the penn treebank and text8 datasets and combining with recurrent batch normalization yields state of the art results on permuted sequential mnist.
stance detection with bidirectional conditional encoding
stance detection is the task of classifying the attitude expressed in a text towards a target such as hillary clinton to be positive negative or neutral . previous work has assumed that either the target is mentioned in the text or that training data for every target is given. this paper considers the more challenging version of this task where targets are not always mentioned and no training data is available for the test targets. we experiment with conditional lstm encoding which builds a representation of the tweet that is dependent on the target and demonstrate that it outperforms encoding the tweet and the target independently. performance is improved further when the conditional model is augmented with bidirectional encoding. we evaluate our approach on the semeval 2016 task 6 twitter stance detection corpus achieving performance second best only to a system trained on semi automatically labelled tweets for the test target. when such weak supervision is added our approach achieves state of the art results.
sms spam filtering using probabilistic topic modelling and stacked denoising autoencoder
in this paper we present a novel approach to spam filtering and demonstrate its applicability with respect to sms messages. our approach requires minimum features engineering and a small set of la belled data samples. features are extracted using topic modelling based on latent dirichlet allocation and then a comprehensive data model is created using a stacked denoising autoencoder sda . topic modelling summarises the data providing ease of use and high interpretability by visualising the topics using word clouds. given that the sms messages can be regarded as either spam unwanted or ham wanted the sda is able to model the messages and accurately discriminate between the two classes without the need for a pre labelled training set. the results are compared against the state of the art spam detection algorithms with our proposed approach achieving over 97 accuracy which compares favourably to the best reported algorithms presented in the literature.
bidirectional recurrent neural networks for medical event detection in electronic health records
sequence labeling for extraction of medical events and their attributes from unstructured text in electronic health record ehr notes is a key step towards semantic understanding of ehrs. it has important applications in health informatics including pharmacovigilance and drug surveillance. the state of the art supervised machine learning models in this domain are based on conditional random fields crfs with features calculated from fixed context windows. in this application we explored various recurrent neural network frameworks and show that they significantly outperformed the crf models.
sequence training and adaptation of highway deep neural networks
highway deep neural network hdnn is a type of depth gated feedforward neural network which has shown to be easier to train with more hidden layers and also generalise better compared to conventional plain deep neural networks dnns . previously we investigated a structured hdnn architecture for speech recognition in which the two gate functions were tied across all the hidden layers and we were able to train a much smaller model without sacrificing the recognition accuracy. in this paper we carry on the study of this architecture with sequence discriminative training criterion and speaker adaptation techniques on the ami meeting speech recognition corpus. we show that these two techniques improve speech recognition accuracy on top of the model trained with the cross entropy criterion. furthermore we demonstrate that the two gate functions that are tied across all the hidden layers are able to control the information flow over the whole network and we can achieve considerable improvements by only updating these gate functions in both sequence training and adaptation experiments.
recurrent highway networks
many sequential processing tasks require complex nonlinear transition functions from one step to the next. however recurrent neural networks with deep transition functions remain difficult to train even when using long short term memory lstm networks. we introduce a novel theoretical analysis of recurrent networks based on gersgorin s circle theorem that illuminates several modeling and optimization issues and improves our understanding of the lstm cell. based on this analysis we propose recurrent highway networks which extend the lstm architecture to allow step to step transition depths larger than one. several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. on the penn treebank corpus solely increasing the transition depth from 1 to 10 improves word level perplexity from 90.6 to 65.4 using the same number of parameters. on the larger wikipedia datasets for character prediction text8 and enwik8 rhns outperform all previous results and achieve an entropy of 1.27 bits per character.
towards cross lingual distributed representations without parallel text trained with adversarial autoencoders
current approaches to learning vector representations of text that are compatible between different languages usually require some amount of parallel text aligned at word sentence or at least document level. we hypothesize however that different natural languages share enough semantic structure that it should be possible in principle to learn compatible vector representations just by analyzing the monolingual distribution of words. in order to evaluate this hypothesis we propose a scheme to map word vectors trained on a source language to vectors semantically compatible with word vectors trained on a target language using an adversarial autoencoder. we present preliminary qualitative results and discuss possible future developments of this technique such as applications to cross lingual sentence representations.
memory visualization for gated recurrent neural networks in speech recognition
recurrent neural networks rnns have shown clear superiority in sequence modeling particularly the ones with gated units such as long short term memory lstm and gated recurrent unit gru . however the dynamic properties behind the remarkable performance remain unclear in many applications e.g. automatic speech recognition asr . this paper employs visualization techniques to study the behavior of lstm and gru when performing speech recognition tasks. our experiments show some interesting patterns in the gated memory and some of them have inspired simple yet effective modifications on the network structure. we report two of such modifications 1 lazy cell update in lstm and 2 shortcut connections for residual learning. both modifications lead to more comprehensible and powerful networks.
neural speech recognizer acoustic to word lstm model for large vocabulary speech recognition
we present results that show it is possible to build a competitive greatly simplified large vocabulary continuous speech recognition system with whole words as acoustic units. we model the output vocabulary of about 100 000 words directly using deep bi directional lstm rnns with ctc loss. the model is trained on 125 000 hours of semi supervised acoustic training data which enables us to alleviate the data sparsity problem for word models. we show that the ctc word models work very well as an end to end all neural speech recognition model without the use of traditional context dependent sub word phone units that require a pronunciation lexicon and without any language model removing the need to decode. we demonstrate that the ctc word models perform better than a strong more complex state of the art baseline with sub word units.
unsupervised pretraining for sequence to sequence learning
this work presents a general unsupervised learning method to improve the accuracy of sequence to sequence seq2seq models. in our method the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine tuned with labeled data. we apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. our main result is that pretraining improves the generalization of seq2seq models. we achieve state of the art results on the wmt english rightarrow german task surpassing a range of methods using both phrase based machine translation and neural machine translation. our method achieves a significant improvement of 1.3 bleu from the previous best models on both wmt 14 and wmt 15 english rightarrow german. we also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.
structured attention networks
attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. however for many tasks we may want to model richer structural dependencies without abandoning end to end training. in this work we experiment with incorporating richer structural distributions encoded using graphical models within deep networks. we show that these structured attention networks are simple extensions of the basic attention procedure and that they allow for extending attention beyond the standard soft selection approach such as attending to partial segmentations or to subtrees. we experiment with two different classes of structured attention networks a linear chain conditional random field and a graph based parsing model and describe how these models can be practically implemented as neural network layers. experiments show that this approach is effective for incorporating structural biases and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks tree transduction neural machine translation question answering and natural language inference. we further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.
end to end multi view networks for text classification
we propose a multi view network for text classification. our method automatically creates various views of its input text each taking the form of soft attention weights that distribute the classifier s focus among a set of base features. for a bag of words representation each view focuses on a different subset of the text s words. aggregating many such views results in a more discriminative and robust representation. through a novel architecture that both stacks and concatenates views we produce a network that emphasizes both depth and width allowing training to converge quickly. using our multi view architecture we establish new state of the art accuracies on two benchmark tasks.
differentiable scheduled sampling for credit assignment
we demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence to sequence seq2seq models. by incorporating this approximation into the scheduled sampling training procedure bengio et al. 2015 a well known technique for correcting exposure bias we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. in addition by using a related approximation we demonstrate a similar approach to sampled based training. finally we show that our approach outperforms cross entropy training and scheduled sampling procedures in two sequence prediction tasks named entity recognition and machine translation.
phone aware neural language identification
pure acoustic neural models particularly the lstm rnn model have shown great potential in language identification lid . however the phonetic information has been largely overlooked by most of existing neural lid models although this information has been used in the conventional phonetic lid systems with a great success. we present a phone aware neural lid architecture which is a deep lstm rnn lid system but accepts output from an rnn based asr system. by utilizing the phonetic knowledge the lid performance can be significantly improved. interestingly even if the test language is not involved in the asr training the phonetic knowledge still presents a large contribution. our experiments conducted on four languages within the babel corpus demonstrated that the phone aware approach is highly effective.
detecting off topic responses to visual prompts
automated methods for essay scoring have made great progress in recent years achieving accuracies very close to human annotators. however a known weakness of such automated scorers is not taking into account the semantic relevance of the submitted text. while there is existing work on detecting answer relevance given a textual prompt very little previous research has been done to incorporate visual writing prompts. we propose a neural architecture and several extensions for detecting off topic responses to visual prompts and evaluate it on a dataset of texts written by language learners.
dual rectified linear units drelus a replacement for tanh activation functions in quasi recurrent neural networks
in this paper we introduce a novel type of rectified linear unit relu called a dual rectified linear unit drelu . a drelu which comes with an unbounded positive and negative image can be used as a drop in replacement for a tanh activation function in the recurrent step of quasi recurrent neural networks qrnns bradbury et al. 2017 . similar to relus drelus are less prone to the vanishing gradient problem they are noise robust and they induce sparse activations. we independently reproduce the qrnn experiments of bradbury et al. 2017 and compare our drelu based qrnns with the original tanh based qrnns and long short term memory networks lstms on sentiment classification and word level language modeling. additionally we evaluate on character level language modeling showing that we are able to stack up to eight qrnn layers with drelus thus making it possible to improve the current state of the art in character level language modeling over shallow architectures based on lstms.
fidelity weighted learning
training deep neural networks requires many training samples but in practice training labels are expensive to obtain and may be of varying quality as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd sourcing. this creates a fundamental quality versus quantity trade off in the learning process. do we learn from the small amount of high quality data or the potentially large amount of weakly labeled data we argue that if the learner could somehow know and take the label quality into account when learning the data representation we could get the best of both worlds. to this end we propose fidelity weighted learning fwl a semi supervised student teacher approach for training deep neural networks using weakly labeled data. fwl modulates the parameter updates to a student network trained on the task we care about on a per sample basis according to the posterior confidence of its label quality estimated by a teacher who has access to the high quality labels . both student and teacher are learned from the data. we evaluate fwl on two tasks in information retrieval and natural language processing where we outperform state of the art alternative semi supervised methods indicating that our approach makes better use of strong and weak labels and leads to better task dependent data representations.
feature learning in deep neural networks studies on speech recognition tasks
recent studies have shown that deep neural networks dnns perform significantly better than shallow networks and gaussian mixture models gmms on large vocabulary speech recognition tasks. in this paper we argue that the improved accuracy achieved by the dnns is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. we show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth which leads to better speech recognition performance with deeper networks. we also show that dnns cannot extrapolate to test samples that are substantially different from the training examples. if the training data are sufficiently representative however internal features learned by the dnn are relatively stable with respect to speaker differences bandwidth differences and environment distortion. this enables dnn based recognizers to perform as well or better than state of the art systems based on gmms or shallow networks without the need for explicit model adaptation or feature normalization.
estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks
in hybrid hidden markov model artificial neural networks hmm ann automatic speech recognition asr system the phoneme class conditional probabilities are estimated by first extracting acoustic features from the speech signal based on prior knowledge such as speech perception or and speech production knowledge and then modeling the acoustic features with an ann. recent advances in machine learning techniques more specifically in the field of image processing and text processing have shown that such divide and conquer strategy i.e. separating feature extraction and modeling steps may not be necessary. motivated from these studies in the framework of convolutional neural networks cnns this paper investigates a novel approach where the input to the ann is raw speech signal and the output is phoneme class conditional probability estimates. on timit phoneme recognition task we study different ann architectures to show the benefit of cnns and compare the proposed approach against conventional approach where spectral based feature mfcc is extracted and modeled by a multilayer perceptron. our studies show that the proposed approach can yield comparable or better phoneme recognition performance when compared to the conventional approach. it indicates that cnns can learn features relevant for phoneme classification automatically from the raw speech signal.
recursive neural networks can learn logical semantics
tree structured recursive neural networks treernns for sentence meaning have been successful for many applications but it remains an open question whether the fixed length representations that they learn can support tasks as demanding as logical deduction. we pursue this question by evaluating whether two such models plain treernns and tree structured neural tensor networks treerntns can correctly learn to identify logical relationships such as entailment and contradiction using these representations. in our first set of experiments we generate artificial data from a logical grammar and use it to evaluate the models ability to learn to handle basic relational reasoning recursive structures and quantification. we then evaluate the models on the more natural sick challenge data. both models perform competitively on the sick data and generalize well in all three experiments on simulated data suggesting that they can learn suitable representations for logical inference in natural language.
a re ranking model for dependency parser with recursive convolutional neural network
in this work we address the problem to model all the nodes words or phrases in a dependency tree with the dense representations. we propose a recursive convolutional neural network rcnn architecture to capture syntactic and compositional semantic representations of phrases and words in a dependency tree. different with the original recursive neural network we introduce the convolution and pooling layers which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. based on rcnn we use a discriminative model to re rank a k best list of candidate dependency parsing trees. the experiments show that rcnn is very effective to improve the state of the art dependency parsing on both english and chinese datasets.
deep speaker vectors for semi text independent speaker verification
recent research shows that deep neural networks dnns can be used to extract deep speaker vectors d vectors that preserve speaker characteristics and can be used in speaker verification. this new method has been tested on text dependent speaker verification tasks and improvement was reported when combined with the conventional i vector method. this paper extends the d vector approach to semi text independent speaker verification tasks i.e. the text of the speech is in a limited set of short phrases. we explore various settings of the dnn structure used for d vector extraction and present a phone dependent training which employs the posterior features obtained from an asr system. the experimental results show that it is possible to apply d vectors on semi text independent speaker recognition and the phone dependent training improves system performance.
advances in very deep convolutional neural networks for lvcsr
very deep cnns with small 3x3 kernels have recently been shown to achieve very strong performance as acoustic models in hybrid nn hmm speech recognition systems. in this paper we investigate how to efficiently scale these models to larger datasets. specifically we address the design choice of pooling and padding along the time dimension which renders convolutional evaluation of sequences highly inefficient. we propose a new cnn design without timepadding and without timepooling which is slightly suboptimal for accuracy but has two significant advantages it enables sequence training and deployment by allowing efficient convolutional evaluation of full utterances and it allows for batch normalization to be straightforwardly adopted to cnns on sequence data. through batch normalization we recover the lost peformance from removing the time pooling while keeping the benefit of efficient convolutional evaluation. we demonstrate the performance of our models both on larger scale data than before and after sequence training. our very deep cnn model sequence trained on the 2000h switchboard dataset obtains 9.4 word error rate on the hub5 test set matching with a single model the performance of the 2015 ibm system combination which was the previous best published result.
learning compact recurrent neural networks
recurrent neural networks rnns including long short term memory lstm rnns have produced state of the art results on a variety of speech recognition tasks. however these models are often too large in size for deployment on mobile devices with memory and latency constraints. in this work we study mechanisms for learning compact rnns and lstms via low rank factorizations and parameter sharing schemes. our goal is to investigate redundancies in recurrent architectures where compression can be admitted without losing performance. a hybrid strategy of using structured matrices in the bottom layers and shared low rank factors on the top layers is found to be particularly effective reducing the parameters of a standard lstm by 75 at a small cost of 0.3 increase in wer on a 2 000 hr english voice search task.
dependency parsing with lstms an empirical evaluation
we propose a transition based dependency parser using recurrent neural networks with long short term memory lstm units. this extends the feedforward neural network parser of chen and manning 2014 and enables modelling of entire sequences of shift reduce transition decisions. on the google web treebank our lstm parser is competitive with the best feedforward parser on overall accuracy and notably achieves more than 3 improvement for long range dependencies which has proved difficult for previous transition based parsers due to error propagation and limited context information. our findings additionally suggest that dropout regularisation on the embedding layer is crucial to improve the lstm s generalisation.
deep sentence embedding using long short term memory networks analysis and application to information retrieval
this paper develops a model that addresses sentence embedding a hot topic in current natural language processing research using recurrent neural networks with long short term memory lstm cells. due to its ability to capture long term memory the lstm rnn accumulates increasingly richer information as it goes through the sentence and when it reaches the last word the hidden layer of the network provides a semantic representation of the whole sentence. in this paper the lstm rnn is trained in a weakly supervised manner on user click through data logged by a commercial web search engine. visualization and analysis are performed to understand how the embedding process works. the model is found to automatically attenuate the unimportant words and detects the salient keywords in the sentence. furthermore these detected keywords are found to automatically activate different cells of the lstm rnn where words belonging to a similar topic activate the same cell. as a semantic representation of the sentence the embedding vector can be used in many different applications. these automatic keyword detection and topic allocation abilities enabled by the lstm rnn allow the network to perform document retrieval a difficult language processing task where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the lstm rnn. on a web search task the lstm rnn embedding is shown to significantly outperform several existing state of the art methods. we emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. a comparison with a well known general sentence embedding method the paragraph vector is performed. the results show that the proposed method in this paper significantly outperforms it for web document retrieval task.
encoding source language with convolutional neural network for machine translation
the recently proposed neural network joint model nnjm devlin et al. 2014 augments the n gram target language model with a heuristically chosen source context window achieving state of the art performance in smt. in this paper we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. with different guiding signals during decoding our specifically designed convolution gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word and fuse them with the context of entire source sentence to form a unified representation. this representation together with target language words are fed to a deep neural network dnn to form a stronger nnjm. experiments on two nist chinese english translation tasks show that the proposed model can achieve significant improvements over the previous nnjm by up to 1.08 bleu points on average
maximum a posteriori adaptation of network parameters in deep models
we present a bayesian approach to adapting parameters of a well trained context dependent deep neural network hidden markov model cd dnn hmm to improve automatic speech recognition performance. given an abundance of dnn parameters but with only a limited amount of data the effectiveness of the adapted dnn model can often be compromised. we formulate maximum a posteriori map adaptation of parameters of a specially designed cd dnn hmm with an augmented linear hidden networks connected to the output tied states or senones and compare it to feature space map linear regression previously proposed. experimental evidences on the 20 000 word open vocabulary wall street journal task demonstrate the feasibility of the proposed framework. in supervised adaptation the proposed map adaptation approach provides more than 10 relative error reduction and consistently outperforms the conventional transformation based methods. furthermore we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones.
context dependent translation selection using convolutional neural network
we propose a novel method for translation selection in statistical machine translation in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. the specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair but also the context containing the phrase in the source language. therefore our approach is able to capture context dependent semantic similarities of translation pairs. we adopt a curriculum learning strategy to train the model we classify the training examples into easy medium and difficult categories and gradually build the ability of representing phrase and sentence level context by using training examples from easy to difficult. experimental results show that our approach significantly outperforms the baseline system by up to 1.4 bleu points.
convolutional neural network architectures for matching natural language sentences
semantic matching is of central importance to many natural language tasks cite bordes2014semantic retrievalqa . a successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. as a step toward this goal we propose convolutional neural network models for matching two sentences by adapting the convolutional strategy in vision and speech. the proposed models not only nicely represent the hierarchical structures of sentences with their layer by layer composition and pooling but also capture the rich matching patterns at different levels. our models are rather generic requiring no prior knowledge on language and can hence be applied to matching tasks of different nature and in different languages. the empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.
long short term memory over tree structures
the chain structured long short term memory lstm has showed to be effective in a wide range of problems such as speech recognition and machine translation. in this paper we propose to extend it to tree structures in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. we call the model s lstm which provides a principled way of considering long distance interaction over hierarchies e.g. language or image parse structures. we leverage the models for semantic composition to understand the meaning of text a fundamental problem in natural language understanding and show that it outperforms a state of the art recursive model by replacing its composition layers with the s lstm memory blocks. we also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.
improving the performance of neural machine translation involving morphologically rich languages
the advent of the attention mechanism in neural machine translation models has improved the performance of machine translation systems by enabling selective lookup into the source sentence. in this paper the efficiencies of translation using bidirectional encoder attention decoder models were studied with respect to translation involving morphologically rich languages. the english tamil language pair was selected for this analysis. first the use of word2vec embedding for both the english and tamil words improved the translation results by 0.73 bleu points over the baseline rnnsearch model with 4.84 bleu score. the use of morphological segmentation before word vectorization to split the morphologically rich tamil words into their respective morphemes before the translation caused a reduction in the target vocabulary size by a factor of 8. also this model rnnmorph improved the performance of neural machine translation by 7.05 bleu points over the rnnsearch model used over the same corpus. since the bleu evaluation of the rnnmorph model might be unreliable due to an increase in the number of matching tokens per sentence the performances of the translations were also compared by means of human evaluation metrics of adequacy fluency and relative ranking. further the use of morphological segmentation also improved the efficacy of the attention mechanism.
a recurrent neural network without chaos
we introduce an exceptionally simple gated recurrent neural network rnn that achieves performance comparable to well known gated architectures such as lstms and grus on the word level language modeling task. we prove that our model has simple predicable and non chaotic dynamics. this stands in stark contrast to more standard gated architectures whose underlying dynamical systems exhibit chaotic behavior.
end to end phoneme sequence recognition using convolutional neural networks
most phoneme recognition state of the art systems rely on a classical neural network classifiers fed with highly tuned features such as mfcc or plp features. recent advances in deep learning approaches questioned such systems but while some attempts were made with simpler features such as spectrograms state of the art systems still rely on mfccs. this might be viewed as a kind of failure from deep learning approaches which are often claimed to have the ability to train with raw signals alleviating the need of hand crafted features. in this paper we investigate a convolutional neural network approach for raw speech signals. while convolutional architectures got tremendous success in computer vision or text processing they seem to have been let down in the past recent years in the speech processing field. we show that it is possible to learn an end to end phoneme sequence classifier system directly from raw signal with similar performance on the timit and wsj datasets than existing systems based on mfcc questioning the need of complex hand crafted features on large datasets.
a deep learning approach to data driven parameterizations for statistical parametric speech synthesis
nearly all statistical parametric speech synthesizers today use mel cepstral coefficients as the vocal tract parameterization of the speech signal. mel cepstral coefficients were never intended to work in a parametric speech synthesis framework but as yet there has been little success in creating a better parameterization that is more suited to synthesis. in this paper we use deep learning algorithms to investigate a data driven parameterization technique that is designed for the specific requirements of synthesis. we create an invertible low dimensional noise robust encoding of the mel log spectrum by training a tapered stacked denoising autoencoder sda . this sda is then unwrapped and used as the initialization for a multi layer perceptron mlp . the mlp is fine tuned by training it to reconstruct the input at the output layer. this mlp is then split down the middle to form encoding and decoding networks. these networks produce a parameterization of the mel log spectrum that is intended to better fulfill the requirements of synthesis. results are reported for experiments conducted using this resulting parameterization with the clustergen speech synthesizer.
addressing the rare word problem in neural machine translation
neural machine translation nmt is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. a significant weakness in conventional nmt systems is their inability to correctly translate very rare words end to end nmts tend to have relatively small vocabularies with a single unk symbol that represents every possible out of vocabulary oov word. in this paper we propose and implement an effective technique to address this problem. we train an nmt system on data that is augmented by the output of a word alignment algorithm allowing the nmt system to emit for each oov word in the target sentence the position of its corresponding word in the source sentence. this information is later utilized in a post processing step that translates every oov word using a dictionary. our experiments on the wmt14 english to french translation task show that this method provides a substantial improvement of up to 2.8 bleu points over an equivalent nmt system that does not use this technique. with 37.5 bleu points our nmt system is the first to surpass the best result achieved on a wmt14 contest task.
investigating the role of prior disambiguation in deep learning compositional models of meaning
this paper aims to explore the effect of prior disambiguation on neural network based compositional models with the hope that better semantic representations for text compounds can be produced. we disambiguate the input word vectors before they are fed into a compositional deep net. a series of evaluations shows the positive effect of prior disambiguation for such deep models.
deep speech scaling up end to end speech recognition
we present a state of the art speech recognition system developed using end to end deep learning. our architecture is significantly simpler than traditional speech systems which rely on laboriously engineered processing pipelines these traditional systems also tend to perform poorly when used in noisy environments. in contrast our system does not need hand designed components to model background noise reverberation or speaker variation but instead directly learns a function that is robust to such effects. we do not need a phoneme dictionary nor even the concept of a phoneme. key to our approach is a well optimized rnn training system that uses multiple gpus as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. our system called deep speech outperforms previously published results on the widely studied switchboard hub5 00 achieving 16.0 error on the full test set. deep speech also handles challenging noisy environments better than widely used state of the art commercial speech systems.
incremental adaptation strategies for neural network language models
it is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. however training these models on large amounts of data can take several days. we present efficient techniques to adapt a neural network language model to new data. instead of training a completely new model or relying on mixture approaches we propose two new methods continued training on resampled data or insertion of adaptation layers. we present experimental results in an cat environment where the post edits of professional translators are used to improve an smt system. both methods are very fast and achieve significant improvements without overfitting the small adaptation data.
joint rnn based greedy parsing and word composition
this paper introduces a greedy parser based on neural networks which leverages a new compositional sub tree representation. the greedy parser and the compositional procedure are jointly trained and tightly depends on each other. the composition procedure outputs a vector representation which summarizes syntactically parsing tags and semantically words sub trees. composition and tagging is achieved over continuous word or tag representations and recurrent neural networks. we reach f1 performance on par with well known existing parsers while having the advantage of speed thanks to the greedy nature of the parser. we provide a fully functional implementation of the method described in this paper.
efficient exact gradient update for training deep networks with very large sparse targets
an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g. neural language models or the learning of word embeddings often posed as predicting the probability of next words among a vocabulary of size d e.g. 200 000 . computing the equally large but typically non sparse d dimensional output vector from a last hidden layer of reasonable dimension d e.g. 500 incurs a prohibitive o dd computational cost for each example as does updating the d x d output weight matrix and computing the gradient needed for backpropagation to previous layers. while efficient handling of large sparse network inputs is trivial the case of large sparse targets is not and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling based approximations during training. in this work we develop an original algorithmic approach which for a family of loss functions that includes squared error and spherical softmax can compute the exact loss gradient update for the output weights and gradient for backpropagation all in o d 2 per example instead of o dd remarkably without ever computing the d dimensional output. the proposed algorithm yields a speedup of d 4d i.e. two orders of magnitude for typical sizes for that critical part of the computations that often dominates the training time in this kind of network architecture.
discriminative neural sentence modeling by tree based convolution
this paper proposes a tree based convolutional neural network tbcnn for discriminative sentence modeling. our models leverage either constituency trees or dependency trees of sentences. the tree based convolution process extracts sentences structural features and these features are aggregated by max pooling. such architecture allows short propagation paths between the output layer and underlying feature detectors which enables effective structural feature learning and extraction. we evaluate our models on two tasks sentiment analysis and question classification. in both experiments tbcnn outperforms previous state of the art results including existing neural networks and dedicated feature rule engineering. we also make efforts to visualize the tree based convolution process shedding light on how our models work.
self adaptive hierarchical sentence model
the ability to accurately model a sentence at varying stages e.g. word phrase sentence plays a central role in natural language processing. as an effort towards this goal we propose a self adaptive hierarchical sentence model adasent . adasent effectively forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments. we design a competitive mechanism through gating networks to allow the representations of the same sentence to be engaged in a particular learning task e.g. classification therefore effectively mitigating the gradient vanishing problem persistent in other recursive models. both qualitative and quantitative analysis shows that adasent can automatically form and select the representations suitable for the task at hand during training yielding superior classification performance over competitor models on 5 benchmark data sets.
classifying relations by ranking with convolutional neural networks
relation classification is an important semantic processing task for which state ofthe art systems still rely on costly handcrafted features. in this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking cr cnn . we propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. we perform experiments using the the semeval 2010 task 8 dataset which is designed for the task of classifying the relationship between two nominals marked in a sentence. using crcnn we outperform the state of the art for this dataset and achieve a f1 of 84.1 without using any costly handcrafted features. additionally our experimental results show that 1 our approach is more effective than cnn followed by a softmax classifier 2 omitting the representation of the artificial class other improves both precision and recall and 3 using only word embeddings as input features is enough to achieve state of the art results if we consider only the text between the two target nominals.
lexical translation model using a deep neural network architecture
in this paper we combine the advantages of a model using global source sentence contexts the discriminative word lexicon and neural networks. by using deep neural networks instead of the linear maximum entropy model in the discriminative word lexicon models we are able to leverage dependencies between different source words due to the non linearity. furthermore the models for different target words can share parameters and therefore data sparsity problems are effectively reduced. by using this approach in a state of the art translation system we can improve the performance by up to 0.5 bleu points for three different language pairs on the ted translation task.
visualizing and understanding recurrent networks
recurrent neural networks rnns and specifically a variant with long short term memory lstm are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. however while lstms provide exceptional results in practice the source of their performance and their limitations remain rather poorly understood. using character level language models as an interpretable testbed we aim to bridge this gap by providing an analysis of their representations predictions and error types. in particular our experiments reveal the existence of interpretable cells that keep track of long range dependencies such as line lengths quotes and brackets. moreover our comparative analysis with finite horizon n gram models traces the source of the lstm improvements to long range structural dependencies. finally we provide analysis of the remaining errors and suggests areas for further study.
a multi layered acoustic tokenizing deep neural network mat dnn for unsupervised discovery of linguistic units and generation of high quality features
this paper summarizes the work done by the authors for the zero resource speech challenge organized in the technical program of interspeech 2015. the goal of the challenge is to discover linguistic units directly from unlabeled speech data. the multi layered acoustic tokenizer mat proposed in this work automatically discovers multiple sets of acoustic tokens from the given corpus. each acoustic token set is specified by a set of hyperparameters that describe the model configuration. these sets of acoustic tokens carry different characteristics of the given corpus and the language behind thus can be mutually reinforced. the multiple sets of token labels are then used as the targets of a multi target dnn mdnn trained on low level acoustic features. bottleneck features extracted from the mdnn are used as feedback for the mat and the mdnn itself. we call this iterative system the multi layered acoustic tokenizing deep neural network mat dnn which generates high quality features for track 1 of the challenge and acoustic tokens for track 2 of the challenge.
author identification using multi headed recurrent neural networks
recurrent neural networks rnns are very good at modelling the flow of text but typically need to be trained on a far larger corpus than is available for the pan 2015 author identification task. this paper describes a novel approach where the output layer of a character level rnn language model is split into several independent predictive sub models each representing an author while the recurrent layer is shared by all. this allows the recurrent layer to model the language as a whole without over fitting while the outputs select aspects of the underlying model that reflect their author s style. the method proves competitive ranking first in two of the four languages.
a deep memory based architecture for sequence to sequence learning
we propose deepmemory a novel deep architecture for sequence to sequence learning which performs the task through a series of nonlinear transformations from the representation of the input sequence e.g. a chinese sentence to the final output sequence e.g. translation to english . inspired by the recently proposed neural turing machine graves et al. 2014 we store the intermediate representations in stacked layers of memories and use read write operations on the memories to realize the nonlinear transformations between the representations. the types of transformations are designed in advance but the parameters are learned from data. through layer by layer transformations deepmemory can model complicated relations between sequences necessary for applications such as machine translation between distant languages. the architecture can be trained with normal back propagation on sequenceto sequence data and the learning can be easily scaled up to a large corpus. deepmemory is broad enough to subsume the state of the art neural translation model in bahdanau et al. 2015 as its special case while significantly improving upon the model with its deeper architecture. remarkably deepmemory being purely neural network based can achieve performance comparable to the traditional phrase based machine translation system moses with a small vocabulary and a modest parameter size.
ask me anything dynamic memory networks for natural language processing
most tasks in natural language processing can be cast into question answering qa problems over language input. we introduce the dynamic memory network dmn a neural network architecture which processes input sequences and questions forms episodic memories and generates relevant answers. questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. these results are then reasoned over in a hierarchical recurrent sequence model to generate answers. the dmn can be trained end to end and obtains state of the art results on several types of tasks and datasets question answering facebook s babi dataset text classification for sentiment analysis stanford sentiment treebank and sequence modeling for part of speech tagging wsj ptb . the training for these different tasks relies exclusively on trained word vector representations and input question answer triplets.
improved deep speaker feature learning for text dependent speaker recognition
a deep learning approach has been proposed recently to derive speaker identifies d vector by a deep neural network dnn . this approach has been applied to text dependent speaker recognition tasks and shows reasonable performance gains when combined with the conventional i vector approach. although promising the existing d vector implementation still can not compete with the i vector baseline. this paper presents two improvements for the deep learning approach a phonedependent dnn structure to normalize phone variation and a new scoring approach based on dynamic time warping dtw . experiments on a text dependent speaker recognition task demonstrated that the proposed methods can provide considerable performance improvement over the existing d vector implementation.
grid long short term memory
this paper introduces grid long short term memory a network of lstm cells arranged in a multidimensional grid that can be applied to vectors sequences or higher dimensional data such as images. the network differs from existing deep lstm architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. the network provides a unified way of using lstm for both deep and sequential computation. we apply the model to algorithmic tasks such as 15 digit integer addition and sequence memorization where it is able to significantly outperform the standard lstm. we then give results for two empirical tasks. we find that 2d grid lstm achieves 1.47 bits per character on the wikipedia character prediction benchmark which is state of the art among neural approaches. in addition we use the grid lstm to define a novel two dimensional translation model the reencoder and show that it outperforms a phrase based reference system on a chinese to english translation task.
a dependency based neural network for relation classification
previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. in this paper we further explore how to make full use of the combination of these dependency information. we first propose a new structure termed augmented dependency path adp which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. to exploit the semantic representation behind the adp structure we develop dependency based neural networks depnn a recursive neural network designed to model the subtrees and a convolutional neural network to capture the most important features on the shortest path. experiments on the semeval 2010 dataset show that our proposed method achieves state of art results.
pte predictive text embedding through large scale heterogeneous text networks
unsupervised text embedding methods such as skip gram and paragraph vector have been attracting increasing attention due to their simplicity scalability and effectiveness. however comparing to sophisticated deep learning architectures such as convolutional neural networks these methods usually yield inferior results when applied to particular machine learning tasks. one possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way without leveraging the labeled information available for the task. although the low dimensional representations learned are applicable to many different tasks they are not particularly tuned for any task. in this paper we fill this gap by proposing a semi supervised representation learning method for text data which we call the textit predictive text embedding pte . predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. the labeled information and different levels of word co occurrence information are first represented as a large scale heterogeneous text network which is then embedded into a low dimensional space through a principled and efficient algorithm. this low dimensional embedding not only preserves the semantic closeness of words and documents but also has a strong predictive power for the particular task. compared to recent supervised approaches based on convolutional neural networks predictive text embedding is comparable or more effective much more efficient and has fewer parameters to tune.
relation classification via recurrent neural network
deep learning has gained much success in sentence level relation classification. for example convolutional neural networks cnn have delivered competitive performance without much effort on feature engineering as the conventional pattern based methods. thus a lot of works have been produced based on cnn structures. however a key issue that has not been well addressed by the cnn based method is the lack of capability to learn temporal features especially long distance dependency between nominal pairs. in this paper we propose a simple framework based on recurrent neural networks rnn and compare it with cnn based model. to show the limitation of popular used semeval 2010 task 8 dataset we introduce another dataset refined from mimlre angeli et al. 2014 . experiments on two different datasets strongly indicates that the rnn based model can deliver better performance on relation classification and it is particularly capable of learning long distance relation patterns. this makes it suitable for real world applications where complicated expressions are often involved.
learning from lda using deep neural networks
latent dirichlet allocation lda is a three level hierarchical bayesian model for topic inference. in spite of its great success inferring the latent topic distribution with lda is time consuming. motivated by the transfer learning approach proposed by newcite hinton2015distilling we present a novel method that uses lda to supervise the training of a deep neural network dnn so that the dnn can approximate the costly lda inference with less computation. our experiments on a document classification task show that a simple dnn can learn the lda behavior pretty well while the inference is speeded up tens or hundreds of times.
online representation learning in recurrent neural language models
we investigate an extension of continuous online learning in recurrent neural network language models. the model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. the initial experiments give promising results indicating that the method is able to increase language modelling accuracy while also decreasing the parameters needed to store the model along with the computation required at each step.
a sensitivity analysis of and practitioners guide to convolutional neural networks for sentence classification
convolutional neural networks cnns have recently achieved remarkably strong performance on the practically important task of sentence classification kim 2014 kalchbrenner 2014 johnson 2014 . however these models require practitioners to specify an exact model architecture and set accompanying hyperparameters including the filter region size regularization parameters and so on. it is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. we thus conduct a sensitivity analysis of one layer cnns to explore the effect of architecture components on model performance our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. we focus on one layer cnns to the exclusion of more complex models due to their comparative simplicity and strong empirical performance which makes it a modern standard baseline method akin to support vector machine svms and logistic regression. we derive practical advice from our extensive empirical results for those interested in getting the most out of cnns for sentence classification in real world settings.
prediction adaptation correction recurrent neural networks for low resource language speech recognition
in this paper we investigate the use of prediction adaptation correction recurrent neural networks pac rnns for low resource speech recognition. a pac rnn is comprised of a pair of neural networks in which a it correction network uses auxiliary information given by a it prediction network to help estimate the state probability. the information from the correction network is also used by the prediction network in a recurrent loop. our model outperforms other state of the art neural networks dnns lstms on iarpa babel tasks. moreover transfer learning from a language that is similar to the target language can help improve performance further.
generating text with deep reinforcement learning
we introduce a novel schema for sequence to sequence learning with a deep q network dqn which decodes the output sequence iteratively. the aim here is to enable the decoder to first tackle easier portions of the sequences and then turn to cope with difficult parts. specifically in each iteration an encoder decoder long short term memory lstm network is employed to from the input sequence automatically create features to represent the internal states of and formulate a list of potential actions for the dqn. take rephrasing a natural sentence as an example. this list can contain ranked potential words. next the dqn learns to make decision on which action e.g. word will be selected from the list to modify the current decoded sequence. the newly modified output sequence is subsequently used as the input to the dqn for the next decoding iteration. in each iteration we also bias the reinforcement learning s attention to explore sequence portions which are previously difficult to be decoded. for evaluation the proposed strategy was trained to decode ten thousands natural sentences. our experiments indicate that when compared to a left to right greedy beam search lstm decoder the proposed method performed competitively well when decoding sentences from the training set but significantly outperformed the baseline when decoding unseen sentences in terms of bleu score obtained.
detecting interrogative utterances with recurrent neural networks
in this paper we explore different neural network architectures that can predict if a speaker of a given utterance is asking a question or making a statement. we com pare the outcomes of regularization methods that are popularly used to train deep neural networks and study how different context functions can affect the classification performance. we also compare the efficacy of gated activation functions that are favorably used in recurrent neural networks and study how to combine multimodal inputs. we evaluate our models on two multimodal datasets msr skype and callhome.
a neural transducer
sequence to sequence models have achieved impressive results on various tasks. however they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. this is because they generate an output sequence conditioned on an entire input sequence. in this paper we present a neural transducer that can make incremental predictions as more input arrives without redoing the entire computation. unlike sequence to sequence models the neural transducer computes the next step distribution conditioned on the partially observed input sequence and the partially generated sequence. at each time step the transducer can decide to emit zero to many output symbols. the data can be processed using an encoder and presented as input to the transducer. the discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. it is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. our experiments show that the neural transducer works well in settings where it is required to produce output predictions as data come in. we also find that the neural transducer performs well for long sequences even when attention mechanisms are not used.
skip thought memory networks
question answering qa is fundamental to natural language processing in that most nlp problems can be phrased as qa kumar et al. 2015 . current weakly supervised memory network models that have been proposed so far struggle at answering questions that involve relations among multiple entities such as facebook s babi qa5 three arg relations in weston et al. 2015 . to address this problem of learning multi argument multi hop semantic relations for the purpose of qa we propose a method that combines the jointly learned long term read write memory and attentive inference components of end to end memory networks memn2n sukhbaatar et al. 2015 with distributed sentence vector representations encoded by a skip thought model kiros et al. 2015 . this choice to append skip thought vectors to the existing memn2n framework is motivated by the fact that skip thought vectors have been shown to accurately model multi argument semantic relations kiros et al. 2015 .
named entity recognition with bidirectional lstm cnns
named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. in this paper we present a novel neural network architecture that automatically detects word and character level features using a hybrid bidirectional lstm and cnn architecture eliminating the need for most feature engineering. we also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. extensive evaluation shows that given only tokenized text and publicly available word embeddings our system is competitive on the conll 2003 dataset and surpasses the previously reported state of the art performance on the ontonotes 5.0 dataset by 2.13 f1 points. by using two lexicons constructed from publicly available sources we establish new state of the art performance with an f1 score of 91.62 on conll 2003 and 86.28 on ontonotes surpassing systems that employ heavy feature engineering proprietary lexicons and rich entity linking information.
generating news headlines with recurrent neural networks
we describe an application of an encoder decoder recurrent neural network with lstm units and attention to generating headlines from the text of news articles. we find that the model is quite effective at concisely paraphrasing news articles. furthermore we study how the neural network decides which input words to pay attention to and specifically we identify the function of the different neurons in a simplified attention mechanism. interestingly our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles.
words are not equal graded weighting model for building composite document vectors
despite the success of distributional semantics composing phrases from word vectors remains an important challenge. several methods have been tried for benchmark tasks such as sentiment classification including word vector averaging matrix vector approaches based on parsing and on the fly learning of paragraph vectors. most models usually omit stop words from the composition. instead of such an yes no decision we consider several graded schemes where words are weighted according to their discriminatory relevance with respect to its use in the document e.g. idf . some of these methods particularly tf idf are seen to result in a significant improvement in performance over prior state of the art. further combining such approaches into an ensemble based on alternate classifiers such as the rnn model results in an 1.6 performance improvement on the standard imdb movie review dataset and a 7.01 improvement on amazon product reviews. since these are language free models and can be obtained in an unsupervised manner they are of interest also for under resourced languages such as hindi as well and many more languages. we demonstrate the language free aspects by showing a gain of 12 for two review datasets over earlier results and also release a new larger dataset for future testing singh 2015 .
small footprint deep neural networks with highway connections for speech recognition
for speech recognition deep neural networks dnns have significantly improved the recognition accuracy in most of benchmark datasets and application domains. however compared to the conventional gaussian mixture models dnn based acoustic models usually have much larger number of model parameters making it challenging for their applications in resource constrained platforms e.g. mobile devices. in this paper we study the application of the recently proposed highway network to train small footprint dnns which are it thinner and it deeper and have significantly smaller number of model parameters compared to conventional dnns. we investigated this approach on the ami meeting speech transcription corpus which has around 70 hours of audio data. the highway neural networks constantly outperformed their plain dnn counterparts and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy.
backward and forward language modeling for constrained sentence generation
recent language models especially those based on recurrent neural networks rnns make it possible to generate natural language from a learned probability. language generation has wide applications including machine translation summarization question answering conversation systems etc. existing methods typically learn a joint probability of words conditioned on additional information which is either statically or dynamically fed to rnn s hidden layer. in many applications we are likely to impose hard constraints on the generated texts i.e. a particular word must appear in the sentence. unfortunately existing approaches could not solve this problem. in this paper we propose a novel backward and forward language model. provided a specific word we use rnns to generate previous words and future words either simultaneously or asynchronously resulting in two model variants. in this way the given word could appear at any position in the sentence. experimental results show that the generated texts are comparable to sequential lms in quality.
online keyword spotting with a character level recurrent neural network
in this paper we propose a context aware keyword spotting model employing a character level recurrent neural network rnn for spoken term detection in continuous speech. the rnn is end to end trained with connectionist temporal classification ctc to generate the probabilities of character and word boundary labels. there is no need for the phonetic transcription senone modeling or system dictionary in training and testing. also keywords can easily be added and modified by editing the text based keyword list without retraining the rnn. moreover the unidirectional rnn processes an infinitely long input audio streams without pre segmentation and keywords are detected with low latency before the utterance is finished. experimental results show that the proposed keyword spotter significantly outperforms the deep neural network dnn and hidden markov model hmm based keyword filler model even with less computations.
domain specific author attribution based on feedforward neural network language models
authorship attribution refers to the task of automatically determining the author based on a given sample of text. it is a problem with a long history and has a wide range of application. building author profiles using language models is one of the most successful methods to automate this task. new language modeling methods based on neural networks alleviate the curse of dimensionality and usually outperform conventional n gram methods. however there have not been much research applying them to authorship attribution. in this paper we present a novel setup of a neural network language model nnlm and apply it to a database of text samples from different authors. we investigate how the nnlm performs on a task with moderate author set size and relatively limited training and test data and how the topics of the text samples affect the accuracy. nnlm achieves nearly 2.5 reduction in perplexity a measurement of fitness of a trained language model to the test data. given 5 random test sentences it also increases the author classification accuracy by 3.43 on average compared with the n gram methods using srilm tools. an open source implementation of our methodology is freely available at https github.com zge authorship attribution .
segmental recurrent neural networks for end to end speech recognition
we study the segmental recurrent neural network for end to end acoustic modelling. this model connects the segmental conditional random field crf with a recurrent neural network rnn used for feature extraction. compared to most previous crf based acoustic models it does not rely on an external system to provide features or segmentation boundaries. instead this model marginalises out all the possible segmentations and features are extracted from the rnn trained together with the segmental crf. in essence this model is self contained and can be trained end to end. in this paper we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. we performed experiments on the timit dataset. we achieved 17.3 phone error rate per from the first pass decoding the best reported result using crfs despite the fact that we only used a zeroth order crf and without using any language model.
how transferable are neural networks in nlp applications 
transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. it is particularly important to neural networks which are very likely to be overfitting. in some fields like image processing many studies have shown the effectiveness of neural network based transfer learning. for neural nlp however existing studies have only casually applied transfer learning and conclusions are inconsistent. in this paper we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in nlp.
recurrent neural network encoder with attention for community question answering
we apply a general recurrent neural network rnn encoder framework to community question answering cqa tasks. our approach does not rely on any linguistic processing and can be applied to different languages or domains. further improvements are observed when we extend the rnn encoders with a neural attention mechanism that encourages reasoning over entire sequences. to deal with practical issues such as data sparsity and imbalanced labels we apply various techniques such as transfer learning and multitask learning. our experiments on the semeval 2016 cqa task show 10 improvement on a map score compared to an information retrieval based approach and achieve comparable performance to a strong handcrafted feature based method.
recursive neural language architecture for tag prediction
we consider the problem of learning distributed representations for tags from their associated content for the task of tag recommendation. considering tagging information is usually very sparse effective learning from content and tag association is very crucial and challenging task. recently various neural representation learning models such as wsabie and its variants show promising performance mainly due to compact feature representations learned in a semantic space. however their capacity is limited by a linear compositional approach for representing tags as sum of equal parts and hurt their performance. in this work we propose a neural feedback relevance model for learning tag representations with weighted feature representations. our experiments on two widely used datasets show significant improvement for quality of recommendations over various baselines.
on the compression of recurrent neural networks with an application to lvcsr acoustic modeling for embedded speech recognition
we study the problem of compressing recurrent neural networks rnns . in particular we focus on the compression of rnn acoustic models which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. in this work we present a technique for general recurrent model compression that jointly compresses both recurrent and non recurrent inter layer weight matrices. we find that the proposed technique allows us to reduce the size of our long short term memory lstm acoustic model to a third of its original size with negligible loss in accuracy.
pointing the unknown words
the problem of rare and unknown words is an important issue that can potentially influence the performance of many nlp systems including both the traditional count based and the deep learning models. we propose a novel way to deal with the rare and unseen words for the neural network models using attention. our model uses two softmax layers in order to predict the next word in conditional language models one predicts the location of a word in the source sentence and the other predicts a word in the shortlist vocabulary. at each time step the decision of which softmax layer to use choose adaptively made by an mlp which is conditioned on the context. we motivate our work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. we observe improvements on two tasks neural machine translation on the europarl english to french parallel corpora and text summarization on the gigaword dataset using our proposed model.
learning multiscale features directly from waveforms
deep learning has dramatically improved the performance of speech recognition systems through learning hierarchies of features optimized for the task at hand. however true end to end learning where features are learned directly from waveforms has only recently reached the performance of hand tailored representations based on the fourier transform. in this paper we detail an approach to use convolutional filters to push past the inherent tradeoff of temporal and frequency resolution that exists for spectral representations. at increased computational cost we show that increasing temporal resolution via reduced stride and increasing frequency resolution via additional filters delivers significant performance improvements. further we find more efficient representations by simultaneously learning at multiple scales leading to an overall decrease in word error rate on a difficult internal speech test set by 20.7 relative to networks with the same number of parameters trained on spectrograms.
joint learning of sentence embeddings for relevance and entailment
we consider the problem of recognizing textual entailment within an information retrieval context where we must simultaneously determine the relevancy as well as degree of entailment for individual pieces of evidence to determine a yes no answer to a binary natural language question. we compare several variants of neural networks for sentence embeddings in a setting of decision making based on evidence of varying relevance. we propose a basic model to integrate evidence for entailment show that joint training of the sentence embeddings to model relevance and entailment is feasible even with no explicit per evidence supervision and show the importance of evaluating strong baselines. we also demonstrate the benefit of carrying over text comprehension model trained on an unrelated task for our small datasets. our research is motivated primarily by a new open dataset we introduce consisting of binary questions and news based evidence snippets. we also apply the proposed relevance entailment model on a similar task of ranking multiple choice test answers evaluating it on a preliminary dataset of school test questions as well as the standard mctest dataset where we improve the neural model state of art.
deep api learning
developers often wonder how to implement a certain functionality e.g. how to parse xml files using apis. obtaining an api usage sequence based on an api related natural language query is very helpful in this regard. given a query existing approaches utilize information retrieval models to search for matching api sequences. these approaches treat queries and apis as bag of words i.e. keyword matching or word to word alignment and lack a deep understanding of the semantics of the query. we propose deepapi a deep learning based approach to generate api usage sequences for a given natural language query. instead of a bags of words assumption it learns the sequence of words in a query and the sequence of associated apis. deepapi adapts a neural language model named rnn encoder decoder. it encodes a word sequence user query into a fixed length context vector and generates an api sequence based on the context vector. we also augment the rnn encoder decoder by considering the importance of individual apis. we empirically evaluate our approach with more than 7 million annotated code snippets collected from github. the results show that our approach generates largely accurate api sequences and outperforms the related approaches.
does multimodality help human and machine for translation and image captioning 
this paper presents the systems developed by lium and cvc for the wmt16 multimodal machine translation challenge. we explored various comparative methods namely phrase based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. we also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. our systems obtained the best results for both tasks according to the automatic evaluation metrics bleu and meteor.
very deep convolutional networks for text classification
the dominant approach for many nlp tasks are recurrent neural networks in particular lstms and convolutional neural networks. however these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state of the art in computer vision. we present a new architecture vdcnn for text processing which operates directly at the character level and uses only small convolutions and pooling operations. we are able to show that the performance of this model increases with depth using up to 29 convolutional layers we report improvements over the state of the art on several public text classification tasks. to the best of our knowledge this is the first time that very deep convolutional nets have been applied to text processing.
improving recurrent neural networks for sequence labelling
in this paper we study different types of recurrent neural networks rnn for sequence labeling tasks. we propose two new variants of rnns integrating improvements for sequence labeling and we compare them to the more traditional elman and jordan rnns. we compare all models either traditional or new on four distinct tasks of sequence labeling two on spoken language understanding atis and media and two of pos tagging for the french treebank ftb and the penn treebank ptb corpora. the results show that our new variants of rnns are always more effective than the others.
sentence similarity measures for fine grained estimation of topical relevance in learner essays
we investigate the task of assessing sentence level prompt relevance in learner essays. various systems using word overlap neural embeddings and neural compositional models are evaluated on two datasets of learner writing. we propose a new method for sentence level similarity calculation which learns to adjust the weights of pre trained word embeddings for a specific task achieving substantially higher accuracy compared to other relevant baselines.
deep cnns along the time axis with intermap pooling for robustness to spectral variations
convolutional neural networks cnns with convolutional and pooling operations along the frequency axis have been proposed to attain invariance to frequency shifts of features. however this is inappropriate with regard to the fact that acoustic features vary in frequency. in this paper we contend that convolution along the time axis is more effective. we also propose the addition of an intermap pooling imp layer to deep cnns. in this layer filters in each group extract common but spectrally variant features then the layer pools the feature maps of each group. as a result the proposed imp cnn can achieve insensitivity to spectral variations characteristic of different speakers and utterances. the effectiveness of the imp cnn architecture is demonstrated on several lvcsr tasks. even without speaker adaptation techniques the architecture achieved a wer of 12.7 on the swb part of the hub5 2000 evaluation test set which is competitive with other state of the art methods.
automatic text scoring using neural networks
automated text scoring ats provides a cost effective and consistent alternative to human marking. however in order to achieve good performance the predictive features of the system need to be manually engineered by human experts. we introduce a model that forms word representations by learning the extent to which specific words contribute to the text s score. using long short term memory networks to represent the meaning of texts we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. in an attempt to make our results more interpretable and inspired by recent advances in visualizing neural networks we introduce a novel method for identifying the regions of the text that the model has found more discriminative.
a comprehensive study of deep bidirectional lstm rnns for acoustic modeling in speech recognition
we present a comprehensive study of deep bidirectional long short term memory lstm recurrent neural network rnn based acoustic models for automatic speech recognition asr . we study the effect of size and depth and train models of up to 8 layers. we investigate the training aspect and study different variants of optimization methods batching truncated backpropagation different regularization techniques such as dropout and l 2 regularization and different gradient clipping variants. the major part of the experimental analysis was performed on the quaero corpus. additional experiments also were performed on the switchboard corpus. our best lstm model has a relative improvement in word error rate of over 14 compared to our best feed forward neural network ffnn baseline on the quaero task. on this task we get our best result with an 8 layer bidirectional lstm and we show that a pretraining scheme with layer wise construction helps for deep lstms. finally we compare the training calculation time of many of the presented experiments in relation with recognition performance. all the experiments were done with returnn the rwth extensible training framework for universal recurrent neural networks in combination with rasr the rwth asr toolkit.
sequence level knowledge distillation
neural machine translation nmt offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. however to reach competitive performance nmt models need to be exceedingly large. in this paper we consider applying knowledge distillation approaches bucila et al. 2006 hinton et al. 2015 that have proven successful for reducing the size of neural models in other domains to the problem of nmt. we demonstrate that standard knowledge distillation applied to word level prediction can be effective for nmt and also introduce two novel sequence level versions of knowledge distillation that further improve performance and somewhat surprisingly seem to eliminate the need for beam search even when applied on the original teacher model . our best student model runs 10 times faster than its state of the art teacher with little loss in performance. it is also significantly better than a baseline model trained without knowledge distillation by 4.2 1.7 bleu with greedy decoding beam search. applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model with a decrease of 0.4 bleu.
learning semantically coherent and reusable kernels in convolution neural nets for sentence classification
the state of the art cnn models give good performance on sentence classification tasks. the purpose of this work is to empirically study desirable properties such as semantic coherence attention mechanism and reusability of cnns in these tasks. semantically coherent kernels are preferable as they are a lot more interpretable for explaining the decision of the learned cnn model. we observe that the learned kernels do not have semantic coherence. motivated by this observation we propose to learn kernels with semantic coherence using clustering scheme combined with word2vec representation and domain knowledge such as sentiwordnet. we suggest a technique to visualize attention mechanism of cnns for decision explanation purpose. reusable property enables kernels learned on one problem to be used in another problem. this helps in efficient learning as only a few additional domain specific filters may have to be learned. we demonstrate the efficacy of our core ideas of learning semantically coherent kernels and leveraging reusable kernels for efficient learning on several benchmark datasets. experimental results show the usefulness of our approach by achieving performance close to the state of the art methods but with semantic and reusable properties.
returnn the rwth extensible training framework for universal recurrent neural networks
in this work we release our extensible and easily configurable neural network training software. it provides a rich set of functional layers with a particular focus on efficient training of recurrent neural network topologies on multiple gpus. the source of the software package is public and freely available for academic research purposes and can be used as a framework or as a standalone tool which supports a flexible configuration. the software allows to train state of the art deep bidirectional long short term memory lstm models on both one dimensional data like speech or two dimensional data like handwritten text and was used to develop successful submission systems in several evaluation campaigns.
character level language modeling with hierarchical recurrent neural networks
recurrent neural network rnn based character level language models clms are extremely useful for modeling out of vocabulary words by nature. however their performance is generally much worse than the word level language models wlms since clms need to consider longer history of tokens to properly predict the next one. we address this problem by proposing hierarchical rnn architectures which consist of multiple modules with different timescales. despite the multi timescale structures the input and output layers operate with the character level clock which allows the existing rnn clm training approaches to be directly applicable without any modifications. our clm models show better perplexity than kneser ney kn 5 gram wlms on the one billion word benchmark with only 2 of parameters. also we present real time character level end to end speech recognition examples on the wall street journal wsj corpus where replacing traditional mono clock rnn clms with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30 .
multi task recurrent model for true multilingual speech recognition
research on multilingual speech recognition remains attractive yet challenging. recent studies focus on learning shared structures under the multi task paradigm in particular a feature sharing structure. this approach has been found effective to improve performance on each individual language. however this approach is only useful when the deployed system supports just one language. in a true multilingual scenario where multiple languages are allowed performance will be significantly reduced due to the competition among languages in the decoding space. this paper presents a multi task recurrent model that involves a multilingual speech recognition asr component and a language recognition lr component and the asr component is informed of the language information by the lr component leading to a language aware recognition. we tested the approach on an english chinese bilingual recognition task. the results show that the proposed multi task recurrent model can improve performance of multilingual recognition systems.
sentiment analysis on bangla and romanized bangla text brbt using deep recurrent models
sentiment analysis sa is an action research area in the digital age. with rapid and constant growth of online social media sites and services and the increasing amount of textual data such as statuses comments reviews etc. available in them application of automatic sa is on the rise. however most of the research works on sa in natural language processing nlp are based on english language. despite being the sixth most widely spoken language in the world bangla still does not have a large and standard dataset. because of this recent research works in bangla have failed to produce results that can be both comparable to works done by others and reusable as stepping stones for future researchers to progress in this field. therefore we first tried to provide a textual dataset that includes not just bangla but romanized bangla texts as well is substantial post processed and multiple validated ready to be used in sa experiments. we tested this dataset in deep recurrent model specifically long short term memory lstm using two types of loss functions binary crossentropy and categorical crossentropy and also did some experimental pre training by using data from one validation to pre train the other and vice versa. lastly we documented the results along with some analysis on them which were promising.
attending to characters in neural sequence labeling models
sequence labeling architectures use word embeddings for capturing similarity but suffer when handling previously unseen or rare words. we investigate character level extensions to such models and propose a novel architecture for combining alternative word representations. by using an attention mechanism the model is able to dynamically decide how much information to use from a word or character level component. we evaluated different architectures on a range of sequence labeling datasets and character level extensions were found to improve performance on every benchmark. in addition the proposed attention based architecture delivered the best results even with a smaller number of trainable parameters.
visualizing and understanding curriculum learning for long short term memory networks
curriculum learning emphasizes the order of training instances in a computational learning setup. the core hypothesis is that simpler instances should be learned early as building blocks to learn more complex ones. despite its usefulness it is still unknown how exactly the internal representation of models are affected by curriculum learning. in this paper we study the effect of curriculum learning on long short term memory lstm networks which have shown strong competency in many natural language processing nlp problems. our experiments on sentiment analysis task and a synthetic task similar to sequence prediction tasks in nlp show that curriculum learning has a positive effect on the lstm s internal states by biasing the model towards building constructive representations i.e. the internal representation at the previous timesteps are used as building blocks for the final prediction. we also find that smaller models significantly improves when they are trained with curriculum learning. lastly we show that curriculum learning helps more when the amount of training data is limited.
dense prediction on sequences with time dilated convolutions for speech recognition
in computer vision pixelwise dense prediction is the task of predicting a label for each pixel in the image. convolutional neural networks achieve good performance on this task while being computationally efficient. in this paper we carry these ideas over to the problem of assigning a sequence of labels to a set of speech frames a task commonly known as framewise classification. we show that dense prediction view of framewise classification offers several advantages and insights including computational efficiency and the ability to apply batch normalization. when doing dense prediction we pay specific attention to strided pooling in time and introduce an asymmetric dilated convolution called time dilated convolution that allows for efficient and elegant implementation of pooling in time. we show results using time dilated convolutions in a very deep vgg style cnn with batch normalization on the hub5 switchboard 2000 benchmark task. with a big n gram language model we achieve 7.7 wer which is the best single model single pass performance reported so far.
end to end asr free keyword search from speech
end to end e2e systems have achieved competitive results compared to conventional hybrid hidden markov model hmm deep neural network based automatic speech recognition asr systems. such e2e systems are attractive due to the lack of dependence on alignments between input acoustic and output grapheme or hmm state sequence during training. this paper explores the design of an asr free end to end system for text query based keyword search kws from speech trained with minimal supervision. our e2e kws system consists of three sub systems. the first sub system is a recurrent neural network rnn based acoustic auto encoder trained to reconstruct the audio through a finite dimensional representation. the second sub system is a character level rnn language model using embeddings learned from a convolutional neural network. since the acoustic and text query embeddings occupy different representation spaces they are input to a third feed forward neural network that predicts whether the query occurs in the acoustic utterance or not. this e2e asr free kws system performs respectably despite lacking a conventional asr system and trains much faster.
training language models using target propagation
while truncated back propagation through time bptt is the most popular approach to training recurrent neural networks rnns it suffers from being inherently sequential making parallelization difficult and from truncating gradient flow between distant time steps. we investigate whether target propagation tprop style approaches can address these shortcomings. unfortunately extensive experiments suggest that tprop generally underperforms bptt and we end with an analysis of this phenomenon and suggestions for future work.
deep voice real time neural text to speech
we present deep voice a production quality text to speech system constructed entirely from deep neural networks. deep voice lays the groundwork for truly end to end neural speech synthesis. the system comprises five major building blocks a segmentation model for locating phoneme boundaries a grapheme to phoneme conversion model a phoneme duration prediction model a fundamental frequency prediction model and an audio synthesis model. for the segmentation model we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification ctc loss. for the audio synthesis model we implement a variant of wavenet that requires fewer parameters and trains faster than the original. by using a neural network for each component our system is simpler and more flexible than traditional text to speech systems where each component requires laborious feature engineering and extensive domain expertise. finally we show that inference with our system can be performed faster than real time and describe optimized wavenet inference kernels on both cpu and gpu that achieve up to 400x speedups over existing implementations.
improved variational autoencoders for text modeling using dilated convolutions
recent work on generative modeling of text has found that variational auto encoders vae incorporating lstm decoders perform worse than simpler lstm language models bowman et al. 2015 . this negative result is so far poorly understood but has been attributed to the propensity of lstm decoders to ignore conditioning information from the encoder. in this paper we experiment with a new type of decoder for vae a dilated cnn. by changing the decoder s dilation architecture we control the effective context from previously generated words. in experiments we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. we show that with the right decoder vae can outperform lstm language models. we demonstrate perplexity gains on two datasets representing the first positive experimental result on the use vae for generative modeling of text. further we conduct an in depth investigation of the use of vae with our new decoding architecture for semi supervised and unsupervised labeling tasks demonstrating gains over several strong baselines.
gram ctc automatic unit selection and target decomposition for sequence labelling
most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. these methods suffer from two major drawbacks 1 the set of basic units is fixed such as the set of words characters or phonemes in speech recognition and 2 the decomposition of target sequences is fixed. these drawbacks usually result in sub optimal performance of modeling sequences. in this pa per we extend the popular ctc loss criterion to alleviate these limitations and propose a new loss function called gram ctc. while preserving the advantages of ctc gram ctc automatically learns the best set of basic units grams as well as the most suitable decomposition of tar get sequences. unlike ctc gram ctc allows the model to output variable number of characters at each time step which enables the model to capture longer term dependency and improves the computational efficiency. we demonstrate that the proposed gram ctc improves ctc in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data and that with gram ctc we can outperform the state of the art on a standard speech benchmark.
ask me even more dynamic memory tensor networks extended model 
we examine memory networks for the task of question answering qa under common real world scenario where training examples are scarce and under weakly supervised scenario that is only extrinsic labels are available for training. we propose extensions for the dynamic memory network dmn specifically within the attention mechanism we call the resulting neural architecture as dynamic memory tensor network dmtn . ultimately we see that our proposed extensions results in over 80 improvement in the number of task passed against the baselined standard dmn and 20 more task passed compared to state of the art end to end memory network for facebook s single task weakly trained 1k babi dataset.
simplified end to end mmi training and voting for asr
a simplified speech recognition system that uses the maximum mutual information mmi criterion is considered. end to end training using gradient descent is suggested similarly to the training of connectionist temporal classification ctc . we use an mmi criterion with a simple language model in the training stage and a standard hmm decoder. our method compares favorably to ctc in terms of performance robustness decoding time disk footprint and quality of alignments. the good alignments enable the use of a straightforward ensemble method obtained by simply averaging the predictions of several neural network models that were trained separately end to end. the ensemble method yields a considerable reduction in the word error rate.
learning to generate reviews and discovering sentiment
we explore the properties of byte level recurrent language models. when given sufficient amounts of capacity training data and compute time the representations learned by these models include disentangled features corresponding to high level concepts. specifically we find a single unit which performs sentiment analysis. these representations learned in an unsupervised manner achieve state of the art on the binary subset of the stanford sentiment treebank. they are also very data efficient. when using only a handful of labeled examples our approach matches the performance of strong baselines trained on full datasets. we also demonstrate the sentiment unit has a direct influence on the generative process of the model. simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.
semi supervised multitask learning for sequence labeling
we propose a sequence labeling framework with a secondary training objective learning to predict surrounding words for every word in the dataset. this language modeling objective incentivises the system to learn general purpose patterns of semantic and syntactic composition which are also useful for improving accuracy on different sequence labeling tasks. the architecture was evaluated on a range of datasets covering the tasks of error detection in learner texts named entity recognition chunking and pos tagging. the novel language modeling objective provided consistent performance improvements on every benchmark without requiring any additional annotated or unannotated data.
going wider recurrent neural network with parallel cells
recurrent neural network rnn has been widely applied for sequence modeling. in rnn the hidden states at current step are full connected to those at previous step thus the influence from less related features at previous step may potentially decrease model s learning ability. we propose a simple technique called parallel cells pcs to enhance the learning ability of recurrent neural network rnn . in each layer we run multiple small rnn cells rather than one single large cell. in this paper we evaluate pcs on 2 tasks. on language modeling task on ptb penn tree bank our model outperforms state of art models by decreasing perplexity from 78.6 to 75.3. on chinese english translation task our model increases bleu score for 0.39 points than baseline model.
phonetic temporal neural model for language identification
deep neural models particularly the lstm rnn model have shown great potential for language identification lid . however the use of phonetic information has been largely overlooked by most existing neural lid methods although this information has been used very successfully in conventional phonetic lid systems. we present a phonetic temporal neural model for lid which is an lstm rnn lid system that accepts phonetic features produced by a phone discriminative dnn as the input rather than raw acoustic features. this new model is similar to traditional phonetic lid methods but the phonetic knowledge here is much richer it is at the frame level and involves compacted information of all phones. our experiments conducted on the babel database and the ap16 olr database demonstrate that the temporal phonetic neural approach is very effective and significantly outperforms existing acoustic neural models. it also outperforms the conventional i vector approach on short utterances and in noisy conditions.
relevance based word embedding
learning a high dimensional dense representation for vocabulary terms also known as a word embedding has recently attracted much attention in natural language processing and information retrieval tasks. the embedding vectors are typically learned based on term proximity in a large corpus. this means that the objective in well known word embedding algorithms e.g. word2vec is to accurately predict adjacent word s for a given word or context. however this objective is not necessarily equivalent to the goal of many information retrieval ir tasks. the primary objective in various ir tasks is to capture relevance instead of term proximity syntactic or even semantic similarity. this is the motivation for developing unsupervised relevance based word embedding models that learn word representations based on query document relevance information. in this paper we propose two learning models with different objective functions one learns a relevance distribution over the vocabulary set for each query and the other classifies each term as belonging to the relevant or non relevant class for each query. to train our models we used over six million unique queries and the top ranked documents retrieved in response to each query which are assumed to be relevant to the query. we extrinsically evaluate our learned word representation models using two ir tasks query expansion and query classification. both query expansion experiments on four trec collections and query classification experiments on the kdd cup 2005 dataset suggest that the relevance based word embedding models significantly outperform state of the art proximity based embedding models such as word2vec and glove.
deriving neural architectures from sequence and graph kernels
the design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. in this work we appeal to kernels over combinatorial structures such as sequences and graphs to derive appropriate neural operations. we introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. our recurrent modules compare the input to virtual reference objects cf. filters in cnn via the kernels. similar to traditional neural operations these reference objects are parameterized and directly optimized in end to end training. we empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression achieving state of the art results across these applications.
on multilingual training of neural dependency parsers
we show that a recently proposed neural dependency parser can be improved by joint training on multiple languages from the same family. the parser is implemented as a deep neural network whose only input is orthographic representations of words. in order to successfully parse the network has to discover how linguistically relevant concepts can be inferred from word spellings. we analyze the representations of characters and words that are learned by the network to establish which properties of languages were accounted for. in particular we show that the parser has approximately learned to associate latin characters with their cyrillic counterparts and that it can group polish and russian words that have a similar grammatical function. finally we evaluate the parser on selected languages from the universal dependencies dataset and show that it is competitive with other recently proposed state of the art methods while having a simple structure.
semi supervised phoneme recognition with recurrent ladder networks
ladder networks are a notable new concept in the field of semi supervised learning by showing state of the art results in image recognition tasks while being compatible with many existing neural architectures. we present the recurrent ladder network a novel modification of the ladder network for semi supervised learning of recurrent neural networks which we evaluate with a phoneme recognition task on the timit corpus. our results show that the model is able to consistently outperform the baseline and achieve fully supervised baseline performance with only 75 of all labels which demonstrates that the model is capable of using unsupervised data as an effective regulariser.
adversarially regularized autoencoders
while autoencoders are a key technique in representation learning for continuous structures such as images or wave forms developing general purpose autoencoders for discrete structures such as text sequence or discretized images has proven to be more challenging. in particular discrete inputs make it more difficult to learn a smooth encoder that preserves the complex local relationships in the input space. in this work we propose an adversarially regularized autoencoder arae with the goal of learning more robust discrete space representations. arae jointly trains both a rich discrete space encoder such as an rnn and a simpler continuous space generator function while using generative adversarial network gan training to constrain the distributions to be similar. this method yields a smoother contracted code space that maps similar inputs to nearby codes and also an implicit latent variable gan model for generation. experiments on text and discretized images demonstrate that the gan model produces clean interpolations and captures the multimodality of the original space and that the autoencoder produces improve ments in semi supervised learning as well as state of the art results in unaligned text style transfer task using only a shared continuous space representation.
auxiliary objectives for neural error detection models
we investigate the utility of different auxiliary objectives and training strategies within a neural sequence labeling approach to error detection in learner writing. auxiliary costs provide the model with additional linguistic information allowing it to learn general purpose compositional features that can then be exploited for other objectives. our experiments show that a joint learning approach trained with parallel labels on in domain data improves performance over the previous best error detection system. while the resulting model has the same number of parameters the additional objectives allow it to be optimised more efficiently and achieve better performance.
an error oriented approach to word embedding pre training
we propose a novel word embedding pre training approach that exploits writing errors in learners scripts. we compare our method to previous models that tune the embeddings based on script scores and the discrimination between correct and corrupt word contexts in addition to the generic commonly used embeddings pre trained on large corpora. the comparison is achieved by using the aforementioned models to bootstrap a neural network that learns to predict a holistic score for scripts. furthermore we investigate augmenting our model with error corrections and monitor the impact on performance. our results show that our error oriented approach outperforms other comparable ones which is further demonstrated when training on more data. additionally extending the model with corrections provides further performance gains when data sparsity is an issue.
a continuous relaxation of beam search for end to end training of neural sequence models
beam search is a desirable choice of test time decoding algorithm for neural sequence models because it potentially avoids search errors made by simpler greedy methods. however typical cross entropy training procedures for these models do not directly consider the behaviour of the final decoding method. as a result for cross entropy trained models beam decoding can sometimes yield reduced test performance when compared with greedy decoding. in order to train models that can more effectively make use of beam search we propose a new training procedure that focuses on the final loss metric e.g. hamming loss evaluated on the output of beam search. while well defined this direct loss objective is itself discontinuous and thus difficult to optimize. hence in our approach we form a sub differentiable surrogate objective by introducing a novel continuous approximation of the beam search decoding procedure. in experiments we show that optimizing this new training objective yields substantially better results on two sequence tasks named entity recognition and ccg supertagging when compared with both cross entropy trained greedy decoding and cross entropy trained beam decoding baselines.
regularizing and optimizing lstm language models
recurrent neural networks rnns such as long short term memory networks lstms serve as a fundamental building block for many sequence learning tasks including machine translation language modeling and question answering. in this paper we consider the specific problem of word level language modeling and investigate strategies for regularizing and optimizing lstm based models. we propose the weight dropped lstm which uses dropconnect on hidden to hidden weights as a form of recurrent regularization. further we introduce nt asgd a variant of the averaged stochastic gradient method wherein the averaging trigger is determined using a non monotonic condition as opposed to being tuned by the user. using these and other regularization strategies we achieve state of the art word level perplexities on two data sets 57.3 on penn treebank and 65.8 on wikitext 2. in exploring the effectiveness of a neural cache in conjunction with our proposed model we achieve an even lower state of the art perplexity of 52.8 on penn treebank and 52.0 on wikitext 2.
supervised speech separation based on deep learning an overview
speech separation is the task of separating target speech from background interference. traditionally speech separation is studied as a signal processing problem. a more recent approach formulates speech separation as a supervised learning problem where the discriminative patterns of speech speakers and background noise are learned from training data. over the past decade many supervised separation algorithms have been put forward. in particular the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. this article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. we first introduce the background of speech separation and the formulation of supervised separation. then we discuss three main components of supervised separation learning machines training targets and acoustic features. much of the overview is on separation algorithms where we review monaural methods including speech enhancement speech nonspeech separation speaker separation multi talker separation and speech dereverberation as well as multi microphone techniques. the important issue of generalization unique to supervised learning is discussed. this overview provides a historical perspective on how advances are made. in addition we discuss a number of conceptual issues including what constitutes the target source.
grasping the finer point a supervised similarity network for metaphor detection
the ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding. yet the majority of metaphor processing systems to date rely on hand engineered features and there is still no consensus in the field as to which features are optimal for this task. in this paper we present the first deep learning architecture designed to capture metaphorical composition. our results demonstrate that it outperforms the existing approaches in the metaphor identification task.
think globally embed locally locally linear meta embedding of words
distributed word embeddings have shown superior performances in numerous natural language processing nlp tasks. however their performances vary significantly across different tasks implying that the word embeddings learnt by those methods capture complementary aspects of lexical semantics. therefore we believe that it is important to combine the existing word embeddings to produce more accurate and complete emph meta embeddings of words. for this purpose we propose an unsupervised locally linear meta embedding learning method that takes pre trained word embeddings as the input and produces more accurate meta embeddings. unlike previously proposed meta embedding learning methods that learn a global projection over all words in a vocabulary our proposed method is sensitive to the differences in local neighbourhoods of the individual source word embeddings. moreover we show that vector concatenation a previously proposed highly competitive baseline approach for integrating word embeddings can be derived as a special case of the proposed method. experimental results on semantic similarity word analogy relation classification and short text classification tasks show that our meta embeddings to significantly outperform prior methods in several benchmark datasets establishing a new state of the art for meta embeddings.
keyvec key semantics preserving document representations
previous studies have demonstrated the empirical success of word embeddings in various applications. in this paper we investigate the problem of learning distributed representations for text documents which many machine learning algorithms take as input for a number of nlp tasks. we propose a neural network model keyvec which learns document representations with the goal of preserving key semantics of the input text. it enables the learned low dimensional vectors to retain the topics and important information from the documents that will flow to downstream tasks. our empirical evaluations show the superior quality of keyvec representations in two different document understanding tasks.
exploring asymmetric encoder decoder structure for context based sentence representation learning
context information plays an important role in human language understanding and it is also useful for machines to learn vector representations of language. in this paper we explore an asymmetric encoder decoder structure for unsupervised context based sentence representation learning. as a result we build an encoder decoder architecture with an rnn encoder and a cnn decoder. we further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. our model is trained on two different large unlabeled corpora and in both cases transferability is evaluated on a set of downstream language understanding tasks. we empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.
cnn is all you need
the convolution neural network cnn has demonstrated the unique advantage in audio image and text learning recently it has also challenged recurrent neural networks rnns with long short term memory cells lstm in sequence to sequence learning since the computations involved in cnn are easily parallelizable whereas those involved in rnn are mostly sequential leading to a performance bottleneck. however unlike rnn the native cnn lacks the history sensitivity required for sequence transformation therefore enhancing the sequential order awareness or position sensitivity becomes the key to make cnn the general deep learning model. in this work we introduce an extended cnn model with strengthen position sensitivity called posenet. a notable feature of posenet is the asymmetric treatment of position information in the encoder and the decoder. experiments shows that posenet allows us to improve the accuracy of cnn based sequence to sequence learning significantly achieving around 33 36 bleu scores on the wmt 2014 english to german translation task and around 44 46 bleu scores on the english to french translation task.
combining representation learning with logic for language processing
the current state of the art in many natural language processing and automated knowledge base completion tasks is held by representation learning methods which learn distributed vector representations of symbols via gradient based optimization. they require little or no hand crafted features thus avoiding the need for most preprocessing steps and task specific assumptions. however in many cases representation learning requires a large amount of annotated training data to generalize well to unseen data. such labeled training data is provided by human annotators who often use formal logic as the language for specifying annotations. this thesis investigates different combinations of representation learning methods with logic for reducing the need for annotated training data and for improving generalization.
a note on topology preservation in classification and the construction of a universal neuron grid
it will be shown that according to theorems of k. menger every neuron grid if identified with a curve is able to preserve the adopted qualitative structure of a data space. furthermore if this identification is made the neuron grid structure can always be mapped to a subset of a universal neuron grid which is constructable in three space dimensions. conclusions will be drawn for established neuron grid types as well as neural fields.
linear nonlinear poisson neuron networks perform bayesian inference on boltzmann machines
one conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back end. however to our knowledge a detailed correspondence has not yet been set up which is important if we want to bridge between neuroscience and machine learning. recent researches emphasized the biological plausibility of linear nonlinear poisson lnp neuron model. we show that with neurally plausible settings the whole network is capable of representing any boltzmann machine and performing a semi stochastic bayesian inference algorithm lying between gibbs sampling and variational inference.
mapping temporal variables into the neucube for improved pattern recognition predictive modelling and understanding of stream data
this paper proposes a new method for an optimized mapping of temporal variables describing a temporal stream data into the recently proposed neucube spiking neural network architecture. this optimized mapping extends the use of the neucube which was initially designed for spatiotemporal brain data to work on arbitrary stream data and to achieve a better accuracy of temporal pattern recognition a better and earlier event prediction and a better understanding of complex temporal stream data through visualization of the neucube connectivity. the effect of the new mapping is demonstrated on three bench mark problems. the first one is early prediction of patient sleep stage event from temporal physiological data. the second one is pattern recognition of dynamic temporal patterns of traffic in the bay area of california and the last one is the challenge 2012 contest data set. in all cases the use of the proposed mapping leads to an improved accuracy of pattern recognition and event prediction and a better understanding of the data when compared to traditional machine learning techniques or spiking neural network reservoirs with arbitrary mapping of the variables.
an evolutionary algorithm to learn sparql queries for source target pairs finding patterns for human associations in dbpedia
efficient usage of the knowledge provided by the linked data community is often hindered by the need for domain experts to formulate the right sparql queries to answer questions. for new questions they have to decide which datasets are suitable and in which terminology and modelling style to phrase the sparql query. in this work we present an evolutionary algorithm to help with this challenging task. given a training list of source target node pair examples our algorithm can learn patterns sparql queries from a sparql endpoint. the learned patterns can be visualised to form the basis for further investigation or they can be used to predict target nodes for new source nodes. amongst others we apply our algorithm to a dataset of several hundred human associations such as circle square to find patterns for them in dbpedia. we show the scalability of the algorithm by running it against a sparql endpoint loaded with 7.9 billion triples. further we use the resulting sparql queries to mimic human associations with a mean average precision map of 39.9 and a recall 10 of 63.9 .
a geometric framework for convolutional neural networks
in this paper a geometric framework for neural networks is proposed. this framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component based form but in a coordinate free manner. convolutional neural networks are described in this framework in a compact form with the gradients of standard and higher order loss functions calculated for each layer of the network. this approach can be applied to other network structures and provides a basis on which to create new networks.
a novel representation of neural networks
deep neural networks dnns have become very popular for prediction in many areas. their strength is in representation with a high number of parameters that are commonly learned via gradient descent or similar optimization methods. however the representation is non standardized and the gradient calculation methods are often performed using component based approaches that break parameters down into scalar units instead of considering the parameters as whole entities. in this work these problems are addressed. standard notation is used to represent dnns in a compact framework. gradients of dnn loss functions are calculated directly over the inner product space on which the parameters are defined. this framework is general and is applied to two common network types the multilayer perceptron and the deep autoencoder.
converting cascade correlation neural nets into probabilistic generative models
humans are not only adept in recognizing what class an input instance belongs to i.e. classification task but perhaps more remarkably they can imagine i.e. generate plausible instances of a desired class with ease when prompted. inspired by this we propose a framework which allows transforming cascade correlation neural networks ccnns into probabilistic generative models thereby enabling ccnns to generate samples from a category of interest. ccnns are a well known class of deterministic discriminative nns which autonomously construct their topology and have been successful in giving accounts for a variety of psychological phenomena. our proposed framework is based on a markov chain monte carlo mcmc method called the metropolis adjusted langevin algorithm which capitalizes on the gradient information of the target distribution to direct its explorations towards regions of high probability thereby achieving good mixing properties. through extensive simulations we demonstrate the efficacy of our proposed framework.
on the performance of network parallel training in artificial neural networks
artificial neural networks anns have received increasing attention in recent years with applications that span a wide range of disciplines including vital domains such as medicine network security and autonomous transportation. however neural network architectures are becoming increasingly complex and with an increasing need to obtain real time results from such models it has become pivotal to use parallelization as a mechanism for speeding up network training and deployment. in this work we propose an implementation of network parallel training through cannon s algorithm for matrix multiplication. we show that increasing the number of processes speeds up training until the point where process communication costs become prohibitive this point varies by network complexity. we also show through empirical efficiency calculations that the speedup obtained is superlinear.
programmable agents
we build deep rl agents that execute declarative programs expressed in formal language. the agents learn to ground the terms in this language in their environment and can generalize their behavior at test time to execute new programs that refer to objects that were not referenced during training. the agents develop disentangled interpretable representations that allow them to generalize to a wide variety of zero shot semantic tasks.
explainable artificial intelligence understanding visualizing and interpreting deep learning models
with the availability of large databases and recent improvements in deep learning methodology the performance of ai systems is reaching or even exceeding the human level on an increasing number of complex tasks. impressive examples of this development can be found in domains such as image classification sentiment analysis speech understanding or strategic game playing. however because of their nested non linear structure these highly successful machine learning and artificial intelligence models are usually applied in a black box manner i.e. no information is provided about what exactly makes them arrive at their predictions. since this lack of transparency can be a major drawback e.g. in medical applications the development of methods for visualizing explaining and interpreting deep learning models has recently attracted increasing attention. this paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. furthermore it presents two approaches to explaining predictions of deep learning models one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. these methods are evaluated on three classification tasks.
can deep reinforcement learning solve erdos selfridge spencer games 
deep reinforcement learning has achieved many recent successes but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior and correspondingly diagnose individual actions against such a characterization. here we consider a family of combinatorial games arising from work of erdos selfridge and spencer and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. these games have a number of appealing features they are challenging for current learning approaches but they form i a low dimensional simply parametrized environment where ii there is a linear closed form solution for optimal behavior from any state and iii the difficulty of the game can be tuned by changing environment parameters in an interpretable way. we use these erdos selfridge spencer games not only to compare different algorithms but test for generalization make comparisons to supervised learning analyse multiagent play and even develop a self play algorithm.
emergence of grid like representations by training recurrent neural networks to perform spatial localization
decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. the entorhinal cortex ec of the mammalian brain contains a rich set of spatial correlates including grid cells which encode space using tessellating patterns. however the mechanisms and functional significance of these spatial representations remain largely mysterious. as a new way to understand these neural representations we trained recurrent neural networks rnns to perform navigation tasks in 2d arenas based on velocity inputs. surprisingly we find that grid like spatial response patterns emerge in trained networks along with units that exhibit other spatial correlates including border cells and band like cells. all these different functional types of neurons have been observed experimentally. the order of the emergence of grid like and border cells is also consistent with observations from developmental studies. together our results suggest that grid cells border cells and others as observed in ec may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits.
dimensionality reduction and reconstruction using mirroring neural networks and object recognition based on reduced dimension characteristic vector
in this paper we present a mirroring neural network architecture to perform non linear dimensionality reduction and object recognition using a reduced lowdimensional characteristic vector. in addition to dimensionality reduction the network also reconstructs mirrors the original high dimensional input vector from the reduced low dimensional data. the mirroring neural network architecture has more number of processing elements adalines in the outer layers and the least number of elements in the central layer to form a converging diverging shape in its configuration. since this network is able to reconstruct the original image from the output of the innermost layer which contains all the information about the input pattern these outputs can be used as object signature to classify patterns. the network is trained to minimize the discrepancy between actual output and the input by back propagating the mean squared error from the output layer to the input layer. after successfully training the network it can reduce the dimension of input vectors and mirror the patterns fed to it. the mirroring neural network architecture gave very good results on various test patterns.
parcellation of fmri datasets with ica and pls a data driven approach
inter subject parcellation of functional magnetic resonance imaging fmri data based on a standard general linear model glm and spectral clustering was recently proposed as a means to alleviate the issues associated with spatial normalization in fmri. however for all its appeal a glm based parcellation approach introduces its own biases in the form of a priori knowledge about the shape of hemodynamic response function hrf and task related signal changes or about the subject behaviour during the task. in this paper we introduce a data driven version of the spectral clustering parcellation based on independent component analysis ica and partial least squares pls instead of the glm. first a number of independent components are automatically selected. seed voxels are then obtained from the associated ica maps and we compute the pls latent variables between the fmri signal of the seed voxels which covers regional variations of the hrf and the principal components of the signal across all voxels. finally we parcellate all subjects data with a spectral clustering of the pls latent variables. we present results of the application of the proposed method on both single subject and multi subject fmri datasets. preliminary experimental results evaluated with intra parcel variance of glm t values and pls derived t values indicate that this data driven approach offers improvement in terms of parcellation accuracy over glm based techniques.
iris codes classification using discriminant and witness directions
the main topic discussed in this paper is how to use intelligence for biometric decision defuzzification. a neural training model is proposed and tested here as a possible solution for dealing with natural fuzzification that appears between the intra and inter class distribution of scores computed during iris recognition tests. it is shown here that the use of proposed neural network support leads to an improvement in the artificial perception of the separation between the intra and inter class score distributions by moving them away from each other.
algorithms for image analysis and combination of pattern classifiers with application to medical diagnosis
medical informatics and the application of modern signal processing in the assistance of the diagnostic process in medical imaging is one of the more recent and active research areas today. this thesis addresses a variety of issues related to the general problem of medical image analysis specifically in mammography and presents a series of algorithms and design approaches for all the intermediate levels of a modern system for computer aided diagnosis cad . the diagnostic problem is analyzed with a systematic approach first defining the imaging characteristics and features that are relevant to probable pathology in mammo grams. next these features are quantified and fused into new integrated radio logical systems that exhibit embedded digital signal processing in order to improve the final result and minimize the radiological dose for the patient. in a higher level special algorithms are designed for detecting and encoding these clinically interest ing imaging features in order to be used as input to advanced pattern classifiers and machine learning models. finally these approaches are extended in multi classifier models under the scope of game theory and optimum collective deci sion in order to produce efficient solutions for combining classifiers with minimum computational costs for advanced diagnostic systems. the material covered in this thesis is related to a total of 18 published papers 6 in scientific journals and 12 in international conferences.
deep neural networks are easily fooled high confidence predictions for unrecognizable images
deep neural networks dnns have recently been achieving state of the art performance on a variety of pattern recognition tasks most notably visual classification problems. given that dnns are now able to classify objects in images with near human level performance questions naturally arise as to what differences remain between computer and human vision. a recent study revealed that changing an image e.g. of a lion in a way imperceptible to humans can cause a dnn to label the image as something else entirely e.g. mislabeling a lion a library . here we show a related result it is easy to produce images that are completely unrecognizable to humans but that state of the art dnns believe to be recognizable objects with 99.99 confidence e.g. labeling with certainty that white noise static is a lion . specifically we take convolutional neural networks trained to perform well on either the imagenet or mnist datasets and then find images with evolutionary algorithms or gradient ascent that dnns label with high confidence as belonging to each dataset class. it is possible to produce images totally unrecognizable to human eyes that dnns believe with near certainty are familiar objects which we call fooling images more generally fooling examples . our results shed light on interesting differences between human vision and current dnns and raise questions about the generality of dnn computer vision.
homogeneous spiking neuromorphic system for real world pattern recognition
a neuromorphic chip that combines cmos analog spiking neurons and memristive synapses offers a promising solution to brain inspired computing as it can provide massive neural network parallelism and density. previous hybrid analog cmos memristor approaches required extensive cmos circuitry for training and thus eliminated most of the density advantages gained by the adoption of memristor synapses. further they used different waveforms for pre and post synaptic spikes that added undesirable circuit overhead. here we describe a hardware architecture that can feature a large number of memristor synapses to learn real world patterns. we present a versatile cmos neuron that combines integrate and fire behavior drives passive memristors and implements competitive learning in a compact circuit module and enables in situ plasticity in the memristor synapses. we demonstrate handwritten digits recognition using the proposed architecture using transistor level circuit simulations. as the described neuromorphic architecture is homogeneous it realizes a fundamental building block for large scale energy efficient brain inspired silicon chips that could lead to next generation cognitive computing.
crowd behavior analysis a review where physics meets biology
although the traits emerged in a mass gathering are often non deliberative the act of mass impulse may lead to irre vocable crowd disasters. the two fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision towards effective and proactive crowd surveillance. computer vision stud ies related to crowd are observed to resonate with the understanding of the emergent behavior in physics complex systems and biology animal swarm . these studies which are inspired by biology and physics share surprisingly common insights and interesting contradictions. however this aspect of discussion has not been fully explored. therefore this survey provides the readers with a review of the state of the art methods in crowd behavior analysis from the physics and biologically inspired perspectives. we provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision.
can pretrained neural networks detect anatomy 
convolutional neural networks demonstrated outstanding empirical results in computer vision and speech recognition tasks where labeled training data is abundant. in medical imaging there is a huge variety of possible imaging modalities and contrasts where annotated data is usually very scarce. we present two approaches to deal with this challenge. a network pretrained in a different domain with abundant data is used as a feature extractor while a subsequent classifier is trained on a small target dataset and a deep architecture trained with heavy augmentation and equipped with sophisticated regularization methods. we test the approaches on a corpus of x ray images to design an anatomy detection system.
metaheuristic algorithms for convolution neural network
a typical modern optimization technique is usually either heuristic or metaheuristic. this technique has managed to solve some optimization problems in the research area of science engineering and industry. however implementation strategy of metaheuristic for accuracy improvement on convolution neural networks cnn a famous deep learning method is still rarely investigated. deep learning relates to a type of machine learning technique where its aim is to move closer to the goal of artificial intelligence of creating a machine that could successfully perform any intellectual tasks that can be carried out by a human. in this paper we propose the implementation strategy of three popular metaheuristic approaches that is simulated annealing differential evolution and harmony search to optimize cnn. the performances of these metaheuristic methods in optimizing cnn on classifying mnist and cifar dataset were evaluated and compared. furthermore the proposed methods are also compared with the original cnn. although the proposed methods show an increase in the computation time their accuracy has also been improved up to 7.14 percent .
hadamard product for low rank bilinear pooling
bilinear models provide rich representations compared with linear models. they have been applied in various visual tasks such as object recognition segmentation and visual question answering to get state of the art performances taking advantage of the expanded representations. however bilinear representations tend to be high dimensional limiting the applicability to computationally complex tasks. we propose low rank bilinear pooling using hadamard product for an efficient attention mechanism of multimodal learning. we show that our model outperforms compact bilinear pooling in visual question answering tasks with the state of the art results on the vqa dataset having a better parsimonious property.
incremental network quantization towards lossless cnns with low precision weights
this paper presents incremental network quantization inq a novel method targeting to efficiently convert any pre trained full precision convolutional neural network cnn model into a low precision version whose weights are constrained to be either powers of two or zero. unlike existing methods which are struggled in noticeable accuracy loss our inq has the potential to resolve this issue as benefiting from two innovations. on one hand we introduce three interdependent operations namely weight partition group wise quantization and re training. a well proven measure is employed to divide the weights in each layer of a pre trained cnn model into two disjoint groups. the weights in the first group are responsible to form a low precision base thus they are quantized by a variable length encoding method. the weights in the other group are responsible to compensate for the accuracy loss from the quantization thus they are the ones to be re trained. on the other hand these three operations are repeated on the latest re trained group in an iterative manner until all the weights are converted into low precision ones acting as an incremental network quantization and accuracy enhancement procedure. extensive experiments on the imagenet classification task using almost all known deep cnn architectures including alexnet vgg 16 googlenet and resnets well testify the efficacy of the proposed method. specifically at 5 bit quantization our models have improved accuracy than the 32 bit floating point references. taking resnet 18 as an example we further show that our quantized models with 4 bit 3 bit and 2 bit ternary weights have improved or very similar accuracy against its 32 bit floating point baseline. besides impressive results with the combination of network pruning and inq are also reported. the code is available at https github.com zhouaojun incremental network quantization.
lesionseg semantic segmentation of skin lesions using deep convolutional neural network
we present a method for skin lesion segmentation for the isic 2017 skin lesion segmentation challenge. our approach is based on a fully convolutional network architecture which is trained end to end from scratch on a limited dataset. our semantic segmentation architecture utilizes several recent innovations in particularly in the combined use of i use of atrous convolutions to increase the effective field of view of the network s receptive field without increasing the number of parameters ii the use of network in network 1 times1 convolution layers to add capacity to the network and iii state of art super resolution upsampling of predictions using subpixel cnn layers. we reported a mean iou score of 0.642 on the validation set provided by the organisers.
convolutional spike timing dependent plasticity based feature learning in spiking neural networks
brain inspired learning models attempt to mimic the cortical architecture and computations performed in the neurons and synapses constituting the human brain to achieve its efficiency in cognitive tasks. in this work we present convolutional spike timing dependent plasticity based feature learning with biologically plausible leaky integrate and fire neurons in spiking neural networks snns . we use shared weight kernels that are trained to encode representative features underlying the input patterns thereby improving the sparsity as well as the robustness of the learning model. we demonstrate that the proposed unsupervised learning methodology learns several visual categories for object recognition with fewer number of examples and outperforms traditional fully connected snn architectures while yielding competitive accuracy. additionally we observe that the learning model performs out of set generalization further making the proposed biologically plausible framework a viable and efficient architecture for future neuromorphic applications.
adversarial transformation networks learning to generate adversarial examples
multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. these approaches involve either directly computing gradients with respect to the image pixels or directly solving an optimization on the image pixels. in this work we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. we efficiently train feed forward neural networks in a self supervised manner to generate adversarial examples against a target network or set of networks. we call such a network an adversarial transformation network atn . atns are trained to generate adversarial examples that minimally modify the classifier s outputs given the original input while constraining the new classification to match an adversarial target class. we present methods to train atns and analyze their effectiveness targeting a variety of mnist classifiers as well as the latest state of the art imagenet classifier inception resnet v2.
opening the black box of financial ai with clear trade a class enhanced attentive response approach for explaining and visualizing deep learning driven stock market prediction
deep learning has been shown to outperform traditional machine learning algorithms across a wide range of problem domains. however current deep learning algorithms have been criticized as uninterpretable black boxes which cannot explain their decision making processes. this is a major shortcoming that prevents the widespread application of deep learning to domains with regulatory processes such as finance. as such industries such as finance have to rely on traditional models like decision trees that are much more interpretable but less effective than deep learning for complex problems. in this paper we propose clear trade a novel financial ai visualization framework for deep learning driven stock market prediction that mitigates the interpretability issue of deep learning methods. in particular clear trade provides a effective way to visualize and explain decisions made by deep stock market prediction models. we show the efficacy of clear trade in enhancing the interpretability of stock market prediction by conducting experiments based on s p 500 stock index prediction. the results demonstrate that clear trade can provide significant insight into the decision making process of deep learning driven financial models particularly for regulatory processes thus improving their potential uptake in the financial industry.
fast yolo a fast you only look once system for real time embedded object detection in video
object detection is considered one of the most challenging problems in this field of computer vision as it involves the combination of object classification and object localization within a scene. recently deep neural networks dnns have been demonstrated to achieve superior object detection performance compared to other approaches with yolov2 an improved you only look once model being one of the state of the art in dnn based object detection methods in terms of both speed and accuracy. although yolov2 can achieve real time performance on a powerful gpu it still remains very challenging for leveraging this approach for real time object detection in video on embedded computing devices with limited computational power and limited memory. in this paper we propose a new framework called fast yolo a fast you only look once framework which accelerates yolov2 to be able to perform object detection in video on embedded devices in a real time manner. first we leverage the evolutionary deep intelligence framework to evolve the yolov2 network architecture and produce an optimized architecture referred to as o yolov2 here that has 2.8x fewer parameters with just a 2 iou drop. to further reduce power consumption on embedded devices while maintaining performance a motion adaptive inference method is introduced into the proposed fast yolo framework to reduce the frequency of deep inference with o yolov2 based on temporal motion characteristics. experimental results show that the proposed fast yolo framework can reduce the number of deep inferences by an average of 38.13 and an average speedup of 3.3x for objection detection in video compared to the original yolov2 leading fast yolo to run an average of 18fps on a nvidia jetson tx1 embedded system.
nest a neural network synthesis tool based on a grow and prune paradigm
neural networks nns have begun to have a pervasive impact on various applications of machine learning. however the problem of finding an optimal nn architecture for large applications has remained open for several decades. conventional approaches search for the optimal nn architecture through extensive trial and error. such a procedure is quite inefficient. in addition the generated nn architectures incur substantial redundancy. to address these problems we propose an nn synthesis tool nest that automatically generates very compact architectures for a given dataset. nest starts with a seed nn architecture. it iteratively tunes the architecture with gradient based growth and magnitude based pruning of neurons and connections. our experimental results show that nest yields accurate yet very compact nns with a wide range of seed architecture selection. for example for the lenet 300 100 lenet 5 nn architecture derived from the mnist dataset we reduce network parameters by 34.1x 74.3x and floating point operations flops by 35.8x 43.7x . for the alexnet nn architecture derived from the imagenet dataset we reduce network parameters by 15.7x and flops by 4.6x. all these results are the current state of the art for these architectures.
analysis of supervised and semi supervised growcut applied to segmentation of masses in mammography images
breast cancer is already one of the most common form of cancer worldwide. mammography image analysis is still the most effective diagnostic method to promote the early detection of breast cancer. accurately segmenting tumors in digital mammography images is important to improve diagnosis capabilities of health specialists and avoid misdiagnosis. in this work we evaluate the feasibility of applying growcut to segment regions of tumor and we propose two growcut semi supervised versions. all the analysis was performed by evaluating the application of segmentation techniques to a set of images obtained from the mini mias mammography image database. growcut segmentation was compared to region growing active contours random walks and graph cut techniques. experiments showed that growcut when compared to the other techniques was able to acquire better results for the metrics analyzed. moreover the proposed semi supervised versions of growcut was proved to have a clinically satisfactory quality of segmentation.
empirical explorations in training networks with discrete activations
we present extensive experiments training and testing hidden units in deep networks that emit only a predefined static number of discretized values. these units provide benefits in real world deployment in systems in which memory and or computation may be limited. additionally they are particularly well suited for use in large recurrent network models that require the maintenance of large amounts of internal state in memory. surprisingly we find that despite reducing the number of values that can be represented in the output activations from 2 32 2 64 to between 64 and 256 there is little to no degradation in network performance across a variety of different settings. we investigate simple classification and regression tasks as well as memorization and compression problems. we compare the results with more standard activations such as tanh and relu. unlike previous discretization studies which often concentrate only on binary units we examine the effects of varying the number of allowed activation levels. compared to existing approaches for discretization the approach presented here is both conceptually and programatically simple has no stochastic component and allows the training testing and usage phases to be treated in exactly the same manner.
regularized evolution for image classifier architecture search
the effort devoted to hand crafting image classifiers has motivated the use of architecture search to discover them automatically. reinforcement learning and evolution have both shown promise for this purpose. this study employs a regularized version of a popular asynchronous evolutionary algorithm. we rigorously compare it to the non regularized form and to a highly successful reinforcement learning baseline. using the same hardware compute effort and neural network training code we conduct repeated experiments side by side exploring different datasets search spaces and scales. we show regularized evolution consistently produces models with similar or higher accuracy across a variety of contexts without need for re tuning parameters. in addition evolution exhibits considerably better performance than reinforcement learning at early search stages suggesting it may be the better choice when fewer compute resources are available. this constitutes the first controlled comparison of the two search algorithms in this context. finally we present new architectures discovered with evolution that we nickname amoebanets. these models achieve state of the art results for cifar 10 mean test error 2.13 mobile size imagenet top 1 accuracy 75.1 with 5.1 m parameters and imagenet top 1 accuracy 83.1 . this is the first time evolutionary algorithms produce state of the art image classifiers.
tiny ssd a tiny single shot detection deep convolutional neural network for real time embedded object detection
object detection is a major challenge in computer vision involving both object classification and object localization within a scene. while deep neural networks have been shown in recent years to yield very powerful techniques for tackling the challenge of object detection one of the biggest challenges with enabling such object detection networks for widespread deployment on embedded devices is high computational and memory requirements. recently there has been an increasing focus in exploring small deep neural network architectures for object detection that are more suitable for embedded devices such as tiny yolo and squeezedet. inspired by the efficiency of the fire microarchitecture introduced in squeezenet and the object detection performance of the single shot detection macroarchitecture introduced in ssd this paper introduces tiny ssd a single shot detection deep convolutional neural network for real time embedded object detection that is composed of a highly optimized non uniform fire sub network stack and a non uniform sub network stack of highly optimized ssd based auxiliary convolutional feature layers designed specifically to minimize model size while maintaining object detection performance. the resulting tiny ssd possess a model size of 2.3mb 26x smaller than tiny yolo while still achieving an map of 61.3 on voc 2007 4.2 higher than tiny yolo . these experimental results show that very small deep neural network architectures can be designed for real time object detection that are well suited for embedded scenarios.
inferencing based on unsupervised learning of disentangled representations
combining generative adversarial networks gans with encoders that learn to encode data points has shown promising results in learning data representations in an unsupervised way. we propose a framework that combines an encoder and a generator to learn disentangled representations which encode meaningful information about the data distribution without the need for any labels. while current approaches focus mostly on the generative aspects of gans our framework can be used to perform inference on both real and generated data points. experiments on several data sets show that the encoder learns interpretable disentangled representations which encode descriptive properties and can be used to sample images that exhibit specific characteristics.
the parameter less self organizing map algorithm
the parameter less self organizing map plsom is a new neural network algorithm based on the self organizing map som . it eliminates the need for a learning rate and annealing schemes for learning rate and neighbourhood size. we discuss the relative performance of the plsom and the som and demonstrate some tasks in which the som fails but the plsom performs satisfactory. finally we discuss some example applications of the plsom and present a proof of ordering under certain limited conditions.
simplified firefly algorithm for 2d image key points search
in order to identify an object human eyes firstly search the field of view for points or areas which have particular properties. these properties are used to recognise an image or an object. then this process could be taken as a model to develop computer algorithms for images identification. this paper proposes the idea of applying the simplified firefly algorithm to search for key areas in 2d images. for a set of input test images the proposed version of firefly algorithm has been examined. research results are presented and discussed to show the efficiency of this evolutionary computation method.
deep plant plant identification with convolutional neural networks
this paper studies convolutional neural networks cnn to learn unsupervised feature representations for 44 different plant species collected at the royal botanic gardens kew england. to gain intuition on the chosen features from the cnn model opposed to a black box solution a visualisation technique based on the deconvolutional networks dn is utilized. it is found that venations of different order have been chosen to uniquely represent each of the plant species. experimental results using these cnn features with different classifiers show consistency and superiority compared to the state of the art solutions which rely on hand crafted features.
adapting deep network features to capture psychological representations
deep neural networks have become increasingly successful at solving classic perception problems such as object recognition semantic segmentation and scene understanding often reaching or surpassing human level accuracy. this success is due in part to the ability of dnns to learn useful representations of high dimensional inputs a problem that humans must also solve. we examine the relationship between the representations learned by these networks and human psychological representations recovered from similarity judgments. we find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. however these features do not capture some qualitative distinctions that are a key part of human representations. to remedy this we develop a method for adapting deep features to align with human similarity judgments resulting in image representations that can potentially be used to extend the scope of psychological experiments.
large scale evolution of image classifiers
neural networks have proven effective at solving difficult problems but designing their architectures can be challenging even for image classification problems alone. our goal is to minimize human participation so we employ evolutionary algorithms to discover such networks automatically. despite significant computational requirements we show that it is now possible to evolve models with accuracies within the range of those published in the last year. specifically we employ simple evolutionary techniques at unprecedented scales to discover models for the cifar 10 and cifar 100 datasets starting from trivial initial conditions and reaching accuracies of 94.6 95.6 for ensemble and 77.0 respectively. to do this we use novel and intuitive mutation operators that navigate large search spaces we stress that no human participation is required once evolution starts and that the output is a fully trained model. throughout this work we place special emphasis on the repeatability of results the variability in the outcomes and the computational requirements.
a compact dnn approaching googlenet level accuracy of classification and domain adaptation
recently dnn model compression based on network architecture design e.g. squeezenet attracted a lot attention. no accuracy drop on image classification is observed on these extremely compact networks compared to well known models. an emerging question however is whether these model compression techniques hurt dnn s learning ability other than classifying images on a single dataset. our preliminary experiment shows that these compression methods could degrade domain adaptation da ability though the classification performance is preserved. therefore we propose a new compact network architecture and unsupervised da method in this paper. the dnn is built on a new basic module conv m which provides more diverse feature extractors without significantly increasing parameters. the unified framework of our da method will simultaneously learn invariance across domains reduce divergence of feature representations and adapt label prediction. our dnn has 4.1m parameters which is only 6.7 of alexnet or 59 of googlenet. experiments show that our dnn obtains googlenet level accuracy both on classification and da and our da method slightly outperforms previous competitive ones. put all together our da strategy based on our dnn achieves state of the art on sixteen of total eighteen da tasks on popular office 31 and office caltech datasets.
identifying spatial relations in images using convolutional neural networks
traditional approaches to building a large scale knowledge graph have usually relied on extracting information entities their properties and relations between them from unstructured text e.g. dbpedia . recent advances in convolutional neural networks cnn allow us to shift our focus to learning entities and relations from images as they build robust models that require little or no pre processing of the images. in this paper we present an approach to identify and extract spatial relations e.g. the girl is standing behind the table from images using cnns. our research addresses two specific challenges providing insight into how spatial relations are learned by the network and which parts of the image are used to predict these relations. we use the pre trained network vggnet to extract features from an image and train a multi layer perceptron mlp on a set of synthetic images and the sun09 dataset to extract spatial relations. the mlp predicts spatial relations without a bounding box around the objects or the space in the image depicting the relation. to understand how the spatial relations are represented in the network a heatmap is overlayed on the image to show the regions that are deemed important by the network. also we analyze the mlp to show the relationship between the activation of consistent groups of nodes and the prediction of a spatial relation. we show how the loss of these groups affects the networks ability to identify relations.
hierarchical attentive recurrent tracking
class agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. inspired by how the human visual cortex employs spatial attention and separate where and what processing pathways to actively suppress irrelevant visual features this work develops a hierarchical attentive recurrent model for single object tracking in videos. the first layer of attention discards the majority of background by selecting a region containing the object of interest while the subsequent layers tune in on visual features particular to the tracked object. this framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. to improve training convergence we augment the loss function with terms for a number of auxiliary tasks relevant for tracking. evaluation of the proposed model is performed on two datasets pedestrian tracking on the kth activity recognition dataset and the more difficult kitti object tracking dataset.
psique next sequence prediction of satellite images using a convolutional sequence to sequence network
predicting unseen weather phenomena is an important issue for disaster management. in this paper we suggest a model for a convolutional sequence to sequence autoencoder for predicting undiscovered weather situations from previous satellite images. we also propose a symmetric skip connection between encoder and decoder modules to produce more comprehensive image predictions. to examine our model performance we conducted experiments for each suggested model to predict future satellite images from historical satellite images. a specific combination of skip connection and sequence to sequence autoencoder was able to generate closest prediction from the ground truth image.
evaluation of alzheimer s disease by analysis of mr images using multilayer perceptrons and kohonen som classifiers as an alternative to the adc maps
alzheimer s disease is the most common cause of dementia yet hard to diagnose precisely without invasive techniques particularly at the onset of the disease. this work approaches image analysis and classification of synthetic multispectral images composed by diffusion weighted magnetic resonance mr cerebral images for the evaluation of cerebrospinal fluid area and measuring the advance of alzheimer s disease. a clinical 1.5 t mr imaging system was used to acquire all images presented. the classification methods are based on multilayer perceptrons and kohonen self organized map classifiers. we assume the classes of interest can be separated by hyperquadrics. therefore a 2 degree polynomial network is used to classify the original image generating the ground truth image. the classification results are used to improve the usual analysis of the apparent diffusion coefficient map.
neural tuning size is a key factor underlying holistic face processing
faces are a class of visual stimuli with unique significance for a variety of reasons. they are ubiquitous throughout the course of a person s life and face recognition is crucial for daily social interaction. faces are also unlike any other stimulus class in terms of certain physical stimulus characteristics. furthermore faces have been empirically found to elicit certain characteristic behavioral phenomena which are widely held to be evidence of holistic processing of faces. however little is known about the neural mechanisms underlying such holistic face processing. in other words for the processing of faces by the primate visual system the input and output characteristics are relatively well known but the internal neural computations are not. the main aim of this work is to further the fundamental understanding of what causes the visual processing of faces to be different from that of objects. in this computational modeling work we show that a single factor neural tuning size is able to account for three key phenomena that are characteristic of face processing namely the composite face effect cfe face inversion effect fie and whole part effect wpe . our computational proof of principle provides specific neural tuning properties that correspond to the poorly understood notion of holistic face processing and connects these neural properties to psychophysical behavior. overall our work provides a unified and parsimonious theoretical account for the disparate empirical data on face specific processing deepening the fundamental understanding of face processing.
distribution of the search of evolutionary product unit neural networks for classification
this paper deals with the distributed processing in the search for an optimum classification model using evolutionary product unit neural networks. for this distributed search we used a cluster of computers. our objective is to obtain a more efficient design than those net architectures which do not use a distributed process and which thus result in simpler designs. in order to get the best classification models we use evolutionary algorithms to train and design neural networks which require a very time consuming computation. the reasons behind the need for this distribution are various. it is complicated to train this type of nets because of the difficulty entailed in determining their architecture due to the complex error surface. on the other hand the use of evolutionary algorithms involves running a great number of tests with different seeds and parameters thus resulting in a high computational cost
correlation alignment for unsupervised domain adaptation
in this chapter we present correlation alignment coral a simple yet effective method for unsupervised domain adaptation. coral minimizes domain shift by aligning the second order statistics of source and target distributions without requiring any target labels. in contrast to subspace manifold methods it aligns the original feature distributions of the source and target domains rather than the bases of lower dimensional subspaces. it is also much simpler than other distribution matching methods. coral performs remarkably well in extensive evaluations on standard benchmark datasets. we first describe a solution that applies a linear transformation to source features to align them with target features before classifier training. for linear classifiers we propose to equivalently apply coral to the classifier weights leading to added efficiency when the number of classifiers is small but the number and dimensionality of target examples are very high. the resulting coral linear discriminant analysis coral lda outperforms lda by a large margin on standard domain adaptation benchmarks. finally we extend coral to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks dnns . the resulting deep coral approach works seamlessly with dnns and achieves state of the art performance on standard benchmark datasets. our code is available at url https github.com visionlearninggroup coral 
citlab argus for historical handwritten documents
we describe citlab s recognition system for the htrts competition attached to the 13. international conference on document analysis and recognition icdar 2015. the task comprises the recognition of historical handwritten documents. the core algorithms of our system are based on multi dimensional recurrent neural networks mdrnn and connectionist temporal classification ctc . the software modules behind that as well as the basic utility technologies are essentially powered by planet s argus framework for intelligent text recognition and image processing.
generalized haar filter based deep networks for real time object detection in traffic scene
vision based object detection is one of the fundamental functions in numerous traffic scene applications such as self driving vehicle systems and advance driver assistance systems adas . however it is also a challenging task due to the diversity of traffic scene and the storage power and computing source limitations of the platforms for traffic scene applications. this paper presents a generalized haar filter based deep network which is suitable for the object detection tasks in traffic scene. in this approach we first decompose a object detection task into several easier local regression tasks. then we handle the local regression tasks by using several tiny deep networks which simultaneously output the bounding boxes categories and confidence scores of detected objects. to reduce the consumption of storage and computing resources the weights of the deep networks are constrained to the form of generalized haar filter in training phase. additionally we introduce the strategy of sparse windows generation to improve the efficiency of the algorithm. finally we perform several experiments to validate the performance of our proposed approach. experimental results demonstrate that the proposed approach is both efficient and effective in traffic scene compared with the state of the art.
autoencoder regularized network for driving style representation learning
in this paper we study learning generalized driving style representations from automobile gps trip data. we propose a novel autoencoder regularized deep neural network arnet and a trip encoding framework trip2vec to learn drivers driving styles directly from gps records by combining supervised and unsupervised feature learning in a unified architecture. experiments on a challenging driver number estimation problem and the driver identification problem show that arnet can learn a good generalized driving style representation it significantly outperforms existing methods and alternative architectures by reaching the least estimation error on average 0.68 less than one driver and the highest identification accuracy by at least 3 improvement compared with traditional supervised learning methods.
fashioning with networks neural style transfer to design clothes
convolutional neural networks have been highly successful in performing a host of computer vision tasks such as object recognition object detection image segmentation and texture synthesis. in 2015 gatys et. al 7 show how the style of a painter can be extracted from an image of the painting and applied to another normal photograph thus recreating the photo in the style of the painter. the method has been successfully applied to a wide range of images and has since spawned multiple applications and mobile apps. in this paper the neural style transfer algorithm is applied to fashion so as to synthesize new custom clothes. we construct an approach to personalize and generate new custom clothes based on a users preference and by learning the users fashion choices from a limited set of clothes from their closet. the approach is evaluated by analyzing the generated images of clothes and how well they align with the users fashion style.
globenet convolutional neural networks for typhoon eye tracking from remote sensing imagery
advances in remote sensing technologies have made it possible to use high resolution visual data for weather observation and forecasting tasks. we propose the use of multi layer neural networks for understanding complex atmospheric dynamics based on multichannel satellite images. the capability of our model was evaluated by using a linear regression task for single typhoon coordinates prediction. a specific combination of models and different activation policies enabled us to obtain an interesting prediction result in the northeastern hemisphere enh .
improving efficiency in convolutional neural network with multilinear filters
the excellent performance of deep neural networks has enabled us to solve several automatization problems opening an era of autonomous devices. however current deep net architectures are heavy with millions of parameters and require billions of floating point operations. several works have been developed to compress a pre trained deep network to reduce memory footprint and possibly computation. instead of compressing a pre trained network in this work we propose a generic neural network layer structure employing multilinear projection as the primary feature extractor. the proposed architecture requires several times less memory as compared to the traditional convolutional neural networks cnn while inherits the similar design principles of a cnn. in addition the proposed architecture is equipped with two computation schemes that enable computation reduction or scalability. experimental results show the effectiveness of our compact projection that outperforms traditional cnn while requiring far fewer parameters.
discovery radiomics with clear dr interpretable computer aided diagnosis of diabetic retinopathy
objective radiomics driven computer aided diagnosis cad has shown considerable promise in recent years as a potential tool for improving clinical decision support in medical oncology particularly those based around the concept of discovery radiomics where radiomic sequencers are discovered through the analysis of medical imaging data. one of the main limitations with current cad approaches is that it is very difficult to gain insight or rationale as to how decisions are made thus limiting their utility to clinicians. methods in this study we propose clear dr a novel interpretable cad system based on the notion of class enhanced attentive response discovery radiomics for the purpose of clinical decision support for diabetic retinopathy. results in addition to disease grading via the discovered deep radiomic sequencer the clear dr system also produces a visual interpretation of the decision making process to provide better insight and understanding into the decision making process of the system. conclusion we demonstrate the effectiveness and utility of the proposed clear dr system of enhancing the interpretability of diagnostic grading results for the application of diabetic retinopathy grading. significance clear dr can act as a potential powerful tool to address the uninterpretability issue of current cad systems thus improving their utility to clinicians.
hp gan probabilistic 3d human motion prediction via gan
predicting and understanding human motion dynamics has many applications such as motion synthesis augmented reality security and autonomous vehicles. due to the recent success of generative adversarial networks gan there has been much interest in probabilistic estimation and synthetic data generation using deep neural network architectures and learning algorithms. we propose a novel sequence to sequence model for probabilistic human motion prediction trained with a modified version of improved wasserstein generative adversarial networks wgan gp in which we use a custom loss function designed for human motion prediction. our model which we call hp gan learns a probability density function of future human poses conditioned on previous poses. it predicts multiple sequences of possible future human poses each from the same input sequence but a different vector z drawn from a random distribution. furthermore to quantify the quality of the non deterministic predictions we simultaneously train a motion quality assessment model that learns the probability that a given skeleton sequence is a real human motion. we test our algorithm on two of the largest skeleton datasets nturgb d and human3.6m. we train our model on both single and multiple action types. its predictive power for long term motion estimation is demonstrated by generating multiple plausible futures of more than 30 frames from just 10 frames of input. we show that most sequences generated from the same input have more than 50 probabilities of being judged as a real human sequence. we will release all the code used in this paper to github.
report dynamic eye movement matching and visualization tool in neuro gesture
in the research of the impact of gestures using by a lecturer one challenging task is to infer the attention of a group of audiences. two important measurements that can help infer the level of attention are eye movement data and electroencephalography eeg data. under the fundamental assumption that a group of people would look at the same place if they all pay attention at the same time we apply a method time warp edit distance to calculate the similarity of their eye movement trajectories. moreover we also cluster eye movement pattern of audiences based on these pair wised similarity metrics. besides since we don t have a direct metric for the attention ground truth a visual assessment would be beneficial to evaluate the gesture attention relationship. thus we also implement a visualization tool.
nature vs. nurture the role of environmental resources in evolutionary deep intelligence
evolutionary deep intelligence synthesizes highly efficient deep neural networks architectures over successive generations. inspired by the nature versus nurture debate we propose a study to examine the role of external factors on the network synthesis process by varying the availability of simulated environmental resources. experimental results were obtained for networks synthesized via asexual evolutionary synthesis 1 parent and sexual evolutionary synthesis 2 parent 3 parent and 5 parent using a 10 subset of the mnist dataset. results show that a lower environmental factor model resulted in a more gradual loss in performance accuracy and decrease in storage size. this potentially allows significantly reduced storage size with minimal to no drop in performance accuracy and the best networks were synthesized using the lowest environmental factor models.
a stochastic model of human visual attention with a dynamic bayesian network
recent studies in the field of human vision science suggest that the human responses to the stimuli on a visual display are non deterministic. people may attend to different locations on the same visual input at the same time. based on this knowledge we propose a new stochastic model of visual attention by introducing a dynamic bayesian network to predict the likelihood of where humans typically focus on a video scene. the proposed model is composed of a dynamic bayesian network with 4 layers. our model provides a framework that simulates and combines the visual saliency response and the cognitive state of a person to estimate the most probable attended regions. sample based inference with markov chain monte carlo based particle filter and stream processing with multi core processors enable us to estimate human visual attention in near real time. experimental results have demonstrated that our model performs significantly better in predicting human visual attention compared to the previous deterministic models.
smart content recognition from images using a mixture of convolutional neural networks
with rapid development of the internet web contents become huge. most of the websites are publicly available and anyone can access the contents from anywhere such as workplace home and even schools. nevertheless not all the web contents are appropriate for all users especially children. an example of these contents is pornography images which should be restricted to certain age group. besides these images are not safe for work nsfw in which employees should not be seen accessing such contents during work. recently convolutional neural networks have been successfully applied to many computer vision problems. inspired by these successes we propose a mixture of convolutional neural networks for adult content recognition. unlike other works our method is formulated on a weighted sum of multiple deep neural network models. the weights of each cnn models are expressed as a linear regression problem learned using ordinary least squares ols . experimental results demonstrate that the proposed model outperforms both single cnn model and the average sum of cnn models in adult content recognition.
cortical spatio temporal dimensionality reduction for visual grouping
the visual systems of many mammals including humans is able to integrate the geometric information of visual stimuli and to perform cognitive tasks already at the first stages of the cortical processing. this is thought to be the result of a combination of mechanisms which include feature extraction at single cell level and geometric processing by means of cells connectivity. we present a geometric model of such connectivities in the space of detected features associated to spatio temporal visual stimuli and show how they can be used to obtain low level object segmentation. the main idea is that of defining a spectral clustering procedure with anisotropic affinities over datasets consisting of embeddings of the visual stimuli into higher dimensional spaces. neural plausibility of the proposed arguments will be discussed.
visual sentiment prediction with deep convolutional neural networks
images have become one of the most popular types of media through which users convey their emotions within online social networks. although vast amount of research is devoted to sentiment analysis of textual data there has been very limited work that focuses on analyzing sentiment of image data. in this work we propose a novel visual sentiment prediction framework that performs image understanding with deep convolutional neural networks cnn . specifically the proposed sentiment prediction framework performs transfer learning from a cnn with millions of parameters which is pre trained on large scale data for object recognition. experiments conducted on two real world datasets from twitter and tumblr demonstrate the effectiveness of the proposed visual sentiment analysis framework.
correntropy maximization via admm application to robust hyperspectral unmixing
in hyperspectral images some spectral bands suffer from low signal to noise ratio due to noisy acquisition and atmospheric effects thus requiring robust techniques for the unmixing problem. this paper presents a robust supervised spectral unmixing approach for hyperspectral images. the robustness is achieved by writing the unmixing problem as the maximization of the correntropy criterion subject to the most commonly used constraints. two unmixing problems are derived the first problem considers the fully constrained unmixing with both the non negativity and sum to one constraints while the second one deals with the non negativity and the sparsity promoting of the abundances. the corresponding optimization problems are solved efficiently using an alternating direction method of multipliers admm approach. experiments on synthetic and real hyperspectral images validate the performance of the proposed algorithms for different scenarios demonstrating that the correntropy based unmixing is robust to outlier bands.
identifying individual facial expressions by deconstructing a neural network
this paper focuses on the problem of explaining predictions of psychological attributes such as attractiveness happiness confidence and intelligence from face photographs using deep neural networks. since psychological attribute datasets typically suffer from small sample sizes we apply transfer learning with two base models to avoid overfitting. these models were trained on an age and gender prediction task respectively. using a novel explanation method we extract heatmaps that highlight the parts of the image most responsible for the prediction. we further observe that the explanation method provides important insights into the nature of features of the base model which allow one to assess the aptitude of the base model for a given transfer learning task. finally we observe that the multiclass model is more feature rich than its binary counterpart. the experimental evaluation is performed on the 2222 images from the 10k us faces dataset containing psychological attribute labels as well as on a subset of kdef images.
object boundary detection and classification with image level labels
semantic boundary and edge detection aims at simultaneously detecting object edge pixels in images and assigning class labels to them. systematic training of predictors for this task requires the labeling of edges in images which is a particularly tedious task. we propose a novel strategy for solving this task when pixel level annotations are not available performing it in an almost zero shot manner by relying on conventional whole image neural net classifiers that were trained using large bounding boxes. our method performs the following two steps at test time. firstly it predicts the class labels by applying the trained whole image network to the test images. secondly it computes pixel wise scores from the obtained predictions by applying backprop gradients as well as recent visualization algorithms such as deconvolution and layer wise relevance propagation. we show that high pixel wise scores are indicative for the location of semantic boundaries which suggests that the semantic boundary problem can be approached without using edge labels during the training phase.
evolving spatially aggregated features from satellite imagery for regional modeling
satellite imagery and remote sensing provide explanatory variables at relatively high resolutions for modeling geospatial phenomena yet regional summaries are often desirable for analysis and actionable insight. in this paper we propose a novel method of inducing spatial aggregations as a component of the machine learning process yielding regional model features whose construction is driven by model prediction performance rather than prior assumptions. our results demonstrate that genetic programming is particularly well suited to this type of feature construction because it can automatically synthesize appropriate aggregations as well as better incorporate them into predictive models compared to other regression methods we tested. in our experiments we consider a specific problem instance and real world dataset relevant to predicting snow properties in high mountain asia.
pillar networks distributed non parametric deep and wide networks
in recent work it was shown that combining multi kernel based support vector machines svms can lead to near state of the art performance on an action recognition dataset hmdb 51 dataset . this was 0.4 lower than frameworks that used hand crafted features in addition to the deep convolutional feature extractors. in the present work we show that combining distributed gaussian processes with multi stream deep convolutional neural networks cnn alleviate the need to augment a neural network with hand crafted features. in contrast to prior work we treat each deep neural convolutional network as an expert wherein the individual predictions and their respective uncertainties are combined into a product of experts poe framework.
market based reinforcement learning in partially observable worlds
unlike traditional reinforcement learning rl market based rl is in principle applicable to worlds described by partially observable markov decision processes pomdps where an agent needs to learn short term memories of relevant previous events in order to execute optimal actions. most previous work however has focused on reactive settings mdps instead of pomdps. here we reimplement a recent approach to market based rl and for the first time evaluate it in a toy pomdp setting.
controlled hierarchical filtering model of neocortical sensory processing
a model of sensory information processing is presented. the model assumes that learning of internal hidden generative models which can predict the future and evaluate the precision of that prediction is of central importance for information extraction. furthermore the model makes a bridge to goal oriented systems and builds upon the structural similarity between the architecture of a robust controller and that of the hippocampal entorhinal loop. this generative control architecture is mapped to the neocortex and to the hippocampal entorhinal loop. implicit memory phenomena priming and prototype learning are emerging features of the model. mathematical theorems ensure stability and attractive learning properties of the architecture. connections to reinforcement learning are also established both the control network and the network with a hidden model converge to near optimal policy under suitable conditions. falsifying predictions including the role of the feedback connections between neocortical areas are made.
when do differences matter on line feature extraction through cognitive economy
for an intelligent agent to be truly autonomous it must be able to adapt its representation to the requirements of its task as it interacts with the world. most current approaches to on line feature extraction are ad hoc in contrast this paper presents an algorithm that bases judgments of state compatibility and state space abstraction on principled criteria derived from the psychological principle of cognitive economy. the algorithm incorporates an active form of q learning and partitions continuous state spaces by merging and splitting voronoi regions. the experiments illustrate a new methodology for testing and comparing representations by means of learning curves. results from the puck on a hill task demonstrate the algorithm s ability to learn effective representations superior to those produced by some other well known methods.
applying policy iteration for training recurrent neural networks
recurrent neural networks are often used for learning time series data. based on a few assumptions we model this learning task as a minimization problem of a nonlinear least squares cost function. the special structure of the cost function allows us to build a connection to reinforcement learning. we exploit this connection and derive a convergent policy iteration based algorithm. furthermore we argue that rnn training can be fit naturally into the reinforcement learning framework.
a neural network technique to learn concepts from electroencephalograms
a new technique is presented developed to learn multi class concepts from clinical electroencephalograms. a desired concept is represented as a neuronal computational model consisting of the input hidden and output neurons. in this model the hidden neurons learn independently to classify the electroencephalogram segments presented by spectral and statistical features. this technique has been applied to the electroencephalogram data recorded from 65 sleeping healthy newborns in order to learn a brain maturation concept of newborns aged between 35 and 51 weeks. the 39399 and 19670 segments from these data have been used for learning and testing the concept respectively. as a result the concept has correctly classified 80.1 of the testing segments or 87.7 of the 65 records.
empirical learning aided by weak domain knowledge in the form of feature importance
standard hybrid learners that use domain knowledge require stronger knowledge that is hard and expensive to acquire. however weaker domain knowledge can benefit from prior knowledge while being cost effective. weak knowledge in the form of feature relative importance fri is presented and explained. feature relative importance is a real valued approximation of a feature s importance provided by experts. advantage of using this knowledge is demonstrated by iann a modified multilayer neural network algorithm. iann is a very simple modification of standard neural network algorithm but attains significant performance gains. experimental results in the field of molecular biology show higher performance over other empirical learning algorithms including standard backpropagation and support vector machines. iann performance is even comparable to a theory refinement system kbann that uses stronger domain knowledge. this shows feature relative importance can improve performance of existing empirical learning algorithms significantly with minimal effort.
evolutionary algorithms for reinforcement learning
there are two distinct approaches to solving reinforcement learning problems namely searching in value function space and searching in policy space. temporal difference methods and evolutionary algorithms are well known examples of these approaches. kaelbling littman and moore recently provided an informative survey of temporal difference methods. this article focuses on the application of evolutionary algorithms to the reinforcement learning problem emphasizing alternative policy representations credit assignment methods and problem specific genetic operators. strengths and weaknesses of the evolutionary approach to reinforcement learning are presented along with a survey of representative applications.
on training deep boltzmann machines
the deep boltzmann machine dbm has been an important development in the quest for powerful deep probabilistic models. to date simultaneous or joint training of all layers of the dbm has been largely unsuccessful with existing training methods. we introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms. we demonstrate that this regularization can be easily combined with standard stochastic maximum likelihood to yield an effective training strategy for the simultaneous training of all layers of the deep boltzmann machine.
memristive fuzzy edge detector
fuzzy inference systems always suffer from the lack of efficient structures or platforms for their hardware implementation. in this paper we tried to overcome this problem by proposing new method for the implementation of those fuzzy inference systems which use fuzzy rule base to make inference. to achieve this goal we have designed a multi layer neuro fuzzy computing system based on the memristor crossbar structure by introducing some new concepts like fuzzy minterms. although many applications can be realized through the use of our proposed system in this study we show how the fuzzy xor function can be constructed and how it can be used to extract edges from grayscale images. our memristive fuzzy edge detector implemented in analog form compared with other common edge detectors has this advantage that it can extract edges of any given image all at once in real time.
echo state queueing network a new reservoir computing learning tool
in the last decade a new computational paradigm was introduced in the field of machine learning under the name of reservoir computing rc . rc models are neural networks which a recurrent part the reservoir that does not participate in the learning process and the rest of the system where no recurrence no neural circuit occurs. this approach has grown rapidly due to its success in solving learning tasks and other computational applications. some success was also observed with another recently proposed neural network designed using queueing theory the random neural network randnn . both approaches have good properties and identified drawbacks. in this paper we propose a new rc model called echo state queueing network esqn where we use ideas coming from randnns for the design of the reservoir. esqns consist in esns where the reservoir has a new dynamics inspired by recurrent randnns. the paper positions esqns in the global machine learning area and provides examples of their use and performances. we show on largely used benchmarks that esqns are very accurate tools and we illustrate how they compare with standard esns.
the predictron end to end learning and planning
one of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. in this document we introduce the predictron architecture. the predictron consists of a fully abstract model represented by a markov reward process that can be rolled forward multiple imagined planning steps. each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. the predictron is trained end to end so as to make these accumulated values accurately approximate the true value function. we applied the predictron to procedurally generated random mazes and a simulator for the game of pool. the predictron yielded significantly more accurate predictions than conventional deep neural network architectures.
quadratically constrained quadratic programming for classification using particle swarms and applications
particle swarm optimization is used in several combinatorial optimization problems. in this work particle swarms are used to solve quadratic programming problems with quadratic constraints. the approach of particle swarms is an example for interior point methods in optimization as an iterative technique. this approach is novel and deals with classification problems without the use of a traditional classifier. our method determines the optimal hyperplane or classification boundary for a data set. in a binary classification problem we constrain each class as a cluster which is enclosed by an ellipsoid. the estimation of the optimal hyperplane between the two clusters is posed as a quadratically constrained quadratic problem. the optimization problem is solved in distributed format using modified particle swarms. our method has the advantage of using the direction towards optimal solution rather than searching the entire feasible region. our results on the iris pima wine and thyroid datasets show that the proposed method works better than a neural network and the performance is close to that of svm.
learning to execute
recurrent neural networks rnns with long short term memory units lstm are widely used because they are expressive and are easy to train. our interest lies in empirically evaluating the expressiveness and the learnability of lstms in the sequence to sequence regime by training them to evaluate short computer programs a domain that has traditionally been seen as too complex for neural networks. we consider a simple class of programs that can be evaluated with a single left to right pass using constant memory. our main result is that lstms can learn to map the character level representations of such programs to their correct outputs. notably it was necessary to use curriculum learning and while conventional curriculum learning proved ineffective we developed a new variant of curriculum learning that improved our networks performance in all experimental conditions. the improved curriculum had a dramatic impact on an addition problem making it possible to train an lstm to add two 9 digit numbers with 99 accuracy.
bitwise neural networks
based on the assumption that there exists a neural network that efficiently represents a set of boolean functions between all binary inputs and outputs we propose a process for developing and deploying neural networks whose weight parameters bias terms input and intermediate hidden layer output signals are all binary valued and require only basic bit logic for the feedforward pass. the proposed bitwise neural network bnn is especially suitable for resource constrained environments since it replaces either floating or fixed point arithmetic with significantly more efficient bitwise operations. hence the bnn requires for less spatial complexity less memory bandwidth and less power consumption in hardware. in order to design such networks we propose to add a few training schemes such as weight compression and noisy backpropagation which result in a bitwise network that performs almost as well as its corresponding real valued network. we test the proposed network on the mnist dataset represented using binary features and show that bnns result in competitive performance while offering dramatic computational savings.
graying the black box understanding dqns
in recent years there is a growing interest in using deep representations for reinforcement learning. in this paper we present a methodology and tools to analyze deep q networks dqns in a non blind matter. moreover we propose a new model the semi aggregated markov decision process samdp and an algorithm that learns it automatically. the samdp model allows us to identify spatio temporal abstractions directly from features and may be used as a sub goal detector in future work. using our tools we reveal that the features learned by dqns aggregate the state space in a hierarchical fashion explaining its success. moreover we are able to understand and describe the policies learned by dqns for three different atari2600 games and suggest ways to interpret debug and optimize deep neural networks in reinforcement learning.
evaluation of a tree based pipeline optimization tool for automating data science
as the field of data science continues to grow there will be an ever increasing demand for tools that make machine learning accessible to non experts. in this paper we introduce the concept of tree based pipeline optimization for automating one of the most tedious parts of machine learning pipeline design. we implement an open source tree based pipeline optimization tool tpot in python and demonstrate its effectiveness on a series of simulated and real world benchmark data sets. in particular we show that tpot can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. we also address the tendency for tpot to design overly complex pipelines by integrating pareto optimization which produces compact pipelines without sacrificing classification accuracy. as such this work represents an important step toward fully automating machine learning pipeline design.
probabilistic reasoning via deep learning neural association models
in this paper we propose a new deep learning approach called neural association model nam for probabilistic reasoning in artificial intelligence. we propose to use neural networks to model association between any two events in a domain. neural networks take one event as input and compute a conditional probability of the other event to model how likely these two events are to be associated. the actual meaning of the conditional probabilities varies between applications and depends on how the models are trained. in this work as two case studies we have investigated two nam structures namely deep neural networks dnn and relation modulated neural nets rmnn on several probabilistic reasoning tasks in ai including recognizing textual entailment triple classification in multi relational knowledge bases and commonsense reasoning. experimental results on several popular datasets derived from wordnet freebase and conceptnet have all demonstrated that both dnns and rmnns perform equally well and they can significantly outperform the conventional methods available for these reasoning tasks. moreover compared with dnns rmnns are superior in knowledge transfer where a pre trained model can be quickly extended to an unseen relation after observing only a few training samples. to further prove the effectiveness of the proposed models in this work we have applied nams to solving challenging winograd schema ws problems. experiments conducted on a set of ws problems prove that the proposed models have the potential for commonsense reasoning.
deep reinforcement learning with macro actions
deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high dimensional sensory inputs to actions in complex tasks such as the atari domain. in this paper we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. we concentrate on macro actions and evaluate these on different atari 2600 games where we show that they yield significant improvements in learning speed. additionally we show that they can even achieve better scores than dqn. we offer analysis and explanation for both convergence and final results revealing a problem deep rl approaches have with sparse reward signals.
retain an interpretable predictive model for healthcare using reverse time attention mechanism
accuracy and interpretability are two dominant features of successful predictive models. typically a choice must be made in favor of complex black box models such as recurrent neural networks rnn for accuracy versus less accurate but more interpretable traditional models such as logistic regression. this tradeoff poses challenges in medicine where both accuracy and interpretability are important. we addressed this challenge by developing the reverse time attention model retain for application to electronic health records ehr data. retain achieves high accuracy while remaining clinically interpretable and is based on a two level neural attention model that detects influential past visits and significant clinical variables within those visits e.g. key diagnoses . retain mimics physician practice by attending the ehr data in a reverse time order so that recent clinical visits are likely to receive higher attention. retain was tested on a large health system ehr dataset with 14 million visits completed by 263k patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state of the art methods such as rnn and ease of interpretability comparable to traditional models.
a high speed multi label classifier based on extreme learning machines
in this paper a high speed neural network classifier based on extreme learning machines for multi label classification problem is proposed and dis cussed. multi label classification is a superset of traditional binary and multi class classification problems. the proposed work extends the extreme learning machine technique to adapt to the multi label problems. as opposed to the single label problem both the number of labels the sample belongs to and each of those target labels are to be identified for multi label classification resulting in in creased complexity. the proposed high speed multi label classifier is applied to six benchmark datasets comprising of different application areas such as multi media text and biology. the training time and testing time of the classifier are compared with those of the state of the arts methods. experimental studies show that for all the six datasets our proposed technique have faster execution speed and better performance thereby outperforming all the existing multi label clas sification methods.
an online universal classifier for binary multi class and multi label classification
classification involves the learning of the mapping function that associates input samples to corresponding target label. there are two major categories of classification problems single label classification and multi label classification. traditional binary and multi class classifications are sub categories of single label classification. several classifiers are developed for binary multi class and multi label classification problems but there are no classifiers available in the literature capable of performing all three types of classification. in this paper a novel online universal classifier capable of performing all the three types of classification is proposed. being a high speed online classifier the proposed technique can be applied to streaming data applications. the performance of the developed classifier is evaluated using datasets from binary multi class and multi label problems. the results obtained are compared with state of the art techniques from each of the classification types.
adaptive online sequential elm for concept drift tackling
a machine learning method needs to adapt to over time changes in the environment. such changes are known as concept drift. in this paper we propose concept drift tackling method as an enhancement of online sequential extreme learning machine os elm and constructive enhancement os elm ceos elm by adding adaptive capability for classification and regression problem. the scheme is named as adaptive os elm aos elm . it is a single classifier scheme that works well to handle real drift virtual drift and hybrid drift. the aos elm also works well for sudden drift and recurrent context change type. the scheme is a simple unified method implemented in simple lines of code. we evaluated aos elm on regression and classification problem by using concept drift public data set sea and stagger and other public data sets such as mnist usps and ids. experiments show that our method gives higher kappa value compared to the multiclassifier elm ensemble. even though aos elm in practice does not need hidden nodes increase we address some issues related to the increasing of the hidden nodes such as error condition and rank values. we propose taking the rank of the pseudoinverse matrix as an indicator parameter to detect underfitting condition.
adaptive convolutional elm for concept drift handling in online stream data
in big data era the data continuously generated and its distribution may keep changes overtime. these challenges in online stream of data are known as concept drift. in this paper we proposed the adaptive convolutional elm method acnnelm as enhancement of convolutional neural network cnn with a hybrid extreme learning machine elm model plus adaptive capability. this method is aimed for concept drift handling. we enhanced the cnn as convolutional hiererchical features representation learner combined with elastic elm e 2 lm as a parallel supervised classifier. we propose an adaptive os elm aos elm for concept drift adaptability in classifier level named acnnelm 1 and matrices concatenation ensembles for concept drift adaptability in ensemble level named acnnelm 2 . our proposed adaptive cnnelm is flexible that works well in classifier level and ensemble level while most current methods only proposed to work on either one of the levels. we verified our method in extended mnist data set and not mnist data set. we set the experiment to simulate virtual drift real drift and hybrid drift event and we demonstrated how our cnnelm adaptability works. our proposed method works well and gives better accuracy computation scalability and concept drifts adaptability compared to the regular elm and cnn. further researches are still required to study the optimum parameters and to use more varied image data set.
particle swarm optimization for generating interpretable fuzzy reinforcement learning policies
fuzzy controllers are efficient and interpretable system controllers for continuous state and action spaces. to date such controllers have been constructed manually or trained automatically either using expert generated problem specific cost functions or incorporating detailed knowledge about the optimal control strategy. both requirements for automatic training processes are not found in most real world reinforcement learning rl problems. in such applications online learning is often prohibited for safety reasons because online learning requires exploration of the problem s dynamics during policy training. we introduce a fuzzy particle swarm reinforcement learning fpsrl approach that can construct fuzzy rl policies solely by training parameters on world models that simulate real system dynamics. these world models are created by employing an autonomous machine learning technique that uses previously generated transition samples of a real system. to the best of our knowledge this approach is the first to relate self organizing fuzzy controllers to model based batch rl. therefore fpsrl is intended to solve problems in domains where online learning is prohibited system dynamics are relatively easy to model from previously generated default policy transition samples and it is expected that a relatively easily interpretable control policy exists. the efficiency of the proposed approach with problems from such domains is demonstrated using three standard rl benchmarks i.e. mountain car cart pole balancing and cart pole swing up. our experimental results demonstrate high performing interpretable fuzzy policies.
a growing long term episodic semantic memory
the long term memory of most connectionist systems lies entirely in the weights of the system. since the number of weights is typically fixed this bounds the total amount of knowledge that can be learned and stored. though this is not normally a problem for a neural network designed for a specific task such a bound is undesirable for a system that continually learns over an open range of domains. to address this we describe a lifelong learning system that leverages a fast though non differentiable content addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. this opens the door for investigation into transfer learning and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains.
cognitive discriminative mappings for rapid learning
humans can learn concepts or recognize items from just a handful of examples while machines require many more samples to perform the same task. in this paper we build a computational model to investigate the possibility of this kind of rapid learning. the proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long term memory. we present a simple and intuitive technique called cognitive discriminative mappings cdm to explore the cognitive problem. first cdm separates and clusters the data instances retrieved from long term memory into distinct classes with a discrimination method in working memory when a sensory input triggers the algorithm. cdm then maps each sensory data instance to be as close as possible to the median point of the data group with the same class. the experimental results demonstrate that the cdm approach is effective for learning the discriminative features of supervised classifications with few training sensory input instances.
towards a mathematical understanding of the difficulty in learning with feedforward neural networks
training deep neural networks for solving machine learning problems is one great challenge in the field mainly due to its associated optimisation problem being highly non convex. recent developments have suggested that many training algorithms do not suffer from undesired local minima under certain scenario and consequently led to great efforts in pursuing mathematical explanations for such observations. this work provides an alternative mathematical understanding of the challenge from a smooth optimisation perspective. by assuming exact learning of finite samples sufficient conditions are identified via a critical point analysis to ensure any local minimum to be globally minimal as well. furthermore a state of the art algorithm known as the generalised gauss newton ggn algorithm is rigorously revisited as an approximate newton s algorithm which shares the property of being locally quadratically convergent to a global minimum under the condition of exact learning.
an effective algorithm for hyperparameter optimization of neural networks
a major challenge in designing neural network nn systems is to determine the best structure and parameters for the network given the data for the machine learning problem at hand. examples of parameters are the number of layers and nodes the learning rates and the dropout rates. typically these parameters are chosen based on heuristic rules and manually fine tuned which may be very time consuming because evaluating the performance of a single parametrization of the nn may require several hours. this paper addresses the problem of choosing appropriate parameters for the nn by formulating it as a box constrained mathematical optimization problem and applying a derivative free optimization tool that automatically and effectively searches the parameter space. the optimization tool employs a radial basis function model of the objective function the prediction accuracy of the nn to accelerate the discovery of configurations yielding high accuracy. candidate configurations explored by the algorithm are trained to a small number of epochs and only the most promising candidates receive full training. the performance of the proposed methodology is assessed on benchmark sets and in the context of predicting drug drug interactions showing promising results. the optimization tool used in this paper is open source.
evolutionary training of sparse artificial neural networks a network science perspective
through the success of deep learning artificial neural networks anns are among the most used artificial intelligence methods nowadays. anns have led to major breakthroughs in various domains such as particle physics reinforcement learning speech recognition computer vision and so on. taking inspiration from the network properties of biological neural networks e.g. sparsity scale freeness we argue that contrary to general practice artificial neural networks ann too should not have fully connected layers. we show how anns perform perfectly well with sparsely connected layers. following a darwinian evolutionary approach we propose a novel algorithm which evolves an initial random sparse topology i.e. an erd h o s r enyi random graph of two consecutive layers of neurons into a scale free topology during the ann training process. the resulting sparse layers can safely replace the corresponding fully connected layers. our method allows to quadratically reduce the number of parameters in the fully conencted layers of anns yielding quadratically faster computational times in both phases i.e. training and inference at no decrease in accuracy. we demonstrate our claims on two popular ann types restricted boltzmann machine and multi layer perceptron on two types of tasks supervised and unsupervised learning and on 14 benchmark datasets. we anticipate that our approach will enable anns having billions of neurons and evolved topologies to be capable of handling complex real world tasks that are intractable using state of the art methods.
attend and predict understanding gene regulation by selective attention on chromatin
the past decade has seen a revolution in genomic technologies that enable a flood of genome wide profiling of chromatin marks. recent literature tried to understand gene regulation by predicting gene expression from large scale chromatin measurements. two fundamental challenges exist for such learning tasks 1 genome wide chromatin signals are spatially structured high dimensional and highly modular and 2 the core aim is to understand what are the relevant factors and how they work together previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions. this paper presents an attention based deep learning approach we call attentivechrome that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation. attentivechrome uses a hierarchy of multiple long short term memory lstm modules to encode the input signals and to model how various chromatin marks cooperate automatically. attentivechrome trains two levels of attention jointly with the target prediction enabling it to attend differentially to relevant marks and to locate important positions per mark. we evaluate the model across 56 different cell types tasks in human. not only is the proposed architecture more accurate but its attention scores also provide a better interpretation than state of the art feature visualization methods such as saliency map. code and data are shared at www.deepchrome.org
parallelizing linear recurrent neural nets over sequence length
recurrent neural networks rnns are widely used to model sequential data but their non linear dependencies between sequence elements prevent parallelizing training over sequence length. we show the training of rnns with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm leading to rapid training on long sequences even with small minibatch size. we develop a parallel linear recurrence cuda kernel and show that it can be applied to immediately speed up training and inference of several state of the art rnn architectures by up to 9x. we abstract recent work on linear rnns into a new framework of linear surrogate rnns and develop a linear surrogate model for the long short term memory unit the gilr lstm that utilizes parallel linear recurrence. we extend sequence learning to new extremely long sequence regimes that were previously out of reach by successfully training a gilr lstm on a synthetic sequence classification task with a one million timestep dependency.
feature learning in feature sample networks using multi objective optimization
data and knowledge representation are fundamental concepts in machine learning. the quality of the representation impacts the performance of the learning model directly. feature learning transforms or enhances raw data to structures that are effectively exploited by those models. in recent years several works have been using complex networks for data representation and analysis. however no feature learning method has been proposed for such category of techniques. here we present an unsupervised feature learning mechanism that works on datasets with binary features. first the dataset is mapped into a feature sample network. then a multi objective optimization process selects a set of new vertices to produce an enhanced version of the network. the new features depend on a nonlinear function of a combination of preexisting features. effectively the process projects the input data into a higher dimensional space. to solve the optimization problem we design two metaheuristics based on the lexicographic genetic algorithm and the improved strength pareto evolutionary algorithm spea2 . we show that the enhanced network contains more information and can be exploited to improve the performance of machine learning methods. the advantages and disadvantages of each optimization strategy are discussed.
meta learning and universality deep representations and gradient descent can approximate any learning algorithm
learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. a popular approach to meta learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model or output predictions for new test inputs. alternatively a more recent approach to meta learning aims to acquire deep representations that can be effectively fine tuned via standard gradient descent to new tasks. in this paper we consider the meta learning problem from the perspective of universality formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta learner. in particular we seek to answer the following question does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm we find that this is indeed true and further find in our experiments that gradient based meta learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.
hindsight policy gradients
goal conditional policies allow reinforcement learning agents to pursue specific goals during different episodes. in addition to their potential to generalize desired behavior to unseen goals such policies may also help in defining options for arbitrary subgoals enabling higher level planning. while trying to achieve a specific goal an agent may also be able to exploit information about the degree to which it has achieved alternative goals. reinforcement learning agents have only recently been endowed with such capacity for hindsight which is highly valuable in environments with sparse rewards. in this paper we show how hindsight can be introduced to likelihood ratio policy gradient methods generalizing this capacity to an entire class of highly successful algorithms. our preliminary experiments suggest that hindsight may increase the sample efficiency of policy gradient methods.
squishednets squishing squeezenet further for edge device scenarios via deep evolutionary synthesis
while deep neural networks have been shown in recent years to outperform other machine learning methods in a wide range of applications one of the biggest challenges with enabling deep neural networks for widespread deployment on edge devices such as mobile and other consumer devices is high computational and memory requirements. recently there has been greater exploration into small deep neural network architectures that are more suitable for edge devices with one of the most popular architectures being squeezenet with an incredibly small model size of 4.8mb. taking further advantage of the notion that many applications of machine learning on edge devices are often characterized by a low number of target classes this study explores the utility of combining architectural modifications and an evolutionary synthesis strategy for synthesizing even smaller deep neural architectures based on the more recent squeezenet v1.1 macroarchitecture for applications with fewer target classes. in particular architectural modifications are first made to squeezenet v1.1 to accommodate for a 10 class imagenet 10 dataset and then an evolutionary synthesis strategy is leveraged to synthesize more efficient deep neural networks based on this modified macroarchitecture. the resulting squishednets possess model sizes ranging from 2.4mb to 0.95mb 5.17x smaller than squeezenet v1.1 or 253x smaller than alexnet . furthermore the squishednets are still able to achieve accuracies ranging from 81.2 to 77 and able to process at speeds of 156 images sec to as much as 256 images sec on a nvidia jetson tx1 embedded chip. these preliminary results show that a combination of architectural modifications and an evolutionary synthesis strategy can be a useful tool for producing very small deep neural network architectures that are well suited for edge device scenarios.
autonomous development and learning in artificial intelligence and robotics scaling up deep learning to human like learning
autonomous lifelong development and learning is a fundamental capability of humans differentiating them from current deep learning systems. however other branches of artificial intelligence have designed crucial ingredients towards autonomous learning curiosity and intrinsic motivation social learning and natural interaction with peers and embodiment. these mechanisms guide exploration and autonomous choice of goals and integrating them with deep learning opens stimulating perspectives. deep learning dl approaches made great advances in artificial intelligence but are still far away from human learning. as argued convincingly by lake et al. differences include human capabilities to learn causal models of the world from very little data leveraging compositional representations and priors like intuitive physics and psychology. however there are other fundamental differences between current dl systems and human learning as well as technical ingredients to fill this gap that are either superficially or not adequately discussed by lake et al. these fundamental mechanisms relate to autonomous development and learning. they are bound to play a central role in artificial intelligence in the future. current dl systems require engineers to manually specify a task specific objective function for every new task and learn through off line processing of large training databases. on the contrary humans learn autonomously open ended repertoires of skills deciding for themselves which goals to pursue or value and which skills to explore driven by intrinsic motivation curiosity and social learning through natural interaction with peers. such learning processes are incremental online and progressive. human child development involves a progressive increase of complexity in a curriculum of learning where skills are explored acquired and built on each other through particular ordering and timing. finally human learning happens in the physical world and through bodily and physical experimentation under severe constraints on energy time and computational resources. in the two last decades the field of developmental and cognitive robotics cangelosi and schlesinger 2015 asada et al. 2009 in strong interaction with developmental psychology and neuroscience has achieved significant advances in computational
learning from scarce experience
searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems. typically with each change of the target policy its value is estimated from the results of following that very policy. this requires a large number of interactions with the environment as different polices are considered. we present a family of algorithms based on likelihood ratio estimation that use data gathered when executing one policy or collection of policies to estimate the value of a different policy. the algorithms combine estimation and optimization stages. the former utilizes experience to build a non parametric representation of an optimized function. the latter performs optimization on this estimate. we show positive empirical results and provide the sample complexity bound.
fitness inheritance in the bayesian optimization algorithm
this paper describes how fitness inheritance can be used to estimate fitness for a proportion of newly sampled candidate solutions in the bayesian optimization algorithm boa . the goal of estimating fitness for some candidate solutions is to reduce the number of fitness evaluations for problems where fitness evaluation is expensive. bayesian networks used in boa to model promising solutions and generate the new ones are extended to allow not only for modeling and sampling candidate solutions but also for estimating their fitness. the results indicate that fitness inheritance is a promising concept in boa because population sizing requirements for building appropriate models of promising solutions lead to good fitness estimates even if only a small proportion of candidate solutions is evaluated using the actual fitness function. this can lead to a reduction of the number of actual fitness evaluations by a factor of 30 or more.
the combined technique for detection of artifacts in clinical electroencephalograms of sleeping newborns
in this paper we describe a new method combining the polynomial neural network and decision tree techniques in order to derive comprehensible classification rules from clinical electroencephalograms eegs recorded from sleeping newborns. these eegs are heavily corrupted by cardiac eye movement muscle and noise artifacts and as a consequence some eeg features are irrelevant to classification problems. combining the polynomial network and decision tree techniques we discover comprehensible classification rules whilst also attempting to keep their classification error down. this technique is shown to outperform a number of commonly used machine learning technique applied to automatically recognize artifacts in the sleep eegs.
evolving classifiers methods for incremental learning
the ability of a classifier to take on new information and classes by evolving the classifier without it having to be fully retrained is known as incremental learning. incremental learning has been successfully applied to many classification problems where the data is changing and is not all available at once. in this paper there is a comparison between learn which is one of the most recent incremental learning algorithms and the new proposed method of incremental learning using genetic algorithm iluga . learn has shown good incremental learning capabilities on benchmark datasets on which the new iluga method has been tested. iluga has also shown good incremental learning ability using only a few classifiers and does not suffer from catastrophic forgetting. the results obtained for iluga on the optical character recognition ocr and wine datasets are good with an overall accuracy of 93 and 94 respectively showing a 4 improvement over learn .mt for the difficult multi class ocr dataset.
automatic pattern classification by unsupervised learning using dimensionality reduction of data with mirroring neural networks
this paper proposes an unsupervised learning technique by using multi layer mirroring neural network and forgy s clustering algorithm. multi layer mirroring neural network is a neural network that can be trained with generalized data inputs different categories of image patterns to perform non linear dimensionality reduction and the resultant low dimensional code is used for unsupervised pattern classification using forgy s algorithm. by adapting the non linear activation function modified sigmoidal function and initializing the weights and bias terms to small random values mirroring of the input pattern is initiated. in training the weights and bias terms are changed in such a way that the input presented is reproduced at the output by back propagating the error. the mirroring neural network is capable of reducing the input vector to a great degree approximately 1 30th the original size and also able to reconstruct the input pattern at the output layer from this reduced code units. the feature set output of central hidden layer extracted from this network is fed to forgy s algorithm which classify input data patterns into distinguishable classes. in the implementation of forgy s algorithm initial seed points are selected in such a way that they are distant enough to be perfectly grouped into different categories. thus a new method of unsupervised learning is formulated and demonstrated in this paper. this method gave impressive results when applied to classification of different image patterns.
improving the performance of piecewise linear separation incremental algorithms for practical hardware implementations
in this paper we shall review the common problems associated with piecewise linear separation incremental algorithms. this kind of neural models yield poor performances when dealing with some classification problems due to the evolving schemes used to construct the resulting networks. so as to avoid this undesirable behavior we shall propose a modification criterion. it is based upon the definition of a function which will provide information about the quality of the network growth process during the learning phase. this function is evaluated periodically as the network structure evolves and will permit as we shall show through exhaustive benchmarks to considerably improve the performance measured in terms of network complexity and generalization capabilities offered by the networks generated by these incremental models.
a novel rough set reduct algorithm for medical domain based on bee colony optimization
feature selection refers to the problem of selecting relevant features which produce the most predictive outcome. in particular feature selection task is involved in datasets containing huge number of features. rough set theory has been one of the most successful methods used for feature selection. however this method is still not able to find optimal subsets. this paper proposes a new feature selection method based on rough set theory hybrid with bee colony optimization bco in an attempt to combat this. this proposed work is applied in the medical domain to find the minimal reducts and experimentally compared with the quick reduct entropy based reduct and other hybrid rough set methods such as genetic algorithm ga ant colony optimization aco and particle swarm optimization pso .
automated query learning with wikipedia and genetic programming
most of the existing information retrieval systems are based on bag of words model and are not equipped with common world knowledge. work has been done towards improving the efficiency of such systems by using intelligent algorithms to generate search queries however not much research has been done in the direction of incorporating human and society level knowledge in the queries. this paper is one of the first attempts where such information is incorporated into the search queries using wikipedia semantics. the paper presents an essential shift from conventional token based queries to concept based queries leading to an enhanced efficiency of information retrieval systems. to efficiently handle the automated query learning problem we propose wikipedia based evolutionary semantics wiki es framework where concept based queries are learnt using a co evolving evolutionary procedure. learning concept based queries using an intelligent evolutionary procedure yields significant improvement in performance which is shown through an extensive study using reuters newswire documents. comparison of the proposed framework is performed with other information retrieval systems. concept based approach has also been implemented on other information retrieval systems to justify the effectiveness of a transition from token based queries to concept based queries.
scaling up estimation of distribution algorithms for continuous optimization
since estimation of distribution algorithms eda were proposed many attempts have been made to improve edas performance in the context of global optimization. so far the studies or applications of multivariate probabilistic model based continuous edas are still restricted to rather low dimensional problems smaller than 100d . traditional edas have difficulties in solving higher dimensional problems because of the curse of dimensionality and their rapidly increasing computational cost. however scaling up continuous edas for higher dimensional optimization is still necessary which is supported by the distinctive feature of edas because a probabilistic model is explicitly estimated from the learnt model one can discover useful properties or features of the problem. besides obtaining a good solution understanding of the problem structure can be of great benefit especially for black box optimization. we propose a novel eda framework with model complexity control eda mcc to scale up edas. by using weakly dependent variable identification wi and subspace modeling sm eda mcc shows significantly better performance than traditional edas on high dimensional problems. moreover the computational cost and the requirement of large population sizes can be reduced in eda mcc. in addition to being able to find a good solution eda mcc can also produce a useful problem structure characterization. eda mcc is the first successful instance of multivariate model based edas that can be effectively applied a general class of up to 500d problems. it also outperforms some newly developed algorithms designed specifically for large scale optimization. in order to understand the strength and weakness of eda mcc we have carried out extensive computational studies of eda mcc. our results have revealed when eda mcc is likely to outperform others on what kind of benchmark functions.
transfer learning soft distance based bias and the hierarchical boa
an automated technique has recently been proposed to transfer learning in the hierarchical bayesian optimization algorithm hboa based on distance based statistics. the technique enables practitioners to improve hboa efficiency by collecting statistics from probabilistic models obtained in previous hboa runs and using the obtained statistics to bias future hboa runs on similar problems. the purpose of this paper is threefold 1 test the technique on several classes of np complete problems including maxsat spin glasses and minimum vertex cover 2 demonstrate that the technique is effective even when previous runs were done on problems of different size 3 provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often yield nearly multiplicative speedups.
discrete dynamical genetic programming in xcs
a number of representation schemes have been presented for use within learning classifier systems ranging from binary encodings to neural networks. this paper presents results from an investigation into using a discrete dynamical system representation within the xcs learning classifier system. in particular asynchronous random boolean networks are used to represent the traditional condition action production system rules. it is shown possible to use self adaptive open ended evolution to design an ensemble of such discrete dynamical systems within xcs to solve a number of well known test problems.
fuzzy dynamical genetic programming in xcsf
a number of representation schemes have been presented for use within learning classifier systems ranging from binary encodings to neural networks and more recently dynamical genetic programming dgp . this paper presents results from an investigation into using a fuzzy dgp representation within the xcsf learning classifier system. in particular asynchronous fuzzy logic networks are used to represent the traditional condition action production system rules. it is shown possible to use self adaptive open ended evolution to design an ensemble of such fuzzy dynamical systems within xcsf to solve several well known continuous valued test problems.
learning based procedural content generation
procedural content generation pcg has recently become one of the hottest topics in computational intelligence and ai game researches. among a variety of pcg techniques search based approaches overwhelmingly dominate pcg development at present. while sbpcg leads to promising results and successful applications it poses a number of challenges ranging from representation to evaluation of the content being generated. in this paper we present an alternative yet generic pcg framework named learning based procedure content generation lbpcg to provide potential solutions to several challenging problems in existing pcg techniques. by exploring and exploiting information gained in game development and public beta test via data driven learning our framework can generate robust content adaptable to end user or target players on line with minimal interruption to their experience. furthermore we develop enabling techniques to implement the various models required in our framework. for a proof of concept we have developed a prototype based on the classic open source first person shooter game quake. simulation results suggest that our framework is promising in generating quality content.
systematic n tuple networks for position evaluation exceeding 90 in the othello league
n tuple networks have been successfully used as position evaluation functions for board games such as othello or connect four. the effectiveness of such networks depends on their architecture which is determined by the placement of constituent n tuples sequences of board locations providing input to the network. the most popular method of placing n tuples consists in randomly generating a small number of long snake shaped board location sequences. in comparison we show that learning n tuple networks is significantly more effective if they involve a large number of systematically placed short straight n tuples. moreover we demonstrate that in order to obtain the best performance and the steepest learning curve for othello it is enough to use n tuples of size just 2 yielding a network consisting of only 288 weights. the best such network evolved in this study has been evaluated in the online othello league obtaining the performance of nearly 96 more than any other player to date.
towards a self organized agent based simulation model for exploration of human synaptic connections
in this paper the early design of our self organized agent based simulation model for exploration of synaptic connections that faithfully generates what is observed in natural situation is given. while we take inspiration from neuroscience our intent is not to create a veridical model of processes in neurodevelopmental biology nor to represent a real biological system. instead our goal is to design a simulation model that learns acting in the same way of human nervous system by using findings on human subjects using reflex methodologies in order to estimate unknown connections.
motion planning of an autonomous mobile robot using artificial neural network
the paper presents the electronic design and motion planning of a robot based on decision making regarding its straight motion and precise turn using artificial neural network ann . the ann helps in learning of robot so that it performs motion autonomously. the weights calculated are implemented in microcontroller. the performance has been tested to be excellent.
learning bayesian network equivalence classes with ant colony optimization
bayesian networks are a useful tool in the representation of uncertain knowledge. this paper proposes a new algorithm called aco e to learn the structure of a bayesian network. it does this by conducting a search through the space of equivalence classes of bayesian networks using ant colony optimization aco . to this end two novel extensions of traditional aco techniques are proposed and implemented. firstly multiple types of moves are allowed. secondly moves can be given in terms of indices that are not based on construction graph nodes. the results of testing show that aco e performs better than a greedy search and other state of the art and metaheuristic algorithms whilst searching in the space of equivalence classes.
probabilistic neural programs
we present probabilistic neural programs a framework for program induction that permits flexible specification of both a computational model and inference algorithm while simultaneously enabling the use of deep neural networks. probabilistic neural programs combine a computation graph for specifying a neural network with an operator for weighted nondeterministic choice. thus a program describes both a collection of decisions as well as the neural network architecture used to make each one. we evaluate our approach on a challenging diagram question answering task where probabilistic neural programs correctly execute nearly twice as many programs as a baseline model.
cognitive deep machine can train itself
machine learning is making substantial progress in diverse applications. the success is mostly due to advances in deep learning. however deep learning can make mistakes and its generalization abilities to new tasks are questionable. we ask when and how one can combine network outputs when i details of the observations are evaluated by learned deep components and ii facts and confirmation rules are available in knowledge based systems. we show that in limited contexts the required number of training samples can be low and self improvement of pre trained networks in more general context is possible. we argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods an important requirement for engineering applications. we argue that supervised learning of labels may be fully eliminated under certain conditions a component based architecture together with a knowledge based system can train itself and provide high quality answers. we demonstrate these concepts on the state farm distracted driver detection benchmark. we argue that the view of the study panel 2016 may overestimate the requirements on years of focused research and careful unique construction for ai systems .
summary terpret a probabilistic programming language for program induction
we study machine learning formulations of inductive program synthesis that is given input output examples synthesize source code that maps inputs to corresponding outputs. our key contribution is terpret a domain specific language for expressing program synthesis problems. a terpret model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs. the inference task is to observe a set of input output examples and infer the underlying program. from a terpret model we automatically perform inference using four different back ends gradient descent thus each terpret model can be seen as defining a differentiable interpreter linear program lp relaxations for graphical models discrete satisfiability solving and the sketch program synthesis system. terpret has two main benefits. first it enables rapid exploration of a range of domains program representations and interpreter models. second it separates the model specification from the inference algorithm allowing proper comparisons between different approaches to inference. we illustrate the value of terpret by developing several interpreter models and performing an extensive empirical comparison between alternative inference algorithms on a variety of program models. to our knowledge this is the first work to compare gradient based search over program space to traditional search based alternatives. our key empirical finding is that constraint solvers dominate the gradient descent and lp based formulations. this is a workshop summary of a longer report at arxiv 1608.04428
learning in the machine random backpropagation and the deep learning channel
random backpropagation rbp is a variant of the backpropagation algorithm for training neural networks where the transpose of the forward matrices are replaced by fixed random matrices in the calculation of the weight updates. it is remarkable both because of its effectiveness in spite of using random matrices to communicate error information and because it completely removes the taxing requirement of maintaining symmetric weights in a physical neural system. to better understand random backpropagation we first connect it to the notions of local learning and learning channels. through this connection we derive several alternatives to rbp including skipped rbp srpb adaptive rbp arbp sparse rbp and their combinations e.g. asrbp and analyze their computational complexity. we then study their behavior through simulations using the mnist and cifar 10 bechnmark datasets. these simulations show that most of these variants work robustly almost as well as backpropagation and that multiplication by the derivatives of the activation functions is important. as a follow up we study also the low end of the number of bits required to communicate error information over the learning channel. we then provide partial intuitive explanations for some of the remarkable properties of rbp and its variations. finally we prove several mathematical results including the convergence to fixed points of linear chains of arbitrary length the convergence to fixed points of linear autoencoders with decorrelated data the long term existence of solutions for linear systems with a single hidden layer and convergence in special cases and the convergence to fixed points of non linear chains when the derivative of the activation functions is included.
highway and residual networks learn unrolled iterative estimation
the past year saw the introduction of new architectures such as highway networks and residual networks which for the first time enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. while depth of representation has been posited as a primary reason for their success there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer. in this report we argue that this view is incomplete and does not adequately explain several recent findings. we propose an alternative viewpoint based on unrolled iterative estimation a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. we demonstrate that this viewpoint directly leads to the construction of highway and residual networks. finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.
deep neural heart rate variability analysis
despite of the pain and limited accuracy of blood tests for early recognition of cardiovascular disease they dominate risk screening and triage. on the other hand heart rate variability is non invasive and cheap but not considered accurate enough for clinical practice. here we tackle heart beat interval based classification with deep learning. we introduce an end to end differentiable hybrid architecture consisting of a layer of biological neuron models of cardiac dynamics modified fitzhugh nagumo neurons and several layers of a standard feed forward neural network. the proposed model is evaluated on ecgs from 474 stable at risk coronary artery disease patients and 1172 chest pain patients of an emergency department. we show that it can significantly outperform models based on traditional heart rate variability predictors as well as approaching or in some cases outperforming clinical blood tests based only on 60 seconds of inter beat intervals.
a neural network approach to ordinal regression
ordinal regression is an important type of learning which has properties of both classification and regression. here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories. our approach is a generalization of the perceptron method for ordinal regression. on several benchmark datasets our method nnrank outperforms a neural network classification method. compared with the ordinal regression methods using gaussian processes and support vector machines nnrank achieves comparable performance. moreover nnrank has the advantages of traditional neural networks learning in both online and batch modes handling very large training datasets and making rapid predictions. these features make nnrank a useful and complementary tool for large scale data processing tasks such as information retrieval web page ranking collaborative filtering and protein ranking in bioinformatics.
computational model of music sight reading a reinforcement learning approach
although the music sight reading process has been studied from the cognitive psychology view points but the computational learning methods like the reinforcement learning have not yet been used to modeling of such processes. in this paper with regards to essential properties of our specific problem we consider the value function concept and will indicate that the optimum policy can be obtained by the method we offer without to be getting involved with computing of the complex value functions. also we will offer a normative behavioral model for the interaction of the agent with the musical pitch environment and by using a slightly different version of partially observable markov decision processes we will show that our method helps for faster learning of state action pairs in our implemented agents.
using artificial bee colony algorithm for mlp training on earthquake time series data prediction
nowadays computer scientists have shown the interest in the study of social insect s behaviour in neural networks area for solving different combinatorial and statistical problems. chief among these is the artificial bee colony abc algorithm. this paper investigates the use of abc algorithm that simulates the intelligent foraging behaviour of a honey bee swarm. multilayer perceptron mlp trained with the standard back propagation algorithm normally utilises computationally intensive training algorithms. one of the crucial problems with the backpropagation bp algorithm is that it can sometimes yield the networks with suboptimal weights because of the presence of many local optima in the solution space. to overcome abc algorithm used in this work to train mlp learning the complex behaviour of earthquake time series data trained by bp the performance of mlp abc is benchmarked against mlp training with the standard bp. the experimental result shows that mlp abc performance is better than mlp bp for time series data.
multiple chaotic central pattern generators with learning for legged locomotion and malfunction compensation
an originally chaotic system can be controlled into various periodic dynamics. when it is implemented into a legged robot s locomotion control as a central pattern generator cpg sophisticated gait patterns arise so that the robot can perform various walking behaviors. however such a single chaotic cpg controller has difficulties dealing with leg malfunction. specifically in the scenarios presented here its movement permanently deviates from the desired trajectory. to address this problem we extend the single chaotic cpg to multiple cpgs with learning. the learning mechanism is based on a simulated annealing algorithm. in a normal situation the cpgs synchronize and their dynamics are identical. with leg malfunction or disability the cpgs lose synchronization leading to independent dynamics. in this case the learning mechanism is applied to automatically adjust the remaining legs oscillation frequencies so that the robot adapts its locomotion to deal with the malfunction. as a consequence the trajectory produced by the multiple chaotic cpgs resembles the original trajectory far better than the one produced by only a single cpg. the performance of the system is evaluated first in a physical simulation of a quadruped as well as a hexapod robot and finally in a real six legged walking machine called amosii. the experimental results presented here reveal that using multiple cpgs with learning is an effective approach for adaptive locomotion generation where for instance different body parts have to perform independent movements for malfunction compensation.
teaching deep convolutional neural networks to play go
mastering the game of go has remained a long standing challenge to the field of ai. modern computer go systems rely on processing millions of possible future positions to play well but intuitively a stronger and more humanlike way to play the game would be to rely on pattern recognition abilities rather then brute force computation. following this sentiment we train deep convolutional neural networks to play go by training them to predict the moves made by expert go players. to solve this problem we introduce a number of novel techniques including a method of tying weights in the network to hard code symmetries that are expect to exist in the target function and demonstrate in an ablation study they considerably improve performance. our final networks are able to achieve move prediction accuracies of 41.1 and 44.4 on two different go datasets surpassing previous state of the art on this task by significant margins. additionally while previous move prediction programs have not yielded strong go playing programs we show that the networks trained in this work acquired high levels of skill. our convolutional neural networks can consistently defeat the well known go program gnu go indicating it is state of the art among programs that do not use monte carlo tree search. it is also able to win some games against state of the art go playing program fuego while using a fraction of the play time. this success at playing go indicates high level principles of the game were learned.
polyphonic music generation by modeling temporal dependencies using a rnn dbn
in this paper we propose a generic technique to model temporal dependencies and sequences using a combination of a recurrent neural network and a deep belief network. our technique rnn dbn is an amalgamation of the memory state of the rnn that allows it to provide temporal information and a multi layer dbn that helps in high level representation of the data. this makes rnn dbns ideal for sequence generation. further the use of a dbn in conjunction with the rnn makes this model capable of significantly more complex data representation than an rbm. we apply this technique to the task of polyphonic music generation.
massively parallel methods for deep reinforcement learning
we present the first massively distributed architecture for deep reinforcement learning. this architecture uses four main components parallel actors that generate new behaviour parallel learners that are trained from stored experience a distributed neural network to represent the value function or behaviour policy and a distributed store of experience. we used our architecture to implement the deep q network algorithm dqn . our distributed algorithm was applied to 49 games from atari 2600 games from the arcade learning environment using identical hyperparameters. our performance surpassed non distributed dqn in 41 of the 49 games and also reduced the wall time required to achieve these results by an order of magnitude on most games.
a genetic algorithm for autonomous navigation in partially observable domain
the problem of autonomous navigation is one of the basic problems for robotics. although in general it may be challenging when an autonomous vehicle is placed into partially observable domain. in this paper we consider simplistic environment model and introduce a navigation algorithm based on learning classifier system.
distributed deep q learning
we propose a distributed deep learning model to successfully learn control policies directly from high dimensional sensory input using reinforcement learning. the model is based on the deep q network a convolutional neural network trained with a variant of q learning. its input is raw pixels and its output is a value function estimating future rewards from taking an action given a system state. to distribute the deep q network training we adapt the distbelief software framework to the context of efficiently training reinforcement learning agents. as a result the method is completely asynchronous and scales well with the number of machines. we demonstrate that the deep q network agent receiving only the pixels and the game score as inputs was able to achieve reasonable success on a simple game with minimal parameter tuning.
lifted relational neural networks
we propose a method combining relational logic representations with neural network learning. a general lifted architecture possibly reflecting some background domain knowledge is described through relational rules which may be handcrafted or learned. the relational rule set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. different networks corresponding to different examples share their weights which co evolve during training by stochastic gradient descent algorithm. the framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method.
giraffe using deep reinforcement learning to play chess
this report presents giraffe a chess engine that uses self play to discover all its domain specific knowledge with minimal hand crafted knowledge given by the programmer. unlike previous attempts using machine learning only to perform parameter tuning on hand crafted evaluation functions giraffe s learning system also performs automatic feature extraction and pattern recognition. the trained evaluation function performs comparably to the evaluation functions of state of the art chess engines all of which containing thousands of lines of carefully hand crafted pattern recognizers tuned over many years by both computer chess experts and human chess masters. giraffe is the most successful attempt thus far at using end to end machine learning to play chess.
attention with intention for a neural network conversation model
in a conversation or a dialogue process attention and intention play intrinsic roles. this paper proposes a neural network based approach that models the attention and intention processes. it essentially consists of three recurrent networks. the encoder network is a word level model representing source side sentences. the intention network is a recurrent network that models the dynamics of the intention process. the decoder network is a recurrent network produces responses to the input from the source side. it is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words when predicting a symbol in the response. the model is trained end to end without labeling data. experiments show that this model generates natural responses to user inputs.
deep reinforcement learning in parameterized action space
recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. however to the best of our knowledge no previous work has succeeded at using deep neural networks in structured parameterized continuous action spaces. to fill this gap this paper focuses on learning within the domain of simulated robocup soccer which features a small set of discrete action types each of which is parameterized with continuous variables. the best learned agent can score goals more reliably than the 2012 robocup champion agent. as such this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space mdps.
mazebase a sandbox for learning from games
this paper introduces mazebase an environment for simple 2d games designed as a sandbox for machine learning approaches to reasoning and planning. within it we create 10 simple games embodying a range of algorithmic tasks e.g. if then statements or set negation . a variety of neural models fully connected convolutional network memory network are deployed via reinforcement learning on these games with and without a procedurally generated curriculum. despite the tasks simplicity the performance of the models is far from optimal suggesting directions for future development. we also demonstrate the versatility of mazebase by using it to emulate small combat scenarios from starcraft. models trained on the mazebase version can be directly applied to starcraft where they consistently beat the in game ai.
on learning to think algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models
this paper addresses the general problem of reinforcement learning rl in partially observable environments. in 2013 our large rl recurrent neural networks rnns learned from scratch to drive simulated cars from high dimensional video input. however real brains are more powerful in many ways. in particular they learn a predictive model of their initially unknown environment and somehow use it for abstract e.g. hierarchical planning and reasoning. guided by algorithmic information theory we describe rnn based ais rnnais designed to do the same. such an rnnai can be trained on never ending sequences of tasks some of them provided by the user others invented by the rnnai itself in a curious playful fashion to improve its rnn based world model. unlike our previous model building rnn based rl machines dating back to 1990 the rnnai learns to actively query its model for abstract reasoning and planning and decision making essentially learning to think. the basic ideas of this report can be applied to many other cases where one rnn like system exploits the algorithmic information content of another. they are taken from a grant proposal submitted in fall 2014 and also explain concepts such as mirror neurons. experimental results will be described in separate papers.
an empirical comparison of neural architectures for reinforcement learning in partially observable environments
this paper explores the performance of fitted neural q iteration for reinforcement learning in several partially observable environments using three recurrent neural network architectures long short term memory gated recurrent unit and mut1 a recurrent neural architecture evolved from a pool of several thousands candidate architectures. a variant of fitted q iteration based on advantage values instead of q values is also explored. the results show that gru performs significantly better than lstm and mut1 for most of the problems considered requiring less training episodes and less cpu time before learning a very good policy. advantage learning also tends to produce better results.
predicting clinical events by combining static and dynamic information using recurrent neural networks
in clinical data sets we often find static information e.g. patient gender blood type etc. combined with sequences of data that are recorded during multiple hospital visits e.g. medications prescribed tests performed etc. . recurrent neural networks rnns have proven to be very successful for modelling sequences of data in many areas of machine learning. in this work we present an approach based on rnns specifically designed for the clinical domain that combines static and dynamic information in order to predict future events. we work with a database collected in the charit e hospital in berlin that contains complete information concerning patients that underwent a kidney transplantation. after the transplantation three main endpoints can occur rejection of the kidney loss of the kidney and death of the patient. our goal is to predict based on information recorded in the electronic health record of each patient whether any of those endpoints will occur within the next six or twelve months after each visit to the clinic. we compared different types of rnns that we developed for this work with a model based on a feedforward neural network and a logistic regression model. we found that the rnn that we developed based on gated recurrent units provides the best performance for this task. we also used the same models for a second task i.e. next event prediction and found that here the model based on a feedforward neural network outperformed the other models. our hypothesis is that long term dependencies are not as relevant in this task.
weight normalization a simple reparameterization to accelerate training of deep neural networks
we present weight normalization a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. by reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. this means that our method can also be applied successfully to recurrent models such as lstms and to noise sensitive applications such as deep reinforcement learning or generative models for which batch normalization is less well suited. although our method is much simpler it still provides much of the speed up of full batch normalization. in addition the computational overhead of our method is lower permitting more optimization steps to be taken in the same amount of time. we demonstrate the usefulness of our method on applications in supervised image recognition generative modelling and deep reinforcement learning.
bounded rational decision making in feedforward neural networks
bounded rational decision makers transform sensory input into motor output under limited computational resources. mathematically such decision makers can be modeled as information theoretic channels with limited transmission rate. here we apply this formalism for the first time to multilayer feedforward neural networks. we derive synaptic weight update rules for two scenarios where either each neuron is considered as a bounded rational decision maker or the network as a whole. in the update rules bounded rationality translates into information theoretically motivated types of regularization in weight space. in experiments on the mnist benchmark classification task for handwritten digits we show that such information theoretic regularization successfully prevents overfitting across different architectures and attains results that are competitive with other recent techniques like dropout dropconnect and bayes by backprop for both ordinary and convolutional neural networks.
lie access neural turing machine
following the recent trend in explicit neural memory structures we present a new design of an external memory wherein memories are stored in an euclidean key space mathbb r n . an lstm controller performs read and write via specialized read and write heads. it can move a head by either providing a new address in the key space aka random access or moving from its previous position via a lie group action aka lie access . in this way the l and r instructions of a traditional turing machine are generalized to arbitrary elements of a fixed lie group action. for this reason we name this new model the lie access neural turing machine or lantm. we tested two different configurations of lantm against an lstm baseline in several basic experiments. we found the right configuration of lantm to outperform the baseline in all of our experiments. in particular we trained lantm on addition of k digit numbers for 2 le k le 16 but it was able to generalize almost perfectly to 17 le k le 32 all with the number of parameters 2 orders of magnitude below the lstm baseline.
towards machine intelligence
there exists a theory of a single general purpose learning algorithm which could explain the principles of its operation. this theory assumes that the brain has some initial rough architecture a small library of simple innate circuits which are prewired at birth and proposes that all significant mental algorithms can be learned. given current understanding and observations this paper reviews and lists the ingredients of such an algorithm from both architectural and functional perspectives.
dynamic frame skip deep q network
deep reinforcement learning methods have achieved state of the art performance in learning control policies for the games in the atari 2600 domain. one of the important parameters in the arcade learning environment ale is the frame skip rate. it decides the granularity at which agents can control game play. a frame skip value of k allows the agent to repeat a selected action k number of times. the current state of the art architectures like deep q network dqn and dueling network architectures dudqn consist of a framework with a static frame skip rate where the action output from the network is repeated for a fixed number of frames regardless of the current state. in this paper we propose a new architecture dynamic frame skip deep q network dfdqn which makes the frame skip rate a dynamic learnable parameter. this allows us to choose the number of times an action is to be repeated based on the current state. we show empirically that such a setting improves the performance on relatively harder games like seaquest.
programming with a differentiable forth interpreter
given that in practice training data is scarce for all but a small set of problems a core question is how to incorporate prior knowledge into a model. in this paper we consider the case of prior procedural knowledge for neural networks such as knowing how a program should traverse a sequence but not what local actions should be performed at each step. to this end we present an end to end differentiable interpreter for the programming language forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input output data. we can optimise this behaviour directly through gradient descent techniques on user specified objectives and also integrate the program into any larger neural computation graph. we show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. when connected to outputs of an lstm and trained jointly our interpreter achieves state of the art accuracy for end to end reasoning about quantities expressed in natural language stories.
generative choreography using deep learning
recent advances in deep learning have enabled the extraction of high level features from raw sensor data which has opened up new possibilities in many different fields including computer generated choreography. in this paper we present a system chor rnn for generating novel choreographic material in the nuanced choreographic language and style of an individual choreographer. it also shows promising results in producing a higher level compositional cohesion rather than just generating sequences of movement. at the core of chor rnn is a deep recurrent neural network trained on raw motion capture data and that can generate new dance sequences for a solo dancer. chor rnn can be used for collaborative human machine choreography or as a creative catalyst serving as inspiration for a choreographer.
logic tensor networks deep learning and logical reasoning from data and knowledge
we propose logic tensor networks a uniform framework for integrating automatic learning and reasoning. a logic formalism called real logic is defined on a first order language whereby formulas have truth value in the interval 0 1 and semantics defined concretely on the domain of real numbers. logical constants are interpreted as feature vectors of real numbers. real logic promotes a well founded integration of deductive reasoning on a knowledge base and efficient data driven relational machine learning. we show how real logic can be implemented in deep tensor neural networks with the use of google s tensorflow primitives. the paper concludes with experiments applying logic tensor networks on a simple but representative example of knowledge completion.
identifying and harnessing the building blocks of machine learning pipelines for sensible initialization of a data science automation tool
as data science continues to grow in popularity there will be an increasing need to make data science tools more scalable flexible and accessible. in particular automated machine learning automl systems seek to automate the process of designing and optimizing machine learning pipelines. in this chapter we present a genetic programming based automl system called tpot that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification problem. further we analyze a large database of pipelines that were previously used to solve various supervised classification problems and identify 100 short series of machine learning operations that appear the most frequently which we call the building blocks of machine learning pipelines. we harness these building blocks to initialize tpot with promising solutions and find that this sensible initialization method significantly improves tpot s performance on one benchmark at no cost of significantly degrading performance on the others. thus sensible initialization with machine learning pipeline building blocks shows promise for gp based automl systems and should be further refined in future work.
neuroevolution based inverse reinforcement learning
the problem of learning from demonstration is targeted at learning to perform tasks based on observed examples. one approach to learning from demonstration is inverse reinforcement learning in which actions are observed to infer rewards. this work combines a feature based state evaluation approach to inverse reinforcement learning with neuroevolution a paradigm for modifying neural networks based on their performance on a given task. neural networks are used to learn from a demonstrated expert policy and are evolved to generate a policy similar to the demonstration. the algorithm is discussed and evaluated against competitive feature based inverse reinforcement learning approaches. at the cost of execution time neural networks allow for non linear combinations of features in state evaluations. these valuations may correspond to state value or state reward. this results in better correspondence to observed examples as opposed to using linear combinations. this work also extends existing work on bayesian non parametric feature construction for inverse reinforcement learning by using non linear combinations of intermediate data to improve performance. the algorithm is observed to be specifically suitable for a linearly solvable non deterministic markov decision processes in which multiple rewards are sparsely scattered in state space. a conclusive performance hierarchy between evaluated algorithms is presented.
terpret a probabilistic programming language for program induction
we study machine learning formulations of inductive program synthesis given input output examples we try to synthesize source code that maps inputs to corresponding outputs. our aims are to develop new machine learning approaches based on neural networks and graphical models and to understand the capabilities of machine learning techniques relative to traditional alternatives such as those based on constraint solving from the programming languages community. our key contribution is the proposal of terpret a domain specific language for expressing program synthesis problems. terpret is similar to a probabilistic programming language a model is composed of a specification of a program representation declarations of random variables and an interpreter describing how programs map inputs to outputs a model connecting unknowns to observations . the inference task is to observe a set of input output examples and infer the underlying program. terpret has two main benefits. first it enables rapid exploration of a range of domains program representations and interpreter models. second it separates the model specification from the inference algorithm allowing like to like comparisons between different approaches to inference. from a single terpret specification we automatically perform inference using four different back ends. these are based on gradient descent linear program lp relaxations for graphical models discrete satisfiability solving and the sketch program synthesis system. we illustrate the value of terpret by developing several interpreter models and performing an empirical comparison between alternative inference algorithms. our key empirical finding is that constraint solvers dominate the gradient descent and lp based formulations. we conclude with suggestions for the machine learning community to make progress on program synthesis.
multi label classification method based on extreme learning machines
in this paper an extreme learning machine elm based technique for multi label classification problems is proposed and discussed. in multi label classification each of the input data samples belongs to one or more than one class labels. the traditional binary and multi class classification problems are the subset of the multi label problem with the number of labels corresponding to each sample limited to one. the proposed elm based multi label classification technique is evaluated with six different benchmark multi label datasets from different domains such as multimedia text and biology. a detailed comparison of the results is made by comparing the proposed method with the results from nine state of the arts techniques for five different evaluation metrics. the nine methods are chosen from different categories of multi label methods. the comparative results shows that the proposed extreme learning machine based multi label classification technique is a better alternative than the existing state of the art methods for multi label problems.
a novel online real time classifier for multi label data streams
in this paper a novel extreme learning machine based online multi label classifier for real time data streams is proposed. multi label classification is one of the actively researched machine learning paradigm that has gained much attention in the recent years due to its rapidly increasing real world applications. in contrast to traditional binary and multi class classification multi label classification involves association of each of the input samples with a set of target labels simultaneously. there are no real time online neural network based multi label classifier available in the literature. in this paper we exploit the inherent nature of high speed exhibited by the extreme learning machines to develop a novel online real time classifier for multi label data streams. the developed classifier is experimented with datasets from different application domains for consistency performance and speed. the experimental studies show that the proposed method outperforms the existing state of the art techniques in terms of speed and accuracy and can classify multi label data streams in real time.
a novel progressive learning technique for multi class classification
in this paper a progressive learning technique for multi class classification is proposed. this newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. whenever a new class non native to the knowledge learnt thus far is encountered the neural network structure gets remodeled automatically by facilitating new neurons and interconnections and the parameters are calculated in such a way that it retains the knowledge learnt thus far. this technique is suitable for real world applications where the number of classes is often unknown and online learning from real time data is required. the consistency and the complexity of the progressive learning technique are analyzed. several standard datasets are used to evaluate the performance of the developed technique. a comparative study shows that the developed technique is superior.
a novel online multi label classifier for high speed streaming data applications
in this paper a high speed online neural network classifier based on extreme learning machines for multi label classification is proposed. in multi label classification each of the input data sample belongs to one or more than one of the target labels. the traditional binary and multi class classification where each sample belongs to only one target class forms the subset of multi label classification. multi label classification problems are far more complex than binary and multi class classification problems as both the number of target labels and each of the target labels corresponding to each of the input samples are to be identified. the proposed work exploits the high speed nature of the extreme learning machines to achieve real time multi label classification of streaming data. a new threshold based online sequential learning algorithm is proposed for high speed and streaming data classification of multi label problems. the proposed method is experimented with six different datasets from different application domains such as multimedia text and biology. the hamming loss accuracy training time and testing time of the proposed technique is compared with nine different state of the art methods. experimental studies shows that the proposed technique outperforms the existing multi label classifiers in terms of performance and speed.
ternary neural networks for resource efficient ai applications
the computation and storage requirements for deep neural networks dnns are usually high. this issue limits their deployability on ubiquitous computing devices such as smart phones wearables and autonomous drones. in this paper we propose ternary neural networks tnns in order to make deep learning more resource efficient. we train these tnns using a teacher student approach based on a novel layer wise greedy methodology. thanks to our two stage training procedure the teacher network is still able to use state of the art methods such as dropout and batch normalization to increase accuracy and reduce training time. using only ternary weights and activations the student ternary network learns to mimic the behavior of its teacher network without using any multiplication. unlike its 1 1 binary counterparts a ternary neural network inherently prunes the smaller weights by setting them to zero during training. this makes them sparser and thus more energy efficient. we design a purpose built hardware architecture for tnns and implement it on fpga and asic. we evaluate tnns on several benchmark datasets and demonstrate up to 3.1x better energy efficiency with respect to the state of the art while also improving accuracy.
fitted learning models with awareness of their limits
though deep learning has pushed the boundaries of classification forward in recent years hints of the limits of standard classification have begun to emerge. problems such as fooling adding new classes over time and the need to retrain learning models only for small changes to the original problem all point to a potential shortcoming in the classic classification regime where a comprehensive a priori knowledge of the possible classes or concepts is critical. without such knowledge classifiers misjudge the limits of their knowledge and overgeneralization therefore becomes a serious obstacle to consistent performance. in response to these challenges this paper extends the classic regime by reframing classification instead with the assumption that concepts present in the training set are only a sample of the hypothetical final set of concepts. to bring learning models into this new paradigm a novel elaboration of standard architectures called the competitive overcomplete output layer cool neural network is introduced. experiments demonstrate the effectiveness of cool by applying it to fooling separable concept learning one class neural networks and standard classification benchmarks. the results suggest that unlike conventional classifiers the amount of generalization in cool networks can be tuned to match the problem.
learning to learn with backpropagation of hebbian plasticity
hebbian plasticity is a powerful principle that allows biological brains to learn from their lifetime experience. by contrast artificial neural networks trained with backpropagation generally have fixed connection weights that do not change once training is complete. while recent methods can endow neural networks with long term memories hebbian plasticity is currently not amenable to gradient descent. here we derive analytical expressions for activity gradients in neural networks with hebbian plastic connections. using these expressions we can use backpropagation to train not just the baseline weights of the connections but also their plasticity. as a result the networks learn how to learn in order to solve the problem at hand the trained networks automatically perform fast learning of unpredictable environmental features during their lifetime expanding the range of solvable problems. we test the algorithm on various on line learning tasks including pattern completion one shot learning and reversal learning. the algorithm successfully learns how to learn the relevant associations from one shot instruction and fine tunes the temporal dynamics of plasticity to allow for continual learning in response to changing environmental parameters. we conclude that backpropagation of hebbian plasticity offers a powerful model for lifelong learning.
learning by stimulation avoidance a principle to control spiking neural networks dynamics
learning based on networks of real neurons and by extension biologically inspired models of neural networks has yet to find general learning rules leading to widespread applications. in this paper we argue for the existence of a principle allowing to steer the dynamics of a biologically inspired neural network. using carefully timed external stimulation the network can be driven towards a desired dynamical state. we term this principle learning by stimulation avoidance lsa . we demonstrate through simulation that the minimal sufficient conditions leading to lsa in artificial networks are also sufficient to reproduce learning results similar to those obtained in biological neurons by shahaf and marom 1 . we examine the mechanism s basic dynamics in a reduced network and demonstrate how it scales up to a network of 100 neurons. we show that lsa has a higher explanatory power than existing hypotheses about the response of biological neural networks to external simulation and can be used as a learning rule for an embodied application learning of wall avoidance by a simulated robot. the surge in popularity of artificial neural networks is mostly directed to disembodied models of neurons with biologically irrelevant dynamics to the authors knowledge this is the first work demonstrating sensory motor learning with random spiking networks through pure hebbian learning.
surprisal driven zoneout
we propose a novel method of regularization for recurrent neural networks called suprisal driven zoneout. in this method states zoneout maintain their previous value rather than updating when the suprisal discrepancy between the last state s prediction and target is small. thus regularization is adaptive and input driven on a per neuron basis. we demonstrate the effectiveness of this idea by achieving state of the art bits per character of 1.31 on the hutter prize wikipedia dataset significantly reducing the gap to the best known highly engineered compression methods.
neural architecture search with reinforcement learning
neural networks are powerful and flexible models that work well for many difficult learning tasks in image speech and natural language understanding. despite their success neural networks are still hard to design. in this paper we use a recurrent network to generate the model descriptions of neural networks and train this rnn with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. on the cifar 10 dataset our method starting from scratch can design a novel network architecture that rivals the best human invented architecture in terms of test set accuracy. our cifar 10 model achieves a test error rate of 3.65 which is 0.09 percent better and 1.05x faster than the previous state of the art model that used a similar architectural scheme. on the penn treebank dataset our model can compose a novel recurrent cell that outperforms the widely used lstm cell and other state of the art baselines. our cell achieves a test set perplexity of 62.4 on the penn treebank which is 3.6 perplexity better than the previous state of the art model. the cell can also be transferred to the character language modeling task on ptb and achieves a state of the art perplexity of 1.214.
emergence of foveal image sampling from learning to attend in visual scenes
we describe a neural attention model with a learnable retinal sampling lattice. the model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. we explore the tiling properties that emerge in the model s retinal sampling lattice after training. specifically we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina with a high resolution region in the fovea surrounded by a low resolution periphery. furthermore we find conditions where these emergent properties are amplified or eliminated providing clues to their function.
long timescale credit assignment in neuralnetworks with external memory
credit assignment in traditional recurrent neural networks usually involves back propagating through a long chain of tied weight matrices. the length of this chain scales linearly with the number of time steps as the same network is run at each time step. this creates many problems such as vanishing gradients that have been well studied. in contrast a nnem s architecture recurrent activity doesn t involve a long chain of activity though some architectures such as the ntm do utilize a traditional recurrent architecture as a controller . rather the externally stored embedding vectors are used at each time step but no messages are passed from previous time steps. this means that vanishing gradients aren t a problem as all of the necessary gradient paths are short. however these paths are extremely numerous one per embedding vector in memory and reused for a very long time until it leaves the memory . thus the forward pass information of each memory must be stored for the entire duration of the memory. this is problematic as this additional storage far surpasses that of the actual memories to the extent that large memories on infeasible to back propagate through in high dimensional settings. one way to get around the need to hold onto forward pass information is to recalculate the forward pass whenever gradient information is available. however if the observations are too large to store in the domain of interest direct reinstatement of a forward pass cannot occur. instead we rely on a learned autoencoder to reinstate the observation and then use the embedding network to recalculate the forward pass. since the recalculated embedding vector is unlikely to perfectly match the one stored in memory we try out 2 approximations to utilize error gradient w.r.t. the vector in memory.
energy saving additive neural network
in recent years machine learning techniques based on neural networks for mobile computing become increasingly popular. classical multi layer neural networks require matrix multiplications at each stage. multiplication operation is not an energy efficient operation and consequently it drains the battery of the mobile device. in this paper we propose a new energy efficient neural network with the universal approximation property over space of lebesgue integrable functions. this network called additive neural network is very suitable for mobile computing. the neural structure is based on a novel vector product definition called ef operator that permits a multiplier free implementation. in ef operation the product of two real numbers is defined as the sum of their absolute values with the sign determined by the sign of the product of the numbers. this product is used to construct a vector product in r n . the vector product induces the l 1 norm. the proposed additive neural network successfully solves the xor problem. the experiments on mnist dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi layer perceptron and convolutional neural networks lenet .
learning to repeat fine grained action repetition for deep reinforcement learning
reinforcement learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. traditionally such algorithms make decisions i.e. select actions to execute at every single time step of the agent environment interactions. in this paper we propose a novel framework fine grained action repetition figar which enables the agent to decide the action as well as the time scale of repeating it. figar can be used for improving any deep reinforcement learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space. we empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains asynchronous advantage actor critic in the atari 2600 domain trust region policy optimization in mujoco domain and deep deterministic policy gradients in the torcs car racing domain.
survey of reasoning using neural networks
reason and inference require process as well as memory skills by humans. neural networks are able to process tasks like image recognition better than humans but in memory aspects are still limited by attention mechanism size . recurrent neural network rnn and it s modified version lstm are able to solve small memory contexts but as context becomes larger than a threshold it is difficult to use them. the solution is to use large external memory. still it poses many challenges like how to train neural networks for discrete memory representation how to describe long term dependencies in sequential data etc. most prominent neural architectures for such tasks are memory networks inference components combined with long term memory and neural turing machines neural networks using external memory resources. also additional techniques like attention mechanism end to end gradient descent on discrete memory representation are needed to support these solutions. preliminary results of above neural architectures on simple algorithms sorting copying and question answering based on story dialogs application are comparable with the state of the art. in this paper i explain these architectures in general the additional techniques used and the results of their application.
one shot imitation learning
imitation learning has been commonly applied to solve different tasks in isolation. this usually requires either careful feature engineering or a significant number of samples. this is far from what we desire ideally robots should be able to learn from very few demonstrations of any given task and instantly generalize to new situations of the same task without requiring task specific engineering. in this paper we propose a meta learning framework for achieving such capability which we call one shot imitation learning. specifically we consider the setting where there is a very large set of tasks and each task has many instantiations. for example a task could be to stack all blocks on a table into a single tower another task could be to place all blocks on a table into two block towers etc. in each case different instances of the task would consist of different sets of blocks with different initial states. at training time our algorithm is presented with pairs of demonstrations for a subset of all tasks. a neural net is trained that takes as input one demonstration and the current state which initially is the initial state of the other demonstration of the pair and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. at test time a demonstration of a single instance of a new task is presented and the neural net is expected to perform well on new instances of this new task. the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. we anticipate that by training this model on a much greater variety of tasks and settings we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. videos available at https bit.ly nips2017 oneshot .
deep learning for explicitly modeling optimization landscapes
in all but the most trivial optimization problems the structure of the solutions exhibit complex interdependencies between the input parameters. decades of research with stochastic search techniques has shown the benefit of explicitly modeling the interactions between sets of parameters and the overall quality of the solutions discovered. we demonstrate a novel method based on learning deep networks to model the global landscapes of optimization problems. to represent the search space concisely and accurately the deep networks must encode information about the underlying parameter interactions and their contributions to the quality of the solution. once the networks are trained the networks are probed to reveal parameter combinations with high expected performance with respect to the optimization task. these estimates are used to initialize fast randomized local search algorithms which in turn expose more information about the search space that is subsequently used to refine the models. we demonstrate the technique on multiple optimization problems that have arisen in a variety of real world domains including packing graphics job scheduling layout and compression. the problems include combinatoric search spaces discontinuous and highly non linear spaces and span binary higher cardinality discrete as well as continuous parameters. strengths limitations and extensions of the approach are extensively discussed and demonstrated.
stochastic neural networks for hierarchical reinforcement learning
deep reinforcement learning has achieved many impressive results in recent years. however tasks with sparse rewards or long horizons continue to pose significant challenges. to tackle these important problems we propose a general framework that first learns useful skills in a pre training environment and then leverages the acquired skills for learning faster in downstream tasks. our approach brings together some of the strengths of intrinsic motivation and hierarchical methods the learning of useful skill is guided by a single proxy reward the design of which requires very minimal domain knowledge about the downstream tasks. then a high level policy is trained on top of these skills providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. to efficiently pre train a large span of skills we use stochastic neural networks combined with an information theoretic regularizer. our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample efficient way and can significantly boost the learning performance uniformly across a wide range of downstream tasks.
batch reinforcement learning on the industrial benchmark first experiences
the particle swarm optimization policy pso p has been recently introduced and proven to produce remarkable results on interacting with academic reinforcement learning benchmarks in an off policy batch based setting. to further investigate the properties and feasibility on real world applications this paper investigates pso p on the so called industrial benchmark ib a novel reinforcement learning rl benchmark that aims at being realistic by including a variety of aspects found in industrial applications like continuous state and action spaces a high dimensional partially observable state space delayed effects and complex stochasticity. the experimental results of pso p on ib are compared to results of closed form control policies derived from the model based recurrent control neural network rcnn and the model free neural fitted q iteration nfq . experiments show that pso p is not only of interest for academic benchmarks but also for real world industrial applications since it also yielded the best performing policy in our ib setting. compared to other well established rl techniques pso p produced outstanding results in performance and robustness requiring only a relatively low amount of effort in finding adequate parameters or making complex design decisions.
end to end differentiable proving
we introduce neural networks for end to end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. these neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in prolog. specifically we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel thereby combining symbolic reasoning with learning subsymbolic vector representations. by using gradient descent the resulting neural network can be trained to infer facts from a given incomplete knowledge base. it learns to i place representations of similar symbols in close proximity in a vector space ii make use of such similarities to prove queries iii induce logical rules and iv use provided and induced logical rules for multi hop reasoning. we demonstrate that this architecture outperforms complex a state of the art neural link prediction model on three out of four benchmark knowledge bases while at the same time inducing interpretable function free first order logic rules.
multi agent actor critic for mixed cooperative competitive environments
we explore deep reinforcement learning methods for multi agent domains. we begin by analyzing the difficulty of traditional algorithms in the multi agent case q learning is challenged by an inherent non stationarity of the environment while policy gradient suffers from a variance that increases as the number of agents grows. we then present an adaptation of actor critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi agent coordination. additionally we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi agent policies. we show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios where agent populations are able to discover various physical and informational coordination strategies.
getting deep recommenders fit bloom embeddings for sparse binary input output networks
recommendation algorithms that incorporate techniques from deep learning are becoming increasingly popular. due to the structure of the data coming from recommendation domains i.e. one hot encoded vectors of item preferences these algorithms tend to have large input and output dimensionalities that dominate their overall size. this makes them difficult to train due to the limited memory of graphical processing units and difficult to deploy on mobile devices with limited hardware. to address these difficulties we propose bloom embeddings a compression technique that can be applied to the input and output of neural network models dealing with sparse high dimensional binary coded instances. bloom embeddings are computationally efficient and do not seriously compromise the accuracy of the model up to 1 5 compression ratios. in some cases they even improve over the original accuracy with relative increases up to 12 . we evaluate bloom embeddings on 7 data sets and compare it against 4 alternative methods obtaining favorable results. we also discuss a number of further advantages of bloom embeddings such as on the fly constant time operation zero or marginal space requirements training time speedups or the fact that they do not require any change to the core model architecture or training configuration.
beyond monte carlo tree search playing go with deep alternative neural network and long term evaluation
monte carlo tree search mcts is extremely popular in computer go which determines each action by enormous simulations in a broad and deep search tree. however human experts select most actions by pattern analysis and careful evaluation rather than brute search of millions of future nteractions. in this paper we propose a computer go system that follows experts way of thinking and playing. our system consists of two parts. the first part is a novel deep alternative neural network dann used to generate candidates of next move. compared with existing deep convolutional neural network dcnn dann inserts recurrent layer after each convolutional layer and stacks them in an alternative manner. we show such setting can preserve more contexts of local features and its evolutions which are beneficial for move prediction. the second part is a long term evaluation lte module used to provide a reliable evaluation of candidates rather than a single probability from move predictor. this is consistent with human experts nature of playing since they can foresee tens of steps to give an accurate estimation of candidates. in our system for each candidate lte calculates a cumulative reward after several future interactions when local variations are settled. combining criteria from the two parts our system determines the optimal choice of next move. for more comprehensive experiments we introduce a new professional go dataset pgd consisting of 253233 professional records. experiments on gogod and pgd datasets show the dann can substantially improve performance of move prediction over pure dcnn. when combining lte our system outperforms most relevant approaches and open engines based on mcts.
hindsight experience replay
dealing with sparse rewards is one of the biggest challenges in reinforcement learning rl . we present a novel technique called hindsight experience replay which allows sample efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. it can be combined with an arbitrary off policy rl algorithm and may be seen as a form of implicit curriculum. we demonstrate our approach on the task of manipulating objects with a robotic arm. in particular we run experiments on three different tasks pushing sliding and pick and place in each case using only binary rewards indicating whether or not the task is completed. our ablation studies show that hindsight experience replay is a crucial ingredient which makes training possible in these challenging environments. we show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.
trial without error towards safe reinforcement learning via human intervention
ai systems are increasingly applied to complex tasks that involve interaction with humans. during training such systems are potentially dangerous as they haven t yet learned to avoid actions that could cause serious harm. how can an ai system explore and learn without making a single mistake that harms humans or otherwise causes serious damage for model free reinforcement learning having a human in the loop and ready to intervene is currently the only way to prevent all catastrophes. we formalize human intervention for rl and show how to reduce the human labor required by training a supervised learner to imitate the human s intervention decisions. we evaluate this scheme on atari games with a deep rl agent being overseen by a human for four hours. when the class of catastrophes is simple we are able to prevent all catastrophes without affecting the agent s learning whereas an rl baseline fails due to catastrophic forgetting . however this scheme is less successful when catastrophes are more complex it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. extrapolating to more challenging environments we show that our implementation would not scale due to the infeasible amount of human labor required . we outline extensions of the scheme that are necessary if we are to train model free agents without a single catastrophe.
reverse curriculum generation for reinforcement learning
many relevant tasks require an agent to reach a certain state or to manipulate objects into a desired configuration. for example we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. these goal oriented tasks present a considerable challenge for reinforcement learning since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task specific reward shaping function to guide the learning agent. instead we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved. the robot is trained in reverse gradually learning to reach the goal from a set of start states increasingly far from the goal. our method automatically generates a curriculum of start states that adapts to the agent s performance leading to efficient training on goal oriented tasks. we demonstrate our approach on difficult simulated navigation and fine grained manipulation problems not solvable by state of the art reinforcement learning methods.
ideological sublations resolution of dialectic in population based optimization
a population based optimization algorithm was designed inspired by two main thinking modes in philosophy both based on dialectic concept and thesis antithesis paradigm. they impose two different kinds of dialectics. idealistic and materialistic antitheses are formulated as optimization models. based on the models the population is coordinated for dialectical interactions. at the population based context the formulated optimization models are reduced to a simple detection problem for each thinker particle . according to the assigned thinking mode to each thinker and her his measurements of corresponding dialectic with other candidate particles they deterministically decide to interact with a thinker in maximum dialectic with their theses. the position of a thinker at maximum dialectic is known as an available antithesis among the existing solutions. the dialectical interactions at each ideological community are distinguished by meaningful distributions of step sizes for each thinking mode. in fact the thinking modes are regarded as exploration and exploitation elements of the proposed algorithm. the result is a delicate balance without any requirement for adjustment of step size coefficients. main parameter of the proposed algorithm is the number of particles appointed to each thinking modes or equivalently for each kind of motions. an additional integer parameter is defined to boost the stability of the final algorithm in some particular problems. the proposed algorithm is evaluated by a testbed of 12 single objective continuous benchmark functions. moreover its performance and speed were highlighted in sparse reconstruction and antenna selection problems at the context of compressed sensing and massive mimo respectively. the results indicate fast and efficient performance in comparison with well known evolutionary algorithms and dedicated state of the art algorithms.
projectionnet learning efficient on device deep networks using neural projections
deep neural networks have become ubiquitous for applications related to visual recognition and language understanding tasks. however it is often prohibitive to use typical neural networks on devices like mobile phones or smart watches since the model sizes are huge and cannot fit in the limited memory available on such devices. while these devices could make use of machine learning models running on high performance data centers with cpus or gpus this is not feasible for many applications because data can be privacy sensitive and inference needs to be performed directly on device. we introduce a new architecture for training compact neural networks using a joint optimization framework. at its core lies a novel objective that jointly trains using two different types of networks a full trainer neural network using existing architectures like feed forward nns or lstm rnns combined with a simpler projection network that leverages random projections to transform inputs or intermediate representations into bits. the simpler network encodes lightweight and efficient to compute operations in bit space with a low memory footprint. the two networks are trained jointly using backpropagation where the projection network learns from the full network similar to apprenticeship learning. once trained the smaller network can be used directly for inference at low memory and computation cost. we demonstrate the effectiveness of the new approach at significantly shrinking the memory requirements of different types of neural networks while preserving good accuracy on visual recognition and text classification tasks. we also study the question how many neural bits are required to solve a given task using the new framework and show empirical results contrasting model predictive capacity in bits versus accuracy on several datasets.
a flow model of neural networks
based on a natural connection between resnet and transport equation or its characteristic equation we propose a continuous flow model for both resnet and plain net. through this continuous model a resnet can be explicitly constructed as a refinement of a plain net. the flow model provides an alternative perspective to understand phenomena in deep neural networks such as why it is necessary and sufficient to use 2 layer blocks in resnets why deeper is better and why resnets are even deeper and so on. it also opens a gate to bring in more tools from the huge area of differential equations.
multimodal content analysis for effective advertisements on youtube
the rapid advances in e commerce and web 2.0 technologies have greatly increased the impact of commercial advertisements on the general public. as a key enabling technology a multitude of recommender systems exists which analyzes user features and browsing patterns to recommend appealing advertisements to users. in this work we seek to study the characteristics or attributes that characterize an effective advertisement and recommend a useful set of features to aid the designing and production processes of commercial advertisements. we analyze the temporal patterns from multimedia content of advertisement videos including auditory visual and textual components and study their individual roles and synergies in the success of an advertisement. the objective of this work is then to measure the effectiveness of an advertisement and to recommend a useful set of features to advertisement designers to make it more successful and approachable to users. our proposed framework employs the signal processing technique of cross modality feature learning where data streams from different components are employed to train separate neural network models and are then fused together to learn a shared representation. subsequently a neural network model trained on this joint feature embedding representation is utilized as a classifier to predict advertisement effectiveness. we validate our approach using subjective ratings from a dedicated user study the sentiment strength of online viewer comments and a viewer opinion metric of the ratio of the likes and views received by each advertisement from an online platform.
overcoming exploration in reinforcement learning with demonstrations
exploration in environments with sparse rewards has been a persistent problem in reinforcement learning rl . many tasks are natural to specify with a sparse reward and manually shaping a reward function can result in suboptimal performance. however finding a non zero reward is exponentially more difficult with increasing task horizon or action dimensionality. this puts many real world tasks out of practical reach of rl methods. in this work we use demonstrations to overcome the exploration problem and successfully learn to perform long horizon multi step robotics tasks with continuous control such as stacking blocks with a robot arm. our method which builds on top of deep deterministic policy gradients and hindsight experience replay provides an order of magnitude of speedup over rl on simulated robotics tasks. it is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. furthermore our method is able to solve tasks not solvable by either rl or behavior cloning alone and often ends up outperforming the demonstrator policy.
lattice recurrent unit improving convergence and statistical efficiency for sequence modeling
recurrent neural networks have shown remarkable success in modeling sequences. however low resource situations still adversely affect the generalizability of these models. we introduce a new family of models called lattice recurrent units lru to address the challenge of learning deep multi layer recurrent models with limited resources. lru models achieve this goal by creating distinct but coupled flow of information inside the units a first flow along time dimension and a second flow along depth dimension. it also offers a symmetry in how information can flow horizontally and vertically. we analyze the effects of decoupling three different components of our lru model reset gate update gate and projected state. we evaluate this family on new lru models on computational convergence rates and statistical efficiency. our experiments are performed on four publicly available datasets comparing with grid lstm and recurrent highway networks. our results show that lru has better empirical computational convergence rates and statistical efficiency values along with learning more accurate language models.
scalable recollections for continual lifelong learning
given the recent success of deep learning applied to a variety of single tasks it is natural to consider more human realistic settings. perhaps the most difficult of these settings is that of continual lifelong learning where the model must learn online over a continuous stream of non stationary data. a continual lifelong learning system must have three primary capabilities to succeed it must learn and adapt over time it must not forget what it has learned and it must be efficient in both training time and memory. recent techniques have focused their efforts largely on the first two capabilities while the third capability remains largely unexplored. in this paper we consider the problem of efficient and effective storage of experiences over very large time frames. in particular we consider the case where typical experiences are n bits and memories are limited to k bits for k n. we present a novel scalable architecture and training algorithm in this challenging domain and provide an extensive evaluation of its performance. our results show that we can achieve considerable gains on top of state of the art methods such as gem.
hidden tree markov networks deep and wide learning for structured data
the paper introduces the hidden tree markov network htn a neuro probabilistic hybrid fusing the representation power of generative models for trees with the incremental and discriminative learning capabilities of neural networks. we put forward a modular architecture in which multiple generative models of limited complexity are trained to learn structural feature detectors whose outputs are then combined and integrated by neural layers at a later stage. in this respect the model is both deep thanks to the unfolding of the generative models on the input structures as well as wide given the potentially large number of generative modules that can be trained in parallel. experimental results show that the proposed approach can outperform state of the art syntactic kernels as well as generative kernels built on the same probabilistic model as the htn.
hierarchical actor critic
the ability to learn at different resolutions in time may help overcome one of the main challenges in deep reinforcement learning sample efficiency. hierarchical agents that operate at different levels of temporal abstraction can learn tasks more quickly because they can divide the work of learning behaviors among multiple policies and can also explore the environment at a higher level. in this paper we present a novel approach to hierarchical reinforcement learning called hierarchical actor critic hac that enables agents to learn to break down problems involving continuous action spaces into simpler subproblems belonging to different time scales. hac has two key advantages over most existing hierarchical learning methods i the potential for faster learning as agents learn short policies at each level of the hierarchy and ii an end to end approach. we demonstrate that hac significantly accelerates learning in a series of tasks that require behavior over a relatively long time horizon and involve sparse rewards.
proximodistal exploration in motor learning as an emergent property of optimization
to harness the complexity of their high dimensional bodies during sensorimotor development infants are guided by patterns of freezing and freeing of degrees of freedom. for instance when learning to reach infants free the degrees of freedom in their arm proximodistally i.e. from joints that are closer to the body to those that are more distant. here we formulate and study computationally the hypothesis that such patterns can emerge spontaneously as the result of a family of stochastic optimization processes evolution strategies with covariance matrix adaptation without an innate encoding of a maturational schedule. in particular we present simulated experiments with an arm where a computational learner progressively acquires reaching skills through adaptive exploration and we show that a proximodistal organization appears spontaneously which we denote pdff proximodistal freezing and freeing of degrees of freedom . we also compare this emergent organization between different arm morphologies from human like to quite unnatural ones to study the effect of different kinematic structures on the emergence of pdff. keywords human motor learning proximo distal exploration stochastic optimization modelling evolution strategies cross entropy methods policy search morphology. 
null dynamical state models of human cognitive dysfunction
the hard problem in artificial intelligence asks how the shuffling of syntactical symbols in a program can lead to systems which experience semantics and qualia. we address this question in three stages. first we introduce a new class of human semantic symbols which appears when unexpected and drastic environmental change causes humans to become surprised confused uncertain and in extreme cases unresponsive passive and dysfunctional. for this class of symbols pre learned programs become inoperative so these syntactical programs cannot be the source of experienced qualia. second we model the dysfunctional human response to a radically changed environment as being the natural response of any learning machine facing novel inputs from well outside its previous training set. in this situation learning machines are unable to extract information from their input and will typically enter a dynamical state characterized by null outputs and a lack of response. this state immediately predicts and explains the characteristics of the semantic experiences of humans in similar circumstances. in the third stage we consider learning machines trained to implement multiple functions in simple sequential programs using environmental data to specify subroutine names control flow instructions memory calls and so on. drastic change in any of these environmental inputs can again lead to inoperative programs. by examining changes specific to people or locations we can model human cognitive symbols featuring these dependencies such as attachment and grief. our approach links known dynamical machines states with human qualia and thus offers new insight into the hard problem of artificial intelligence.
accelerating deep learning with memcomputing
restricted boltzmann machines rbms and their extensions called deep belief networks are powerful neural networks that have found applications in the fields of machine learning and big data. the standard way to training these models resorts to an iterative unsupervised procedure based on gibbs sampling called contrastive divergence cd and additional supervised tuning via back propagation. however this procedure has been shown not to follow any gradient and can lead to suboptimal solutions. in this paper we show an efficient alternative to cd by means of simulations of digital memcomputing machines dmms . we test our approach on pattern recognition using a modified version of the mnist data set. dmms sample effectively the vast phase space given by the model distribution of the rbm and provide a very good approximation close to the optimum. this efficient search significantly reduces the number of pretraining iterations necessary to achieve a given level of accuracy as well as a total performance gain over cd. in fact the acceleration of pretraining achieved by simulating dmms is comparable to in number of iterations the recently reported hardware application of the quantum annealing method on the same network and data set. notably however dmms perform far better than the reported quantum annealing results in terms of quality of the training. we also compare our method to advances in supervised training like batch normalization and rectifiers that work to reduce the advantage of pretraining. we find that the memcomputing method still maintains a quality advantage 1 in accuracy and a 20 reduction in error rate over these approaches. furthermore our method is agnostic about the connectivity of the network. therefore it can be extended to train full boltzmann machines and even deep networks at once.
mvn2vec preservation and collaboration in multi view network embedding
multi view networks are ubiquitous in real world applications. in order to extract knowledge or business value it is of interest to transform such networks into representations that are easily machine actionable. meanwhile network embedding has emerged as an effective approach to generate distributed network representations. therefore we are motivated to study the problem of multi view network embedding with a focus on the characteristics that are specific and important in embedding this type of networks. in our practice of embedding real world multi view networks we identify two such characteristics which we refer to as preservation and collaboration. we then explore the feasibility of achieving better embedding quality by simultaneously modeling preservation and collaboration and propose the mvn2vec algorithms. with experiments on a series of synthetic datasets an internal snapchat dataset and two public datasets we further confirm the presence and importance of preservation and collaboration. these experiments also demonstrate that better embedding can be obtained by simultaneously modeling the two characteristics while not over complicating the model or requiring additional supervision.
granger causal attentive mixtures of experts
several methods have recently been proposed to detect salient input features for outputs of neural networks. those methods offer a qualitative glimpse at feature importance but they fall short of providing quantifiable attributions that can be compared across decisions and measures of the expected quality of their explanations. to address these shortcomings we present an attentive mixture of experts ame that couples attentive gating with a granger causal objective to jointly produce accurate predictions as well as measures of feature importance. we demonstrate the utility of ames by determining factors driving demand for medical prescriptions comparing predictive features for parkinson s disease and pinpointing discriminatory genes across cancer types.
memorize or generalize searching for a compositional rnn in a haystack
neural networks are very powerful learning systems but they do not readily generalize from one task to the other. this is partly due to the fact that they do not learn in a compositional way that is by discovering skills that are shared by different tasks and recombining them to solve new problems. in this paper we explore the compositional generalization capabilities of recurrent neural networks rnns . we first propose the lookup table composition domain as a simple setup to test compositional behaviour and show that it is theoretically possible for a standard rnn to learn to behave compositionally in this domain when trained with standard gradient descent and provided with additional supervision. we then remove this additional supervision and perform a search over a large number of model initializations to investigate the proportion of rnns that can still converge to a compositional solution. we discover that a small but non negligible proportion of rnns do reach partial compositional solutions even without special architectural constraints. this suggests that a combination of gradient descent and evolutionary strategies directly favouring the minority models that developed more compositional approaches might suffice to lead standard rnns towards compositional solutions.
continual reinforcement learning with complex synapses
unlike humans who are capable of continual learning over their lifetimes artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting whereby new learning can lead to abrupt erasure of previously acquired knowledge. whereas in a neural network the parameters are typically modelled as scalar values an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. in this paper we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity benna fusi 2016 catastrophic forgetting can be mitigated at multiple timescales. in particular we find that as well as enabling continual learning across sequential training of two simple tasks it can also be used to overcome within task forgetting by reducing the need for an experience replay database.
meta reinforcement learning of structured exploration strategies
exploration is a fundamental challenge in reinforcement learning rl . many of the current exploration methods for deep rl use task agnostic objectives such as information gain or bonuses based on state visitation. however many practical applications of rl involve learning more than a single task and prior tasks can be used to inform how exploration should be performed in new tasks. in this work we explore how prior tasks can inform an agent about how to explore effectively in new situations. we introduce a novel gradient based fast adaptation algorithm model agnostic exploration with structured noise maesn to learn exploration strategies from prior experience. the prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy producing exploration strategies that are informed by prior knowledge and are more effective than random action space noise. we show that maesn is more effective at learning exploration strategies when compared to prior meta rl methods rl without learned exploration strategies and task agnostic exploration methods. we evaluate our method on a variety of simulated tasks locomotion with a wheeled robot locomotion with a quadrupedal walker and object manipulation.
approximation algorithms for cascading prediction models
we present an approximation algorithm that takes a pool of pre trained models as input and produces from it a cascaded model with similar accuracy but lower average case cost. applied to state of the art imagenet classification models this yields up to a 2x reduction in floating point multiplications and up to a 6x reduction in average case memory i o. the auto generated cascades exhibit intuitive properties such as using lower resolution input for easier images and requiring higher prediction confidence when using a computationally cheaper model.
coloring black boxes visualization of neural network decisions
neural networks are commonly regarded as black boxes performing incomprehensible functions. for classification problems networks provide maps from high dimensional feature space to k dimensional image space. images of training vector are projected on polygon vertices providing visualization of network function. such visualization may show the dynamics of learning allow for comparison of different networks display training vectors around which potential problems may arise show differences due to regularization and optimization procedures investigate stability of network classification under perturbation of original vectors and place new data sample in relation to training data allowing for estimation of confidence in classification of a given sample. an illustrative example for the three class wine data and five class satimage data is described. the visualization method proposed here is applicable to any black box system that provides continuous outputs.
relational neural expectation maximization unsupervised discovery of objects and their interactions
common sense physical reasoning is an essential ingredient for any intelligent agent operating in the real world. for example it can be used to simulate the environment or to infer the state of parts of the world that are currently unobserved. in order to match real world conditions this causal knowledge must be learned without access to supervised data. to address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely emph unsupervised fashion. it incorporates prior knowledge about the compositional nature of human perception to factor interactions between object pairs and learn efficiently. on videos of bouncing balls we show the superior modelling capabilities of our method compared to other unsupervised neural approaches that do not incorporate such prior knowledge. we demonstrate its ability to handle occlusion and show that it can extrapolate learned knowledge to scenes with different numbers of objects.
a bayesian model for activities recommendation and event structure optimization using visitors tracking
in events that are composed by many activities there is a problem that involves retrieve and management the information of visitors that are visiting the activities. this management is crucial to find some activities that are drawing attention of visitors identify an ideal positioning for activities which path is more frequented by visitors. in this work these features are studied using complex network theory. for the beginning an artificial database was generated to study the mentioned features. secondly this work shows a method to optimize the event structure that is better than a random method and a recommendation system that achieves 95 of accuracy.
the lottery ticket hypothesis training pruned neural networks
recent work on neural network pruning indicates that at training time neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. this paper articulates a new hypothesis to explain this phenomenon. this conjecture which we term the lottery ticket hypothesis proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. larger networks have more of these lottery tickets meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization. this paper conducts a series of experiments with xor and mnist that support the lottery ticket hypothesis. in particular we identify these fortuitously initialized subcomponents by pruning low magnitude weights from trained networks. we then demonstrate that these subcomponents can be successfully retrained in isolation so long as the subnetworks are given the same initializations as they had at the beginning of the training process. initialized as such these small networks reliably converge successfully often faster than the original network at the same level of accuracy. however when these subcomponents are randomly reinitialized or rearranged they perform worse than the original network. in other words large networks that train successfully contain small subnetworks with initializations conducive to optimization. the lottery ticket hypothesis and its connection to pruning are a step toward developing architectures initializations and training strategies that make it possible to solve the same problems with much smaller networks.
learning recurrent dynamics in spiking networks
spiking activity of neurons engaged in learning and performing a task show complex spatiotemporal dynamics. while the output of recurrent network models can learn to perform various tasks the possible range of recurrent dynamics that emerge after learning remains unknown. here we show that modifying the recurrent connectivity with a recursive least squares algorithm provides sufficient flexibility for synaptic and spiking rate dynamics of spiking networks to produce a wide range of spatiotemporal activity. we apply the training method to learn arbitrary firing patterns stabilize irregular spiking activity of a balanced network and reproduce the heterogeneous spiking rate patterns of cortical neurons engaged in motor planning and movement. we identify sufficient conditions for successful learning characterize two types of learning errors and assess the network capacity. our findings show that synaptically coupled recurrent spiking networks possess a vast computational capability that can support the diverse activity patterns in the brain.
principal graphs and manifolds
in many physical statistical biological and other investigations it is desirable to approximate a system of points by objects of lower dimension and or complexity. for this purpose karl pearson invented principal component analysis in 1901 and found lines and planes of closest fit to system of points . the famous k means algorithm solves the approximation problem too but by finite sets instead of lines and planes. this chapter gives a brief practical introduction into the methods of construction of general principal objects i.e. objects embedded in the middle of the multidimensional data set. as a basis the unifying framework of mean squared distance approximation of finite datasets is selected. principal graphs and manifolds are constructed as generalisations of principal components and k means principal points. for this purpose the family of expectation maximisation algorithms with nearest generalisations is presented. construction of principal graphs with controlled complexity is based on the graph grammar approach.
sparse penalty in deep belief networks using the mixed norm constraint
deep belief networks dbn have been successfully applied on popular machine learning tasks. specifically when applied on hand written digit recognition dbns have achieved approximate accuracy rates of 98.8 . in an effort to optimize the data representation achieved by the dbn and maximize their descriptive power recent advances have focused on inducing sparse constraints at each layer of the dbn. in this paper we present a theoretical approach for sparse constraints in the dbn using the mixed norm for both non overlapping and overlapping groups. we explore how these constraints affect the classification accuracy for digit recognition in three different datasets mnist usps rimes and provide initial estimations of their usefulness by altering different parameters such as the group size and overlap percentage.
understanding dropout training multi layer perceptrons with auxiliary independent stochastic neurons
in this paper a simple general method of adding auxiliary stochastic neurons to a multi layer perceptron is proposed. it is shown that the proposed method is a generalization of recently successful methods of dropout hinton et al. 2012 explicit noise injection vincent et al. 2010 bishop 1995 and semantic hashing salakhutdinov hinton 2009 . under the proposed framework an extension of dropout which allows using separate dropping probabilities for different hidden neurons or layers is found to be available. the use of different dropping probabilities for hidden layers separately is empirically investigated.
locally imposing function for generalized constraint neural networks a study on equality constraints
this work is a further study on the generalized constraint neural network gcnn model 1 2 . two challenges are encountered in the study that is to embed any type of prior information and to select its imposing schemes. the work focuses on the second challenge and studies a new constraint imposing scheme for equality constraints. a new method called locally imposing function lif is proposed to provide a local correction to the gcnn prediction function which therefore falls within locally imposing scheme lis . in comparison the conventional lagrange multiplier method is considered as globally imposing scheme gis because its added constraint term exhibits a global impact to its objective function. two advantages are gained from lis over gis. first lis enables constraints to fire locally and explicitly in the domain only where they need on the prediction function. second constraints can be implemented within a network setting directly. we attempt to interpret several constraint methods graphically from a viewpoint of the locality principle. numerical examples confirm the advantages of the proposed method. in solving boundary value problems with dirichlet and neumann constraints the gcnn model with lif is possible to achieve an exact satisfaction of the constraints.
evolution of covariance functions for gaussian process regression using genetic programming
in this contribution we describe an approach to evolve composite covariance functions for gaussian processes using genetic programming. a critical aspect of gaussian processes and similar kernel based models such as svm is that the covariance function should be adapted to the modeled data. frequently the squared exponential covariance function is used as a default. however this can lead to a misspecified model which does not fit the data well. in the proposed approach we use a grammar for the composition of covariance functions and genetic programming to search over the space of sentences that can be derived from the grammar. we tested the proposed approach on synthetic data from two dimensional test functions and on the mauna loa co2 time series. the results show that our approach is feasible finding covariance functions that perform much better than a default covariance function. for the co2 data set a composite covariance function is found that matches the performance of a hand tuned covariance function.
gaussian binary restricted boltzmann machines on modeling natural image statistics
we present a theoretical analysis of gaussian binary restricted boltzmann machines grbms from the perspective of density models. the key aspect of this analysis is to show that grbms can be formulated as a constrained mixture of gaussians which gives a much better insight into the model s capabilities and limitations. we show that grbms are capable of learning meaningful features both in a two dimensional blind source separation task and in modeling natural images. further we show that reported difficulties in training grbms are due to the failure of the training algorithm rather than the model itself. based on our analysis we are able to propose several training recipes which allowed successful and fast training in our experiments. finally we discuss the relationship of grbms to several modifications that have been proposed to improve the model.
training restricted boltzmann machine by perturbation
a new approach to maximum likelihood learning of discrete graphical models and rbm in particular is introduced. our method perturb and descend pd is inspired by two ideas i perturb and map method for sampling ii learning by contrastive divergence minimization. in contrast to perturb and map pd leverages training data to learn the models that do not allow efficient map estimation. during the learning to produce a sample from the current model we start from a training data and descend in the energy landscape of the perturbed model for a fixed number of steps or until a local optima is reached. for rbm this involves linear calculations and thresholding which can be very fast. furthermore we show that the amount of perturbation is closely related to the temperature parameter and it can regularize the model by producing robust features resulting in sparse hidden layer activation.
multilayer bootstrap networks
multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from bottom up for unsupervised nonlinear dimensionality reduction. each layer of the network is a nonparametric density estimator. it consists of a group of k centroids clusterings. each clustering randomly selects data points with randomly selected features as its centroids and learns a one hot encoder by one nearest neighbor optimization. geometrically the nonparametric density estimator at each layer projects the input data space to a uniformly distributed discrete feature space where the similarity of two data points in the discrete feature space is measured by the number of the nearest centroids they share in common. the multilayer network gradually reduces the nonlinear variations of data from bottom up by building a vast number of hierarchical trees implicitly on the original data space. theoretically the estimation error caused by the nonparametric density estimator is proportional to the correlation between the clusterings both of which are reduced by the randomization steps.
invariant backpropagation how to train a transformation invariant neural network
in many classification problems a classifier should be robust to small variations in the input vector. this is a desired property not only for particular transformations such as translation and rotation in image classification problems but also for all others for which the change is small enough to retain the object perceptually indistinguishable. we propose two extensions of the backpropagation algorithm that train a neural network to be robust to variations in the feature vector. while the first of them enforces robustness of the loss function to all variations the second method trains the predictions to be robust to a particular variation which changes the loss function the most. the second methods demonstrates better results but is slightly slower. we analytically compare the proposed algorithm with two the most similar approaches tangent bp and adversarial training and propose their fast versions. in the experimental part we perform comparison of all algorithms in terms of classification accuracy and robustness to noise on mnist and cifar 10 datasets. additionally we analyze how the performance of the proposed algorithm depends on the dataset size and data augmentation.
shared latent subspace modelling within gaussian binary restricted boltzmann machines for nist i vector challenge 2014
this paper presents a novel approach to speaker subspace modelling based on gaussian binary restricted boltzmann machines grbm . the proposed model is based on the idea of shared factors as in the probabilistic linear discriminant analysis plda . grbm hidden layer is divided into speaker and channel factors herein the speaker factor is shared over all vectors of the speaker. then maximum likelihood parameter estimation mle for proposed model is introduced. various new scoring techniques for speaker verification using grbm are proposed. the results for nist i vector challenge 2014 dataset are presented.
a neural transfer function for a smooth and differentiable transition between additive and multiplicative interactions
existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. this leads either to an inefficient distribution of computational resources or an extensive increase in the computational complexity of the training procedure. we present a novel parameterizable transfer function based on the mathematical concept of non integer functional iteration that allows the operation each neuron performs to be smoothly and most importantly differentiablely adjusted between addition and multiplication. this allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.
a probabilistic framework for deep learning
we develop a probabilistic framework for deep learning based on the deep rendering mixture model drmm a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables. we demonstrate that max sum inference in the drmm yields an algorithm that exactly reproduces the operations in deep convolutional neural networks dcns providing a first principles derivation. our framework provides new insights into the successes and shortcomings of dcns as well as a principled route to their improvement. drmm training via the expectation maximization em algorithm is a powerful alternative to dcn back propagation and initial training results are promising. classification based on the drmm and other variants outperforms dcns in supervised digit classification training 2 3x faster while achieving similar accuracy. moreover the drmm is applicable to semi supervised and unsupervised learning tasks achieving results that are state of the art in several categories on the mnist benchmark and comparable to state of the art on the cifar10 benchmark.
neurogenesis deep learning
neural machine learning methods such as deep neural networks dnn have achieved remarkable success in a number of complex data processing tasks. these methods have arguably had their strongest impact on tasks such as image and audio processing data processing domains in which humans have long held clear advantages over conventional algorithms. in contrast to biological neural systems which are capable of learning continuously deep artificial networks have a limited ability for incorporating new information in an already trained network. as a result methods for continuous learning are potentially highly impactful in enabling the application of deep networks to dynamic data sets. here inspired by the process of adult neurogenesis in the hippocampus we explore the potential for adding new neurons to deep layers of artificial neural networks in order to facilitate their acquisition of novel information while preserving previously trained data representations. our results on the mnist handwritten digit dataset and the nist sd 19 dataset which includes lower and upper case letters and digits demonstrate that neurogenesis is well suited for addressing the stability plasticity dilemma that has long challenged adaptive machine learning algorithms.
deep learning for neuroimaging a validation study
deep learning methods have recently made notable advances in the tasks of classification and representation learning. these tasks are important for brain imaging and neuroscience discovery making the methods attractive for porting to a neuroimager s toolbox. success of these methods is in part explained by the flexibility of deep learning models. however this flexibility makes the process of porting to new areas a difficult parameter optimization problem. in this work we demonstrate our results and feasible parameter ranges in application of deep learning methods to structural and functional brain imaging data. we also describe a novel constraint based approach to visualizing high dimensional data. we use it to analyze the effect of parameter choices on data transformations. our results show that deep learning methods are able to learn physiologically important representations and detect latent relations in neuroimaging data.
improving deep neural networks with probabilistic maxout units
we present a probabilistic variant of the recently introduced maxout unit. the success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout when compared to rectified linear units. it however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. starting from this observation we ask the question can the desirable properties of maxout units be preserved while improving their invariance properties we argue that our probabilistic maxout probout units successfully achieve this balance. we quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks cifar 10 cifar 100 and svhn .
how many dissimilarity kernel self organizing map variants do we need 
in numerous applicative contexts data are too rich and too complex to be represented by numerical vectors. a general approach to extend machine learning and data mining techniques to such data is to really on a dissimilarity or on a kernel that measures how different or similar two objects are. this approach has been used to define several variants of the self organizing map som . this paper reviews those variants in using a common set of notations in order to outline differences and similarities between them. it discusses the advantages and drawbacks of the variants as well as the actual relevance of the dissimilarity kernel som for practical applications.
deep unfolding model based inspiration of novel deep architectures
model based methods and deep neural networks have both been tremendously successful paradigms in machine learning. in model based methods problem domain knowledge can be built into the constraints of the model typically at the expense of difficulties during inference. in contrast deterministic deep neural networks are constructed in such a way that inference is straightforward but their architectures are generic and it is unclear how to incorporate knowledge. this work aims to obtain the advantages of both approaches. to do so we start with a model based approach and an associated inference algorithm and emph unfold the inference iterations as layers in a deep network. rather than optimizing the original model we emph untie the model parameters across layers in order to create a more powerful network. the resulting architecture can be trained discriminatively to perform accurate inference within a fixed network size. we show how this framework allows us to interpret conventional networks as mean field inference in markov random fields and to obtain new architectures by instead using belief propagation as the inference algorithm. we then show its application to a non negative matrix factorization model that incorporates the problem domain knowledge that sound sources are additive. deep unfolding of this model yields a new kind of non negative deep neural network that can be trained using a multiplicative backpropagation style update algorithm. we present speech enhancement experiments showing that our approach is competitive with conventional neural networks despite using far fewer parameters.
learning deep dynamical models from image pixels
modeling dynamical systems is important in many disciplines e.g. control robotics or neurotechnology. commonly the state of these systems is not directly observed but only available through noisy and potentially high dimensional observations. in these cases system identification i.e. finding the measurement mapping and the transition mapping system dynamics in latent space can be challenging. for linear system dynamics and measurement mappings efficient solutions for system identification are available. however in practical applications the linearity assumptions does not hold requiring non linear system identification techniques. if additionally the observations are high dimensional e.g. images non linear system identification is inherently hard. to address the problem of non linear system identification from high dimensional observations we combine recent advances in deep learning and system identification. in particular we jointly learn a low dimensional embedding of the observation by means of deep auto encoders and a predictive transition model in this low dimensional space. we demonstrate that our model enables learning good predictive models of dynamical systems from pixel information only.
from neural pca to deep unsupervised learning
a network supporting deep unsupervised learning is presented. the network is an autoencoder with lateral shortcut connections from the encoder to decoder at each level of the hierarchy. the lateral shortcut connections allow the higher levels of the hierarchy to focus on abstract invariant features. while standard autoencoders are analogous to latent variable models with a single layer of stochastic variables the proposed network is analogous to hierarchical latent variables models. learning combines denoising autoencoder and denoising sources separation frameworks. each layer of the network contributes to the cost function a term which measures the distance of the representations produced by the encoder and the decoder. since training signals originate from all levels of the network all layers can learn efficiently even in deep networks. the speedup offered by cost terms from higher levels of the hierarchy and the ability to learn invariant features are demonstrated in experiments.
qualitatively characterizing neural network optimization problems
training neural networks involves solving large scale non convex optimization problems. this task has long been believed to be extremely difficult with fear of local minima and other obstacles motivating a variety of schemes to improve optimization such as unsupervised pretraining. however modern neural networks are able to achieve negligible training error on complex tasks using only direct training with stochastic gradient descent. we introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. we find that in fact on a straight path from initialization to solution a variety of state of the art neural networks never encounter any significant obstacles.
why does deep learning work a perspective from group theory
why does deep learning work what representations does it capture how do higher order representations emerge we study these questions from the perspective of group theory thereby opening a new approach towards a theory of deep learning. one factor behind the recent resurgence of the subject is a key algorithmic step called pre training first search for a good generative model for the input samples and repeat the process one layer at a time. we show deeper implications of this simple principle by establishing a connection with the interplay of orbits and stabilizers of group actions. although the neural networks themselves may not form groups we show the existence of em shadow groups whose elements serve as close approximations. over the shadow groups the pre training step originally introduced as a mechanism to better initialize a network becomes equivalent to a search for features with minimal orbits. intuitively these features are in a way the em simplest . which explains why a deep learning network learns simple features first. next we show how the same principle when repeated in the deeper layers can capture higher order representations and why representation complexity increases as the layers get deeper.
adasecant robust adaptive secant method for stochastic gradient
stochastic gradient algorithms have been the main focus of large scale learning problems and they led to important successes in machine learning. the convergence of sgd depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. in this paper we propose a new adaptive learning rate algorithm which utilizes curvature information for automatically tuning the learning rates. the information about the element wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. we further propose a new variance reduction technique to speed up the convergence. in our preliminary experiments with deep neural networks we obtained better performance compared to the popular stochastic gradient algorithms.
a unified perspective on multi domain and multi task learning
in this paper we provide a new neural network based perspective on multi task learning mtl and multi domain learning mdl . by introducing the concept of a semantic descriptor this framework unifies mdl and mtl as well as encompassing various classic and recent mtl mdl algorithms by interpreting them as different ways of constructing semantic descriptors. our interpretation provides an alternative pipeline for zero shot learning zsl where a model for a novel class can be constructed without training data. moreover it leads to a new and practically relevant problem setting of zero shot domain adaptation zsda which is the analogous to zsl but for novel domains a model for an unseen domain can be generated by its semantic descriptor. experiments across this range of problems demonstrate that our framework outperforms a variety of alternatives.
a neural network anomaly detector using the random cluster model
the random cluster model is used to define an upper bound on a distance measure as a function of the number of data points to be classified and the expected value of the number of classes to form in a hybrid k means and regression classification methodology with the intent of detecting anomalies. conditions are given for the identification of classes which contain anomalies and individual anomalies within identified classes. a neural network model describes the decision region separating surface for offline storage and recall in any new anomaly detection.
a group theoretic perspective on unsupervised deep learning
why does deep learning work what representations does it capture how do higher order representations emerge we study these questions from the perspective of group theory thereby opening a new approach towards a theory of deep learning. one factor behind the recent resurgence of the subject is a key algorithmic step called em pretraining first search for a good generative model for the input samples and repeat the process one layer at a time. we show deeper implications of this simple principle by establishing a connection with the interplay of orbits and stabilizers of group actions. although the neural networks themselves may not form groups we show the existence of em shadow groups whose elements serve as close approximations. over the shadow groups the pre training step originally introduced as a mechanism to better initialize a network becomes equivalent to a search for features with minimal orbits. intuitively these features are in a way the em simplest . which explains why a deep learning network learns simple features first. next we show how the same principle when repeated in the deeper layers can capture higher order representations and why representation complexity increases as the layers get deeper.
a generative model for deep convolutional learning
a generative model is developed for deep multi layered convolutional dictionary learning. a novel probabilistic pooling operation is integrated into the deep model yielding efficient bottom up pretraining and top down refinement probabilistic learning. experimental results demonstrate powerful capabilities of the model to learn multi layer features from images and excellent classification results are obtained on the mnist and caltech 101 datasets.
knowledge transfer pre training
pre training is crucial for learning deep neural networks. most of existing pre training methods train simple models e.g. restricted boltzmann machines and then stack them layer by layer to form the deep structure. this layer wise pre training has found strong theoretical foundation and broad empirical support. however it is not easy to employ such method to pre train models without a clear multi layer structure e.g. recurrent neural networks rnns . this paper presents a new pre training approach based on knowledge transfer learning. in contrast to the layer wise approach which trains model components incrementally the new approach trains the entire model as a whole but with an easier objective function. this is achieved by utilizing soft targets produced by a prior trained model teacher model . compared to the conventional layer wise methods this new method does not care about the model structure so can be used to pre train very complex models. experiments on a speech recognition task demonstrated that with this approach complex rnns can be well trained with a weaker deep neural network dnn model. furthermore the new method can be combined with conventional layer wise pre training to deliver additional gains.
stacked what where auto encoders
we present a novel architecture the stacked what where auto encoders swwae which integrates discriminative and generative pathways and provides a unified approach to supervised semi supervised and unsupervised learning without relying on sampling during training. an instantiation of swwae uses a convolutional net convnet lecun et al. 1998 to encode the input and employs a deconvolutional net deconvnet zeiler et al. 2010 to produce the reconstruction. the objective function includes reconstruction terms that induce the hidden states in the deconvnet to be similar to those of the convnet. each pooling layer produces two sets of variables the what which are fed to the next layer and its complementary variable where that are fed to the corresponding layer in the generative decoder.
training recurrent networks online without backtracking
we introduce the nobacktrack algorithm to train the parameters of dynamical systems such as recurrent neural networks. this algorithm works in an online memoryless setting thus requiring no backpropagation through time and is scalable avoiding the large computational and memory cost of maintaining the full gradient of the current state with respect to the parameters. the algorithm essentially maintains at each time a single search direction in parameter space. the evolution of this search direction is partly stochastic and is constructed in such a way to provide at every time an unbiased random estimate of the gradient of the loss function with respect to the parameters. because the gradient estimate is unbiased on average over time the parameter is updated as it should. the resulting gradient estimate can then be fed to a lightweight kalman like filter to yield an improved algorithm. for recurrent neural networks the resulting algorithms scale linearly with the number of parameters. small scale experiments confirm the suitability of the approach showing that the stochastic approximation of the gradient introduced in the algorithm is not detrimental to learning. in particular the kalman like version of nobacktrack is superior to backpropagation through time bptt when the time span of dependencies in the data is longer than the truncation span for bptt.
deep clustering discriminative embeddings for segmentation and separation
we address the problem of acoustic source separation in a deep learning framework we call deep clustering. rather than directly estimating signals or masking functions we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. previous deep network approaches provide great advantages in terms of learning power and speed but previously it has been unclear how to use them to separate signals in a class independent way. in contrast spectral clustering approaches are flexible with respect to the classes and number of items to be segmented but it has been unclear how to leverage the learning power and speed of deep networks. to obtain the best of both worlds we use an objective function that to train embeddings that yield a low rank approximation to an ideal pairwise affinity matrix in a class independent way. this avoids the high cost of spectral factorization and instead produces compact clusters that are amenable to simple clustering methods. the segmentations are therefore implicitly encoded in the embeddings and can be decoded by clustering. preliminary experiments show that the proposed method can separate speech when trained on spectrogram features containing mixtures of two speakers and tested on mixtures of a held out set of speakers it can infer masking functions that improve signal quality by around 6db. we show that the model can generalize to three speaker mixtures despite training only on two speaker mixtures. the framework can be used without class labels and therefore has the potential to be trained on a diverse set of sound types and to generalize to novel sources. we hope that future work will lead to segmentation of arbitrary sounds with extensions to microphone array methods as well as image segmentation and other domains.
scalable out of sample extension of graph embeddings using deep neural networks
several popular graph embedding techniques for representation learning and dimensionality reduction rely on performing computationally expensive eigendecompositions to derive a nonlinear transformation of the input data space. the resulting eigenvectors encode the embedding coordinates for the training samples only and so the embedding of novel data samples requires further costly computation. in this paper we present a method for the out of sample extension of graph embeddings using deep neural networks dnn to parametrically approximate these nonlinear maps. compared with traditional nonparametric out of sample extension methods we demonstrate that the dnns can generalize with equal or better fidelity and require orders of magnitude less computation at test time. moreover we find that unsupervised pretraining of the dnns improves optimization for larger network sizes thus removing sensitivity to model selection.
model accuracy and runtime tradeoff in distributed deep learning a systematic study
this paper presents rudra a parameter server based distributed computing framework tuned for training large scale deep neural networks. using variants of the asynchronous stochastic gradient descent algorithm we study the impact of synchronization protocol stale gradient updates minibatch size learning rates and number of learners on runtime performance and model accuracy. we introduce a new learning rate modulation strategy to counter the effect of stale gradients and propose a new synchronization protocol that can effectively bound the staleness in gradients improve runtime performance and achieve good model accuracy. our empirical investigation reveals a principled approach for distributed training of neural networks the mini batch size per learner should be reduced as more learners are added to the system to preserve the model accuracy. we validate this approach using commonly used image classification benchmarks cifar10 and imagenet.
convolutional networks on graphs for learning molecular fingerprints
we introduce a convolutional neural network that operates directly on graphs. these networks allow end to end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. the architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. we show that these data driven features are more interpretable and have better predictive performance on a variety of tasks.
population contrastive divergence does consistency help with rbm training 
estimating the log likelihood gradient with respect to the parameters of a restricted boltzmann machine rbm typically requires sampling using markov chain monte carlo mcmc techniques. to save computation time the markov chains are only run for a small number of steps which leads to a biased estimate. this bias can cause rbm training algorithms such as contrastive divergence cd learning to deteriorate. we adopt the idea behind population monte carlo pmc methods to devise a new rbm training algorithm termed population contrastive divergence pop cd . compared to cd it leads to a consistent estimate and may have a significantly lower bias. its computational overhead is negligible compared to cd. however the variance of the gradient estimate increases. we experimentally show that pop cd can significantly outperform cd. in many cases we observed a smaller bias and achieved higher log likelihood values. however when the rbm distribution has many hidden neurons the consistent estimate of pop cd may still have a considerable bias and the variance of the gradient estimate requires a smaller learning rate. thus despite its superior theoretical properties it is not advisable to use pop cd in its current form on large problems.
atomnet a deep convolutional neural network for bioactivity prediction in structure based drug discovery
deep convolutional neural networks comprise a subclass of deep neural networks dnn with a constrained architecture that leverages the spatial and temporal structure of the domain they model. convolutional networks achieve the best predictive performance in areas such as speech and image recognition by hierarchically composing simple local features into complex models. although dnns have been used in drug discovery for qsar and ligand based bioactivity predictions none of these models have benefited from this powerful convolutional architecture. this paper introduces atomnet the first structure based deep convolutional neural network designed to predict the bioactivity of small molecules for drug discovery applications. we demonstrate how to apply the convolutional concepts of feature locality and hierarchical composition to the modeling of bioactivity and chemical interactions. in further contrast to existing dnn techniques we show that atomnet s application of local convolutional filters to structural target information successfully predicts new active molecules for targets with no previously known modulators. finally we show that atomnet outperforms previous docking approaches on a diverse set of benchmarks by a large margin achieving an auc greater than 0.9 on 57.8 of the targets in the dude benchmark.
distillation as a defense to adversarial perturbations against deep neural networks
deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. however recent studies have shown that deep learning like other machine learning techniques is vulnerable to adversarial samples inputs crafted to force a deep neural network dnn to provide adversary selected outputs. such attacks can seriously undermine the security of the system supported by the dnn sometimes with devastating consequences. for example autonomous vehicles can be crashed illicit or illegal content can bypass content filters or biometric authentication systems can be manipulated to allow improper access. in this work we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on dnns. we analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training dnns. we also empirically study the effectiveness of our defense mechanisms on two dnns placed in adversarial settings. the study shows that defensive distillation can reduce effectiveness of sample creation from 95 to less than 0.5 on a studied dnn. such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10 30. we also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800 on one of the dnns we tested.
the variational gaussian process
variational inference is a powerful tool for approximate inference and it has been recently applied for representation learning with deep generative models. we develop the variational gaussian process vgp a bayesian nonparametric variational family which adapts its shape to match complex posterior distributions. the vgp generates approximate posterior samples by generating latent inputs and warping them through random non linear mappings the distribution over random mappings is learned during inference enabling the transformed outputs to adapt to varying complexity. we prove a universal approximation theorem for the vgp demonstrating its representative power for learning any model. for inference we present a variational objective inspired by auto encoders and perform black box inference over a wide class of models. the vgp achieves new state of the art results for unsupervised learning inferring models such as the deep latent gaussian model and the recently proposed draw.
partial reinitialisation for optimisers
heuristic optimisers which search for an optimal configuration of variables relative to an objective function often get stuck in local optima where the algorithm is unable to find further improvement. the standard approach to circumvent this problem involves periodically restarting the algorithm from random initial configurations when no further improvement can be found. we propose a method of partial reinitialization whereby in an attempt to find a better solution only sub sets of variables are re initialised rather than the whole configuration. much of the information gained from previous runs is hence retained. this leads to significant improvements in the quality of the solution found in a given time for a variety of optimisation problems in machine learning.
efficient representation of low dimensional manifolds using deep networks
we consider the ability of deep neural networks to represent data that lies near a low dimensional manifold in a high dimensional space. we show that deep networks can efficiently extract the intrinsic low dimensional coordinates of such data. we first show that the first two layers of a deep network can exactly embed points lying on a monotonic chain a special type of piecewise linear manifold mapping them to a low dimensional euclidean space. remarkably the network can do this using an almost optimal number of parameters. we also show that this network projects nearby points onto the manifold and then embeds them with little error. we then extend these results to more general manifolds.
enhanced perceptrons using contrastive biclusters
perceptrons are neuronal devices capable of fully discriminating linearly separable classes. although straightforward to implement and train their applicability is usually hindered by non trivial requirements imposed by real world classification problems. therefore several approaches such as kernel perceptrons have been conceived to counteract such difficulties. in this paper we investigate an enhanced perceptron model based on the notion of contrastive biclusters. from this perspective a good discriminative bicluster comprises a subset of data instances belonging to one class that show high coherence across a subset of features and high differentiation from nearest instances of the other class under the same features referred to as its contrastive bicluster . upon each local subspace associated with a pair of contrastive biclusters a perceptron is trained and the model with highest area under the receiver operating characteristic curve auc value is selected as the final classifier. experiments conducted on a range of data sets including those related to a difficult biosignal classification problem show that the proposed variant can be indeed very useful prevailing in most of the cases upon standard and kernel perceptrons in terms of accuracy and auc measures.
alternating optimization method based on nonnegative matrix factorizations for deep neural networks
the backpropagation algorithm for calculating gradients has been widely used in computation of weights for deep neural networks dnns . this method requires derivatives of objective functions and has some difficulties finding appropriate parameters such as learning rate. in this paper we propose a novel approach for computing weight matrices of fully connected dnns by using two types of semi nonnegative matrix factorizations semi nmfs . in this method optimization processes are performed by calculating weight matrices alternately and backpropagation bp is not used. we also present a method to calculate stacked autoencoder using a nmf. the output results of the autoencoder are used as pre training data for dnns. the experimental results show that our method using three types of nmfs attains similar error rates to the conventional dnns with bp.
robust large margin deep neural networks
the generalization error of deep neural networks via their classification margin is studied in this work. our approach is based on the jacobian matrix of a deep neural network and can be applied to networks with arbitrary non linearities and pooling layers and to networks with different architectures such as feed forward networks and residual networks. our analysis leads to the conclusion that a bounded spectral norm of the network s jacobian matrix in the neighbourhood of the training samples is crucial for a deep neural network of arbitrary depth and width to generalize well. this is a significant improvement over the current bounds in the literature which imply that the generalization error grows with either the width or the depth of the network. moreover it shows that the recently proposed batch normalization and weight normalization re parametrizations enjoy good generalization properties and leads to a novel network regularizer based on the network s jacobian matrix. the analysis is supported with experimental results on the mnist cifar 10 lared and imagenet datasets.
no bad local minima data independent training error guarantees for multilayer neural networks
we use smoothed analysis techniques to provide guarantees on the training loss of multilayer neural networks mnns at differentiable local minima. specifically we examine mnns with piecewise linear activation functions quadratic loss and a single output under mild over parametrization. we prove that for a mnn with one hidden layer the training error is zero at every differentiable local minimum for almost every dataset and dropout like noise realization. we then extend these results to the case of more than one hidden layer. our theoretical guarantees assume essentially nothing on the training data and are verified numerically. these results suggest why the highly non convex loss of such mnns can be easily optimized using local updates e.g. stochastic gradient descent as observed empirically.
learning structured sparsity in deep neural networks
high demand for computation resources severely hinders deployment of large scale deep neural networks dnn in resource constrained devices. in this work we propose a structured sparsity learning ssl method to regularize the structures i.e. filters channels filter shapes and layer depth of dnns. ssl can 1 learn a compact structure from a bigger dnn to reduce computation cost 2 obtain a hardware friendly structured sparsity of dnn to efficiently accelerate the dnns evaluation. experimental results show that ssl achieves on average 5.1x and 3.1x speedups of convolutional layer computation of alexnet against cpu and gpu respectively with off the shelf libraries. these speedups are about twice speedups of non structured sparsity 3 regularize the dnn structure to improve classification accuracy. the results show that for cifar 10 regularization on layer depth can reduce 20 layers of a deep residual network resnet to 18 layers while improve the accuracy from 91.25 to 92.60 which is still slightly higher than that of original resnet with 32 layers. for alexnet structure regularization by ssl also reduces the error by around 1 . open source code is in https github.com wenwei202 caffe tree scnn
depth width tradeoffs in approximating natural functions with neural networks
we provide several new depth based separation results for feed forward neural networks proving that various types of simple and natural functions can be better approximated using deeper networks than shallower ones even if the shallower networks are much larger. this includes indicators of balls and ellipses non linear functions which are radial with respect to the l 1 norm and smooth non linear functions. we also show that these gaps can be observed experimentally increasing the depth indeed allows better learning than increasing width when training neural networks to learn an indicator of a unit ball.
tensor switching networks
we present a novel neural network algorithm the tensor switching ts network which generalizes the rectified linear unit relu nonlinearity to tensor valued hidden units. the ts network copies its entire input vector to different locations in an expanded representation with the location determined by its hidden unit activity. in this way even a simple linear readout from the ts representation can implement a highly expressive deep network like function. the ts network hence avoids the vanishing gradient problem by construction at the cost of larger representation size. we develop several methods to train the ts network including equivalent kernels for infinitely wide and deep ts networks a one pass linear learning algorithm and two backpropagation inspired representation learning algorithms. our experimental results demonstrate that the ts network is indeed more expressive and consistently learns faster than standard relu networks.
survey of expressivity in deep neural networks
we survey results on neural network expressivity described in on the expressive power of deep neural networks . the paper motivates and develops three natural measures of expressiveness which all display an exponential dependence on the depth of the network. in fact all of these measures are related to a fourth quantity trajectory length. this quantity grows exponentially in the depth of the network and is responsible for the depth sensitivity observed. these results translate to consequences for networks during and after training. they suggest that parameters earlier in a network have greater influence on its expressive power in particular given a layer its influence on expressivity is determined by the remaining depth of the network after that layer. this is verified with experiments on mnist and cifar 10. we also explore the effect of training on the input output map and find that it trades off between the stability and expressivity.
precise recovery of latent vectors from generative adversarial networks
generative adversarial networks gans transform latent vectors into visually plausible images. it is generally thought that the original gan formulation gives no out of the box method to reverse the mapping projecting images back into latent space. we introduce a simple gradient based technique called stochastic clipping. in experiments for images generated by the gan we precisely recover their latent vector pre images 100 of the time. additional experiments demonstrate that this method is robust to noise. finally we show that even for unseen images our method appears to recover unique encodings.
predicting surgery duration with neural heteroscedastic regression
scheduling surgeries is a challenging task due to the fundamental uncertainty of the clinical environment as well as the risks and costs associated with under and over booking. we investigate neural regression algorithms to estimate the parameters of surgery case durations focusing on the issue of heteroscedasticity. we seek to simultaneously estimate the duration of each surgery as well as a surgery specific notion of our uncertainty about its duration. estimating this uncertainty can lead to more nuanced and effective scheduling strategies as we are able to schedule surgeries more efficiently while allowing an informed and case specific margin of error. using surgery records from the uc san diego health system from a large united states health system we demonstrate potential improvements on the order of 20 in terms of minutes overbooked compared to current scheduling techniques. moreover we demonstrate that surgery durations are indeed heteroscedastic. we show that models that estimate case specific uncertainty better fit the data log likelihood . additionally we show that the heteroscedastic predictions can more optimally trade off between over and under booking minutes especially when idle minutes and scheduling collisions confer disparate costs.
depth creates no bad local minima
in deep learning textit depth as well as textit nonlinearity create non convex loss surfaces. then does depth alone create bad local minima in this paper we prove that without nonlinearity depth alone does not create bad local minima although it induces non convex loss surface. using this insight we greatly simplify a recently proposed proof to show that all of the local minima of feedforward deep linear neural networks are global minima. our theoretical results generalize previous results with fewer assumptions and this analysis provides a method to show similar results beyond square loss in deep linear models.
deep semi random features for nonlinear function approximation
we propose semi random features for nonlinear function approximation. the flexibility of semi random feature lies between the fully adjustable units in deep learning and the random features used in kernel methods. for one hidden layer models with semi random features we prove with no unrealistic assumptions that the model classes contain an arbitrarily good function as the width increases universality and despite non convexity we can find such a good function optimization theory that generalizes to unseen new data generalization bound . for deep models with no unrealistic assumptions we prove universal approximation ability a lower bound on approximation error a partial optimization guarantee and a generalization bound. depending on the problems the generalization bound of deep semi random features can be exponentially better than the known bounds of deep relu nets our generalization error bound can be independent of the depth the number of trainable weights as well as the input dimensionality. in experiments we show that semi random features can match the performance of neural networks by using slightly more units and it outperforms random features by using significantly fewer units. moreover we introduce a new implicit ensemble method by using semi random features.
curriculum dropout
dropout is a very effective way of regularizing neural networks. stochastically dropping out units with a certain probability discourages over specific co adaptations of feature detectors preventing overfitting and improving network generalization. besides dropout can be interpreted as an approximate model aggregation technique where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. in this paper we show that using a fixed dropout probability during training is a suboptimal choice. we thus propose a time scheduling for the probability of retaining neurons in the network. this induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. this idea of starting easy and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. indeed we prove that our optimization strategy implements a very general curriculum scheme by gradually adding noise to both the input and intermediate feature representations within the network architecture. experiments on seven image classification datasets and different network architectures show that our method named curriculum dropout frequently yields to better generalization and at worst performs just as well as the standard dropout method.
the power of deeper networks for expressing natural functions
it is well known that neural networks are universal approximators but that deeper networks tend to be much more efficient than shallow ones. we shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks but grows exponentially when merely a single hidden layer is allowed. we also provide evidence that when the number of hidden layers is increased from 1 to k the neuron requirement grows exponentially not with n but with n 1 k suggesting that the minimum number of layers required for computational tractability grows only logarithmically with n .
gradient descent for spiking neural networks
much of studies on neural computation are based on network models of static neurons that produce analog output despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. research in spike based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. here we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. for demonstration we trained recurrent spiking networks on two dynamic tasks one that requires optimizing fast millisecond spike based interactions for efficient encoding of information and a delayed memory xor task over extended duration second . the results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as behavioral time scales. in conclusion our result offers a general purpose supervised learning algorithm for spiking neural networks thus advancing further investigations on spike based computation.
unsure when to stop ask your semantic neighbors
in iterative supervised learning algorithms it is common to reach a point in the search where no further induction seems to be possible with the available data. if the search is continued beyond this point the risk of overfitting increases significantly. following the recent developments in inductive semantic stochastic methods this paper studies the feasibility of using information gathered from the semantic neighborhood to decide when to stop the search. two semantic stopping criteria are proposed and experimentally assessed in geometric semantic genetic programming gsgp and in the semantic learning machine slm algorithm the equivalent algorithm for neural networks . the experiments are performed on real world high dimensional regression datasets. the results show that the proposed semantic stopping criteria are able to detect stopping points that result in a competitive generalization for both gsgp and slm. this approach also yields computationally efficient algorithms as it allows the evolution of neural networks in less than 3 seconds on average and of gp trees in at most 10 seconds. the usage of the proposed semantic stopping criteria in conjunction with the computation of optimal mutation learning steps also results in small trees and neural networks.
anomaly detection on graph time series
in this paper we use variational recurrent neural network to investigate the anomaly detection problem on graph time series. the temporal correlation is modeled by the combination of recurrent neural network rnn and variational inference vi while the spatial information is captured by the graph convolutional network. in order to incorporate external factors we use feature extractor to augment the transition of latent variables which can learn the influence of external factors. with the target function as accumulative elbo it is easy to extend this model to on line method. the experimental study on traffic flow data shows the detection capability of the proposed method.
a neural network architecture combining gated recurrent unit gru and support vector machine svm for intrusion detection in network traffic data
gated recurrent unit gru is a recently developed variation of the long short term memory lstm unit both of which are types of recurrent neural network rnn . through empirical evidence both models have been proven to be effective in a wide variety of machine learning tasks such as natural language processing wen et al. 2015 speech recognition chorowski et al. 2015 and text classification yang et al. 2016 . conventionally like most neural networks both of the aforementioned rnn variants employ the softmax function as its final output layer for its prediction and the cross entropy function for computing its loss. in this paper we present an amendment to this norm by introducing linear support vector machine svm as the replacement for softmax in the final output layer of a gru model. furthermore the cross entropy function shall be replaced with a margin based function. while there have been similar studies alalshekmubarak smith 2013 tang 2013 this proposal is primarily intended for binary classification on intrusion detection using the 2013 network traffic data from the honeypot systems of kyoto university. results show that the gru svm model performs relatively higher than the conventional gru softmax model. the proposed model reached a training accuracy of 81.54 and a testing accuracy of 84.15 while the latter was able to reach a training accuracy of 63.07 and a testing accuracy of 70.75 . in addition the juxtaposition of these two final output layers indicate that the svm would outperform softmax in prediction time a theoretical implication which was supported by the actual training and testing time in the study.
deepsafe a data driven approach for checking adversarial robustness in neural networks
deep neural networks have become widely used obtaining remarkable results in domains such as computer vision speech recognition natural language processing audio recognition social network filtering machine translation and bio informatics where they have produced results comparable to human experts. however these networks can be easily fooled by adversarial perturbations minimal changes to correctly classified inputs that cause the network to mis classify them. this phenomenon represents a concern for both safety and security but it is currently unclear how to measure a network s robustness against such perturbations. existing techniques are limited to checking robustness around a few individual input points providing only very limited guarantees. we propose a novel approach for automatically identifying safe regions of the input space within which the network is robust against adversarial perturbations. the approach is data guided relying on clustering to identify well defined geometric regions as candidate safe regions. we then utilize verification techniques to confirm that these regions are safe or to provide counter examples showing that they are not safe. we also introduce the notion of targeted robustness which for a given target label and reg