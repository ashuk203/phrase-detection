{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "from whoosh.qparser import QueryParser\n",
    "import whoosh.index as index\n",
    "from whoosh.index import open_dir\n",
    "from whoosh.query import Every\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from prdualrank import prDualRank\n",
    "from extractor_helpers import *\n",
    "from final_framework import run_prdualrank, patternSearch, tuple_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1...\n",
      "\n",
      "1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [05:07<20:30, 307.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2...\n",
      "\n",
      "1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [10:26<15:33, 311.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3...\n",
      "\n",
      "1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [16:17<10:46, 323.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4...\n",
      "\n",
      "1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [21:54<05:27, 327.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5...\n",
      "\n",
      "1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [27:33<00:00, 330.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# seed = set([\"machine learning\", \"query optimization\", \"RSA encryption\", \"distributed database systems\"])\n",
    "seed = set([\"machine learning\", \"convex optimization\", \"neural network\", \"reinforcement learning\", \"named entity recognition\", \"supervised learning\"])\n",
    "keywords = seed\n",
    "filename = \"./data/\" + \"arxiv_titles_and_abstracts_short.txt\"\n",
    "iter_num = 5\n",
    "k_depth = 100\n",
    "results_filename = \"./outputs/\" + \"results_new.txt\"\n",
    "new_keywords_obtained = set()\n",
    "\n",
    "with open(results_filename, \"w+\") as f:\n",
    "    for i in tqdm(range(iter_num)):\n",
    "        print(\"Iteration \" + str(i+1) + \"...\\n\")\n",
    "        sorted_patterns = patternSearch(seed, keywords, filename)\n",
    "        f.write(\"\\nSorted Patterns:\\n\")\n",
    "        for pattern in sorted_patterns[0:k_depth]:\n",
    "            f.write(str(pattern))\n",
    "            f.write(\"\\n\")\n",
    "        sorted_keywords = tuple_search(seed, sorted_patterns, filename, k_depth)\n",
    "        f.write(\"Sorted Keywords:\\n\")\n",
    "        f.write(str(sorted_keywords))\n",
    "        new_keywords_obtained.update(sorted_keywords)\n",
    "        keywords = sorted_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/arxivData.json\") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'based on the aristotelian concept of potentiality vs. actuality allowing for the study of energy and dynamics in language we propose a field approach to lexical analysis. falling back on the distributional hypothesis to statistically model word meaning we used evolving fields as a metaphor to express time dependent changes in a vector space model by a combination of random indexing and evolving self organizing maps esom . to monitor semantic drifts within the observation period an experiment was carried out on the term space of a collection of 12.8 million amazon book reviews. for evaluation the semantic consistency of esom term clusters was compared with their respective neighbourhoods in wordnet and contrasted with distances among term vectors by random indexing. we found that at 0.05 level of significance the terms in the clusters showed a high level of semantic consistency. tracking the drift of distributional patterns in the term space across time periods we found that consistency decreased but not at a statistically significant level. our method is highly scalable with interpretations in philosophy.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"[^a-zA-Z0-9.]+\", ' ', data[100]['summary']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/arxiv_titles_and_abstracts.txt\", \"w+\") as f:\n",
    "    for entry in data:\n",
    "        f.write(re.sub(r\"[^a-zA-Z0-9.]+\", ' ', entry['title']).lower() + \"\\n\")\n",
    "        f.write(re.sub(r\"[^a-zA-Z0-9.]+\", ' ', entry['summary']).lower() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/arxiv_titles_and_abstracts_short.txt\", \"w+\") as f:\n",
    "    for entry in data:\n",
    "        f.write(re.sub(r\"[^a-zA-Z0-9.]+\", ' ', entry['title']).lower() + \"\\n\")\n",
    "        f.write(re.sub(r\"[^a-zA-Z0-9.]+\", ' ', entry['summary']).lower() + \"\\n\")\n",
    "    f.truncate(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_keywords_obtained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstractive sentence',\n",
       " 'aco techniques',\n",
       " 'active learning',\n",
       " 'adversarial examples',\n",
       " 'adversarial nets',\n",
       " 'adversarial networks',\n",
       " 'adversarial training',\n",
       " 'anatomical structures',\n",
       " 'arbitrary orientations',\n",
       " 'artificial intelligence',\n",
       " 'associative memory',\n",
       " 'automatic speech',\n",
       " 'back propagation',\n",
       " 'basic units',\n",
       " 'bayesian methods',\n",
       " 'bidirectional lstm',\n",
       " 'big data',\n",
       " 'black box',\n",
       " 'central importance',\n",
       " 'commercial advertisements',\n",
       " 'commercial speech',\n",
       " 'complex behaviors',\n",
       " 'complex tasks',\n",
       " 'compositional models',\n",
       " 'contrastive biclusters',\n",
       " 'convolutional autoencoder',\n",
       " 'convolutional autoencoders',\n",
       " 'convolutional feature',\n",
       " 'convolutional filters',\n",
       " 'convolutional networks',\n",
       " 'coral correlation',\n",
       " 'deep learning',\n",
       " 'deep nets',\n",
       " 'deep networks',\n",
       " 'deep reinforcement',\n",
       " 'deterministic policy',\n",
       " 'different sequence',\n",
       " 'different tasks',\n",
       " 'different types',\n",
       " 'discrete actions',\n",
       " 'downstream tasks',\n",
       " 'dual stream',\n",
       " 'dynamical model',\n",
       " 'dynamical models',\n",
       " 'dynamical systems',\n",
       " 'efficient training',\n",
       " 'electronic health',\n",
       " 'english language',\n",
       " 'ensemble model',\n",
       " 'evolutionary algorithms',\n",
       " 'extensive evaluations',\n",
       " 'external factors',\n",
       " 'extreme learning',\n",
       " 'factoid questions',\n",
       " 'framelet expansion',\n",
       " 'future frames',\n",
       " 'future work',\n",
       " 'generative model',\n",
       " 'generative models',\n",
       " 'hierarchical neural',\n",
       " 'hiererchical features',\n",
       " 'high level',\n",
       " 'high quality',\n",
       " 'high resolution',\n",
       " 'high speed',\n",
       " 'higher level',\n",
       " 'human intelligence',\n",
       " 'human language',\n",
       " 'human perception',\n",
       " 'implicit distributions',\n",
       " 'inductive program',\n",
       " 'initial conditions',\n",
       " 'internal model',\n",
       " 'intrinsic motivation',\n",
       " 'invariant classifiers',\n",
       " 'large amounts',\n",
       " 'large scale',\n",
       " 'learning dl',\n",
       " 'linear models',\n",
       " 'linear networks',\n",
       " 'linear neural',\n",
       " 'linear regions',\n",
       " 'linguistic processing',\n",
       " 'long range',\n",
       " 'long sequences',\n",
       " 'long term',\n",
       " 'low level',\n",
       " 'machine translation',\n",
       " 'main steps',\n",
       " 'many applications',\n",
       " 'many areas',\n",
       " 'many cases',\n",
       " 'many machine',\n",
       " 'many perception',\n",
       " 'many task',\n",
       " 'many tasks',\n",
       " 'medical imaging',\n",
       " 'meta learning',\n",
       " 'metric learning',\n",
       " 'mobile devices',\n",
       " 'multi layer',\n",
       " 'multi view',\n",
       " 'multimodal sequences',\n",
       " 'multiple datasets',\n",
       " 'n tuples',\n",
       " 'natural images',\n",
       " 'natural language',\n",
       " 'neural architecture',\n",
       " 'neural architectures',\n",
       " 'neural heart',\n",
       " 'neural language',\n",
       " 'neural machine',\n",
       " 'neural model',\n",
       " 'neural models',\n",
       " 'neural nets',\n",
       " 'neural network',\n",
       " 'neural networks',\n",
       " 'neural operations',\n",
       " 'neural sequence',\n",
       " 'neural translation',\n",
       " 'next move',\n",
       " 'nonlinear networks',\n",
       " 'nonlinear pca',\n",
       " 'nonlinear transformations',\n",
       " 'novel data',\n",
       " 'online stream',\n",
       " 'orthogonal networks',\n",
       " 'other words',\n",
       " 'patient notes',\n",
       " 'perceptual tasks',\n",
       " 'plant plant',\n",
       " 'possible classes',\n",
       " 'predictive coding',\n",
       " 'preliminary experiments',\n",
       " 'previous work',\n",
       " 'prior disambiguation',\n",
       " 'prior information',\n",
       " 'prior knowledge',\n",
       " 'probabilistic models',\n",
       " 'quantitative results',\n",
       " 'real time',\n",
       " 'real world',\n",
       " 'recent years',\n",
       " 'recommender systems',\n",
       " 'recursive model',\n",
       " 'residual learning',\n",
       " 'residual network',\n",
       " 'residual networks',\n",
       " 'seq2seq framework',\n",
       " 'sequential data',\n",
       " 'several benchmark',\n",
       " 'single directions',\n",
       " 'soft attention',\n",
       " 'sparse network',\n",
       " 'sparse pathways',\n",
       " 'sparse targets',\n",
       " 'speaker vectors',\n",
       " 'standard benchmark',\n",
       " 'statistical machine',\n",
       " 'stochastic gradient',\n",
       " 'strong baselines',\n",
       " 'structured prediction',\n",
       " 'successor reinforcement',\n",
       " 'such models',\n",
       " 'such networks',\n",
       " 'super convergence',\n",
       " 'supervised data',\n",
       " 'supervised learning',\n",
       " 'supervisory signal',\n",
       " 'synaptic clusters',\n",
       " 'syntactic kernels',\n",
       " 'synthetic data',\n",
       " 'synthetic images',\n",
       " 'temporal abstraction',\n",
       " 'textual data',\n",
       " 'time series',\n",
       " 'universal perturbations',\n",
       " 'unlabeled data',\n",
       " 'unstructured environments',\n",
       " 'unsupervised domain',\n",
       " 'unsupervised learning',\n",
       " 'variational inference',\n",
       " 'various applications',\n",
       " 'visual dialog',\n",
       " 'visual question',\n",
       " 'visual stimuli'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_keywords_obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./outputs/prdualrank_result_phrases.txt\", \"w+\") as f:\n",
    "    for word in new_keywords_obtained:\n",
    "        f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
